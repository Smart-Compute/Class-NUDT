{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #gpu\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(\"./mnist_data\", train=True, download=False,\n",
    "                            transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5,),std=(0.5,))  #就会把tensor变成(-1,1)而不是用真实的均值和方差normalize\n",
    "                            ]))\n",
    "batch_size = 32\n",
    "data_loader = tud.DataLoader(mnist_data, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （1）配置网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28*28\n",
    "hidden_size = 256\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),#1：二分类问题，对和不对\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "latent_size= 64\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh()\n",
    ")\n",
    "D = D.to(device)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3)定义优化策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch [0/30], Step [ 0/1875 ], d_loss: 1.3433, g_loss: 0.6916,g_score: 16.0262 \n",
      "Epoch [0/30], Step [ 100/1875 ], d_loss: 0.7320, g_loss: 0.7703,g_score: 14.8297 \n",
      "Epoch [0/30], Step [ 200/1875 ], d_loss: 0.6243, g_loss: 0.8752,g_score: 13.3741 \n",
      "Epoch [0/30], Step [ 300/1875 ], d_loss: 0.5569, g_loss: 0.9496,g_score: 12.3991 \n",
      "Epoch [0/30], Step [ 400/1875 ], d_loss: 0.6711, g_loss: 0.8213,g_score: 14.0852 \n",
      "Epoch [0/30], Step [ 500/1875 ], d_loss: 1.7311, g_loss: 0.8169,g_score: 14.5177 \n",
      "Epoch [0/30], Step [ 600/1875 ], d_loss: 0.4950, g_loss: 1.0549,g_score: 11.1736 \n",
      "Epoch [0/30], Step [ 700/1875 ], d_loss: 0.9609, g_loss: 0.7780,g_score: 15.0454 \n",
      "Epoch [0/30], Step [ 800/1875 ], d_loss: 0.7594, g_loss: 0.9022,g_score: 13.4681 \n",
      "Epoch [0/30], Step [ 900/1875 ], d_loss: 0.6381, g_loss: 0.9036,g_score: 13.0222 \n",
      "Epoch [0/30], Step [ 1000/1875 ], d_loss: 0.9858, g_loss: 0.8562,g_score: 13.9281 \n",
      "Epoch [0/30], Step [ 1100/1875 ], d_loss: 1.0829, g_loss: 0.7997,g_score: 14.4197 \n",
      "Epoch [0/30], Step [ 1200/1875 ], d_loss: 1.2957, g_loss: 0.7904,g_score: 14.6691 \n",
      "Epoch [0/30], Step [ 1300/1875 ], d_loss: 0.7457, g_loss: 0.9178,g_score: 12.8580 \n",
      "Epoch [0/30], Step [ 1400/1875 ], d_loss: 0.8646, g_loss: 0.8963,g_score: 13.2058 \n",
      "Epoch [0/30], Step [ 1500/1875 ], d_loss: 0.7396, g_loss: 0.8557,g_score: 13.8572 \n",
      "Epoch [0/30], Step [ 1600/1875 ], d_loss: 0.7733, g_loss: 0.8739,g_score: 13.7761 \n",
      "Epoch [0/30], Step [ 1700/1875 ], d_loss: 0.9479, g_loss: 0.8155,g_score: 14.4704 \n",
      "Epoch [0/30], Step [ 1800/1875 ], d_loss: 0.6876, g_loss: 0.9328,g_score: 12.7259 \n",
      "1\n",
      "Epoch [1/30], Step [ 0/1875 ], d_loss: 1.0598, g_loss: 0.8087,g_score: 14.5856 \n",
      "Epoch [1/30], Step [ 100/1875 ], d_loss: 1.6927, g_loss: 0.9181,g_score: 13.1979 \n",
      "Epoch [1/30], Step [ 200/1875 ], d_loss: 0.9671, g_loss: 0.9319,g_score: 13.3839 \n",
      "Epoch [1/30], Step [ 300/1875 ], d_loss: 0.9568, g_loss: 0.7589,g_score: 15.3431 \n",
      "Epoch [1/30], Step [ 400/1875 ], d_loss: 1.1443, g_loss: 0.8547,g_score: 13.6394 \n",
      "Epoch [1/30], Step [ 500/1875 ], d_loss: 1.5494, g_loss: 0.9193,g_score: 12.9861 \n",
      "Epoch [1/30], Step [ 600/1875 ], d_loss: 0.7459, g_loss: 0.8319,g_score: 14.1647 \n",
      "Epoch [1/30], Step [ 700/1875 ], d_loss: 1.5880, g_loss: 0.7761,g_score: 14.8441 \n",
      "Epoch [1/30], Step [ 800/1875 ], d_loss: 1.2095, g_loss: 0.7773,g_score: 15.1945 \n",
      "Epoch [1/30], Step [ 900/1875 ], d_loss: 1.4236, g_loss: 0.9156,g_score: 13.0287 \n",
      "Epoch [1/30], Step [ 1000/1875 ], d_loss: 1.1634, g_loss: 0.7860,g_score: 15.0810 \n",
      "Epoch [1/30], Step [ 1100/1875 ], d_loss: 0.8677, g_loss: 0.7985,g_score: 14.4777 \n",
      "Epoch [1/30], Step [ 1200/1875 ], d_loss: 1.4707, g_loss: 0.7575,g_score: 15.3118 \n",
      "Epoch [1/30], Step [ 1300/1875 ], d_loss: 1.1273, g_loss: 0.9823,g_score: 12.0976 \n",
      "Epoch [1/30], Step [ 1400/1875 ], d_loss: 1.3263, g_loss: 0.7568,g_score: 15.2527 \n",
      "Epoch [1/30], Step [ 1500/1875 ], d_loss: 0.9753, g_loss: 0.7918,g_score: 14.6533 \n",
      "Epoch [1/30], Step [ 1600/1875 ], d_loss: 0.8770, g_loss: 0.8644,g_score: 13.6676 \n",
      "Epoch [1/30], Step [ 1700/1875 ], d_loss: 1.1551, g_loss: 0.8497,g_score: 13.7775 \n",
      "Epoch [1/30], Step [ 1800/1875 ], d_loss: 1.1579, g_loss: 0.9699,g_score: 12.6163 \n",
      "2\n",
      "Epoch [2/30], Step [ 0/1875 ], d_loss: 1.0591, g_loss: 0.7741,g_score: 15.1156 \n",
      "Epoch [2/30], Step [ 100/1875 ], d_loss: 0.7661, g_loss: 0.8687,g_score: 13.5725 \n",
      "Epoch [2/30], Step [ 200/1875 ], d_loss: 1.0586, g_loss: 0.8817,g_score: 13.4113 \n",
      "Epoch [2/30], Step [ 300/1875 ], d_loss: 0.7152, g_loss: 0.8753,g_score: 13.5076 \n",
      "Epoch [2/30], Step [ 400/1875 ], d_loss: 1.0004, g_loss: 0.9125,g_score: 13.3471 \n",
      "Epoch [2/30], Step [ 500/1875 ], d_loss: 0.9158, g_loss: 0.9041,g_score: 13.3491 \n",
      "Epoch [2/30], Step [ 600/1875 ], d_loss: 0.9203, g_loss: 1.0284,g_score: 11.9587 \n",
      "Epoch [2/30], Step [ 700/1875 ], d_loss: 1.0254, g_loss: 1.0299,g_score: 11.5903 \n",
      "Epoch [2/30], Step [ 800/1875 ], d_loss: 0.6225, g_loss: 1.0655,g_score: 11.2234 \n",
      "Epoch [2/30], Step [ 900/1875 ], d_loss: 1.4088, g_loss: 0.9017,g_score: 13.5022 \n",
      "Epoch [2/30], Step [ 1000/1875 ], d_loss: 0.9579, g_loss: 0.8701,g_score: 13.5005 \n",
      "Epoch [2/30], Step [ 1100/1875 ], d_loss: 0.8516, g_loss: 1.0387,g_score: 11.5093 \n",
      "Epoch [2/30], Step [ 1200/1875 ], d_loss: 1.3430, g_loss: 0.8649,g_score: 13.7720 \n",
      "Epoch [2/30], Step [ 1300/1875 ], d_loss: 1.2316, g_loss: 0.8810,g_score: 13.9691 \n",
      "Epoch [2/30], Step [ 1400/1875 ], d_loss: 1.1782, g_loss: 0.8571,g_score: 13.9238 \n",
      "Epoch [2/30], Step [ 1500/1875 ], d_loss: 1.0381, g_loss: 0.9184,g_score: 13.3313 \n",
      "Epoch [2/30], Step [ 1600/1875 ], d_loss: 1.7214, g_loss: 0.9673,g_score: 12.9616 \n",
      "Epoch [2/30], Step [ 1700/1875 ], d_loss: 0.8942, g_loss: 0.8395,g_score: 14.2462 \n",
      "Epoch [2/30], Step [ 1800/1875 ], d_loss: 1.5802, g_loss: 0.8926,g_score: 13.9063 \n",
      "3\n",
      "Epoch [3/30], Step [ 0/1875 ], d_loss: 0.7988, g_loss: 1.0743,g_score: 11.1824 \n",
      "Epoch [3/30], Step [ 100/1875 ], d_loss: 1.3322, g_loss: 1.0969,g_score: 11.4138 \n",
      "Epoch [3/30], Step [ 200/1875 ], d_loss: 0.8289, g_loss: 0.9616,g_score: 12.7739 \n",
      "Epoch [3/30], Step [ 300/1875 ], d_loss: 0.9675, g_loss: 0.9813,g_score: 12.1854 \n",
      "Epoch [3/30], Step [ 400/1875 ], d_loss: 1.0246, g_loss: 0.9113,g_score: 13.2189 \n",
      "Epoch [3/30], Step [ 500/1875 ], d_loss: 1.3207, g_loss: 0.8938,g_score: 13.9947 \n",
      "Epoch [3/30], Step [ 600/1875 ], d_loss: 1.0891, g_loss: 0.8061,g_score: 14.7500 \n",
      "Epoch [3/30], Step [ 700/1875 ], d_loss: 1.3205, g_loss: 0.9648,g_score: 12.5497 \n",
      "Epoch [3/30], Step [ 800/1875 ], d_loss: 1.1232, g_loss: 0.8437,g_score: 13.9505 \n",
      "Epoch [3/30], Step [ 900/1875 ], d_loss: 1.0792, g_loss: 0.9911,g_score: 12.1322 \n",
      "Epoch [3/30], Step [ 1000/1875 ], d_loss: 1.0591, g_loss: 0.8649,g_score: 13.7244 \n",
      "Epoch [3/30], Step [ 1100/1875 ], d_loss: 1.2774, g_loss: 0.9544,g_score: 12.7315 \n",
      "Epoch [3/30], Step [ 1200/1875 ], d_loss: 0.7016, g_loss: 0.9635,g_score: 12.5599 \n",
      "Epoch [3/30], Step [ 1300/1875 ], d_loss: 1.3607, g_loss: 1.0454,g_score: 11.7283 \n",
      "Epoch [3/30], Step [ 1400/1875 ], d_loss: 1.3333, g_loss: 0.9204,g_score: 13.2188 \n",
      "Epoch [3/30], Step [ 1500/1875 ], d_loss: 0.8080, g_loss: 0.9575,g_score: 12.5762 \n",
      "Epoch [3/30], Step [ 1600/1875 ], d_loss: 1.0797, g_loss: 0.9974,g_score: 12.2670 \n",
      "Epoch [3/30], Step [ 1700/1875 ], d_loss: 1.2621, g_loss: 0.9012,g_score: 13.2513 \n",
      "Epoch [3/30], Step [ 1800/1875 ], d_loss: 1.1023, g_loss: 0.8629,g_score: 13.6157 \n",
      "4\n",
      "Epoch [4/30], Step [ 0/1875 ], d_loss: 1.2973, g_loss: 0.9200,g_score: 13.1336 \n",
      "Epoch [4/30], Step [ 100/1875 ], d_loss: 1.2036, g_loss: 0.7349,g_score: 15.9334 \n",
      "Epoch [4/30], Step [ 200/1875 ], d_loss: 1.2343, g_loss: 0.8830,g_score: 13.5294 \n",
      "Epoch [4/30], Step [ 300/1875 ], d_loss: 1.1274, g_loss: 0.8079,g_score: 14.5435 \n",
      "Epoch [4/30], Step [ 400/1875 ], d_loss: 0.9226, g_loss: 0.8261,g_score: 14.0911 \n",
      "Epoch [4/30], Step [ 500/1875 ], d_loss: 1.0693, g_loss: 0.8055,g_score: 14.5738 \n",
      "Epoch [4/30], Step [ 600/1875 ], d_loss: 0.9253, g_loss: 0.8500,g_score: 13.9282 \n",
      "Epoch [4/30], Step [ 700/1875 ], d_loss: 1.1324, g_loss: 1.0231,g_score: 11.6223 \n",
      "Epoch [4/30], Step [ 800/1875 ], d_loss: 1.2584, g_loss: 0.8787,g_score: 13.5432 \n",
      "Epoch [4/30], Step [ 900/1875 ], d_loss: 1.0772, g_loss: 0.9478,g_score: 12.5796 \n",
      "Epoch [4/30], Step [ 1000/1875 ], d_loss: 1.3763, g_loss: 0.8161,g_score: 14.4800 \n",
      "Epoch [4/30], Step [ 1100/1875 ], d_loss: 1.5341, g_loss: 0.9198,g_score: 12.9694 \n",
      "Epoch [4/30], Step [ 1200/1875 ], d_loss: 0.9020, g_loss: 1.0131,g_score: 11.8670 \n",
      "Epoch [4/30], Step [ 1300/1875 ], d_loss: 1.2461, g_loss: 0.7823,g_score: 15.3228 \n",
      "Epoch [4/30], Step [ 1400/1875 ], d_loss: 1.1890, g_loss: 1.0344,g_score: 11.9132 \n",
      "Epoch [4/30], Step [ 1500/1875 ], d_loss: 1.1921, g_loss: 1.1152,g_score: 10.7595 \n",
      "Epoch [4/30], Step [ 1600/1875 ], d_loss: 1.1665, g_loss: 0.8632,g_score: 13.8769 \n",
      "Epoch [4/30], Step [ 1700/1875 ], d_loss: 1.0753, g_loss: 1.0578,g_score: 11.2948 \n",
      "Epoch [4/30], Step [ 1800/1875 ], d_loss: 1.4629, g_loss: 0.8317,g_score: 14.3023 \n",
      "5\n",
      "Epoch [5/30], Step [ 0/1875 ], d_loss: 1.1283, g_loss: 0.9121,g_score: 13.3598 \n",
      "Epoch [5/30], Step [ 100/1875 ], d_loss: 1.2201, g_loss: 0.9238,g_score: 12.8774 \n",
      "Epoch [5/30], Step [ 200/1875 ], d_loss: 1.0882, g_loss: 0.8186,g_score: 14.3830 \n",
      "Epoch [5/30], Step [ 300/1875 ], d_loss: 1.3483, g_loss: 0.7457,g_score: 15.6327 \n",
      "Epoch [5/30], Step [ 400/1875 ], d_loss: 1.0971, g_loss: 0.7955,g_score: 14.7711 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Step [ 500/1875 ], d_loss: 1.0495, g_loss: 1.0200,g_score: 11.6216 \n",
      "Epoch [5/30], Step [ 600/1875 ], d_loss: 1.1387, g_loss: 0.8234,g_score: 14.4025 \n",
      "Epoch [5/30], Step [ 700/1875 ], d_loss: 0.9264, g_loss: 0.8878,g_score: 13.6445 \n",
      "Epoch [5/30], Step [ 800/1875 ], d_loss: 0.8531, g_loss: 0.8346,g_score: 14.0805 \n",
      "Epoch [5/30], Step [ 900/1875 ], d_loss: 1.0282, g_loss: 0.8501,g_score: 13.9454 \n",
      "Epoch [5/30], Step [ 1000/1875 ], d_loss: 0.8733, g_loss: 0.9259,g_score: 12.8892 \n",
      "Epoch [5/30], Step [ 1100/1875 ], d_loss: 1.0838, g_loss: 0.9698,g_score: 12.3633 \n",
      "Epoch [5/30], Step [ 1200/1875 ], d_loss: 1.3250, g_loss: 1.0153,g_score: 12.0886 \n",
      "Epoch [5/30], Step [ 1300/1875 ], d_loss: 0.9712, g_loss: 0.9137,g_score: 13.2305 \n",
      "Epoch [5/30], Step [ 1400/1875 ], d_loss: 1.0744, g_loss: 0.9011,g_score: 13.2419 \n",
      "Epoch [5/30], Step [ 1500/1875 ], d_loss: 1.0128, g_loss: 1.1050,g_score: 10.8955 \n",
      "Epoch [5/30], Step [ 1600/1875 ], d_loss: 1.2354, g_loss: 0.7618,g_score: 15.5792 \n",
      "Epoch [5/30], Step [ 1700/1875 ], d_loss: 1.1828, g_loss: 0.9269,g_score: 13.2296 \n",
      "Epoch [5/30], Step [ 1800/1875 ], d_loss: 1.1672, g_loss: 0.9215,g_score: 13.0388 \n",
      "6\n",
      "Epoch [6/30], Step [ 0/1875 ], d_loss: 1.0203, g_loss: 0.8937,g_score: 13.5272 \n",
      "Epoch [6/30], Step [ 100/1875 ], d_loss: 1.2638, g_loss: 1.1717,g_score: 10.1460 \n",
      "Epoch [6/30], Step [ 200/1875 ], d_loss: 1.1120, g_loss: 1.0717,g_score: 11.2236 \n",
      "Epoch [6/30], Step [ 300/1875 ], d_loss: 1.1718, g_loss: 1.0564,g_score: 12.0329 \n",
      "Epoch [6/30], Step [ 400/1875 ], d_loss: 0.9924, g_loss: 0.9362,g_score: 12.8589 \n",
      "Epoch [6/30], Step [ 500/1875 ], d_loss: 1.4423, g_loss: 0.9561,g_score: 12.8075 \n",
      "Epoch [6/30], Step [ 600/1875 ], d_loss: 0.8676, g_loss: 0.8928,g_score: 13.4820 \n",
      "Epoch [6/30], Step [ 700/1875 ], d_loss: 1.4551, g_loss: 1.0483,g_score: 11.6544 \n",
      "Epoch [6/30], Step [ 800/1875 ], d_loss: 0.9621, g_loss: 1.1089,g_score: 10.8223 \n",
      "Epoch [6/30], Step [ 900/1875 ], d_loss: 1.1288, g_loss: 1.0246,g_score: 11.7342 \n",
      "Epoch [6/30], Step [ 1000/1875 ], d_loss: 1.1487, g_loss: 0.9047,g_score: 13.3759 \n",
      "Epoch [6/30], Step [ 1100/1875 ], d_loss: 0.8511, g_loss: 0.8975,g_score: 13.3283 \n",
      "Epoch [6/30], Step [ 1200/1875 ], d_loss: 0.9838, g_loss: 0.9345,g_score: 13.2580 \n",
      "Epoch [6/30], Step [ 1300/1875 ], d_loss: 1.2583, g_loss: 0.9529,g_score: 12.5844 \n",
      "Epoch [6/30], Step [ 1400/1875 ], d_loss: 0.9375, g_loss: 0.8554,g_score: 14.0537 \n",
      "Epoch [6/30], Step [ 1500/1875 ], d_loss: 1.1755, g_loss: 1.1108,g_score: 10.8354 \n",
      "Epoch [6/30], Step [ 1600/1875 ], d_loss: 1.2670, g_loss: 0.9772,g_score: 12.6140 \n",
      "Epoch [6/30], Step [ 1700/1875 ], d_loss: 1.0121, g_loss: 0.8621,g_score: 13.9554 \n",
      "Epoch [6/30], Step [ 1800/1875 ], d_loss: 1.1862, g_loss: 0.9452,g_score: 12.8041 \n",
      "7\n",
      "Epoch [7/30], Step [ 0/1875 ], d_loss: 0.9516, g_loss: 1.0564,g_score: 11.3943 \n",
      "Epoch [7/30], Step [ 100/1875 ], d_loss: 1.2677, g_loss: 0.9835,g_score: 12.3294 \n",
      "Epoch [7/30], Step [ 200/1875 ], d_loss: 1.0763, g_loss: 1.1473,g_score: 10.4002 \n",
      "Epoch [7/30], Step [ 300/1875 ], d_loss: 0.9618, g_loss: 1.0739,g_score: 11.2742 \n",
      "Epoch [7/30], Step [ 400/1875 ], d_loss: 1.6696, g_loss: 0.7284,g_score: 16.7078 \n",
      "Epoch [7/30], Step [ 500/1875 ], d_loss: 0.9338, g_loss: 1.0563,g_score: 11.6434 \n",
      "Epoch [7/30], Step [ 600/1875 ], d_loss: 1.4291, g_loss: 1.2008,g_score: 10.3703 \n",
      "Epoch [7/30], Step [ 700/1875 ], d_loss: 1.0292, g_loss: 1.1587,g_score: 10.5789 \n",
      "Epoch [7/30], Step [ 800/1875 ], d_loss: 1.5892, g_loss: 1.0671,g_score: 11.4598 \n",
      "Epoch [7/30], Step [ 900/1875 ], d_loss: 0.9399, g_loss: 0.8347,g_score: 14.3119 \n",
      "Epoch [7/30], Step [ 1000/1875 ], d_loss: 1.0577, g_loss: 0.9256,g_score: 13.1342 \n",
      "Epoch [7/30], Step [ 1100/1875 ], d_loss: 1.2208, g_loss: 0.9437,g_score: 13.2728 \n",
      "Epoch [7/30], Step [ 1200/1875 ], d_loss: 1.0671, g_loss: 0.8889,g_score: 13.5107 \n",
      "Epoch [7/30], Step [ 1300/1875 ], d_loss: 1.4680, g_loss: 0.9116,g_score: 13.2057 \n",
      "Epoch [7/30], Step [ 1400/1875 ], d_loss: 1.0276, g_loss: 0.8867,g_score: 13.4990 \n",
      "Epoch [7/30], Step [ 1500/1875 ], d_loss: 1.2306, g_loss: 0.8636,g_score: 13.6746 \n",
      "Epoch [7/30], Step [ 1600/1875 ], d_loss: 0.9125, g_loss: 0.9516,g_score: 12.5132 \n",
      "Epoch [7/30], Step [ 1700/1875 ], d_loss: 1.2240, g_loss: 0.9107,g_score: 13.0929 \n",
      "Epoch [7/30], Step [ 1800/1875 ], d_loss: 0.9957, g_loss: 1.0533,g_score: 11.5144 \n",
      "8\n",
      "Epoch [8/30], Step [ 0/1875 ], d_loss: 0.8885, g_loss: 0.8865,g_score: 13.3356 \n",
      "Epoch [8/30], Step [ 100/1875 ], d_loss: 1.4398, g_loss: 0.9228,g_score: 13.2242 \n",
      "Epoch [8/30], Step [ 200/1875 ], d_loss: 0.8235, g_loss: 0.8954,g_score: 13.2729 \n",
      "Epoch [8/30], Step [ 300/1875 ], d_loss: 1.2791, g_loss: 1.0869,g_score: 11.0882 \n",
      "Epoch [8/30], Step [ 400/1875 ], d_loss: 1.2904, g_loss: 0.8378,g_score: 14.7412 \n",
      "Epoch [8/30], Step [ 500/1875 ], d_loss: 1.1081, g_loss: 0.9365,g_score: 13.1477 \n",
      "Epoch [8/30], Step [ 600/1875 ], d_loss: 0.9829, g_loss: 1.0444,g_score: 11.5940 \n",
      "Epoch [8/30], Step [ 700/1875 ], d_loss: 0.9910, g_loss: 1.0107,g_score: 12.0909 \n",
      "Epoch [8/30], Step [ 800/1875 ], d_loss: 1.2405, g_loss: 1.1155,g_score: 10.9543 \n",
      "Epoch [8/30], Step [ 900/1875 ], d_loss: 1.0853, g_loss: 0.9980,g_score: 12.4738 \n",
      "Epoch [8/30], Step [ 1000/1875 ], d_loss: 1.1825, g_loss: 1.1199,g_score: 11.2258 \n",
      "Epoch [8/30], Step [ 1100/1875 ], d_loss: 0.9472, g_loss: 1.2653,g_score: 9.3804 \n",
      "Epoch [8/30], Step [ 1200/1875 ], d_loss: 1.1660, g_loss: 0.8706,g_score: 14.2557 \n",
      "Epoch [8/30], Step [ 1300/1875 ], d_loss: 1.1455, g_loss: 0.7746,g_score: 15.5608 \n",
      "Epoch [8/30], Step [ 1400/1875 ], d_loss: 1.0595, g_loss: 1.0461,g_score: 11.5471 \n",
      "Epoch [8/30], Step [ 1500/1875 ], d_loss: 1.1970, g_loss: 0.9339,g_score: 13.1280 \n",
      "Epoch [8/30], Step [ 1600/1875 ], d_loss: 1.0681, g_loss: 1.1018,g_score: 10.8974 \n",
      "Epoch [8/30], Step [ 1700/1875 ], d_loss: 1.2335, g_loss: 1.0376,g_score: 11.6453 \n",
      "Epoch [8/30], Step [ 1800/1875 ], d_loss: 1.1119, g_loss: 0.8246,g_score: 14.5258 \n",
      "9\n",
      "Epoch [9/30], Step [ 0/1875 ], d_loss: 0.7968, g_loss: 0.9078,g_score: 13.2495 \n",
      "Epoch [9/30], Step [ 100/1875 ], d_loss: 1.5023, g_loss: 0.8996,g_score: 13.3137 \n",
      "Epoch [9/30], Step [ 200/1875 ], d_loss: 0.9348, g_loss: 0.9240,g_score: 13.1858 \n",
      "Epoch [9/30], Step [ 300/1875 ], d_loss: 1.1130, g_loss: 1.0447,g_score: 11.4889 \n",
      "Epoch [9/30], Step [ 400/1875 ], d_loss: 1.2321, g_loss: 1.0645,g_score: 11.2967 \n",
      "Epoch [9/30], Step [ 500/1875 ], d_loss: 1.0013, g_loss: 0.8945,g_score: 13.4764 \n",
      "Epoch [9/30], Step [ 600/1875 ], d_loss: 1.1457, g_loss: 1.0184,g_score: 11.8413 \n",
      "Epoch [9/30], Step [ 700/1875 ], d_loss: 1.3596, g_loss: 1.2141,g_score: 9.8081 \n",
      "Epoch [9/30], Step [ 800/1875 ], d_loss: 0.9973, g_loss: 0.9296,g_score: 12.9591 \n",
      "Epoch [9/30], Step [ 900/1875 ], d_loss: 1.1933, g_loss: 0.9114,g_score: 13.2145 \n",
      "Epoch [9/30], Step [ 1000/1875 ], d_loss: 1.1460, g_loss: 1.0812,g_score: 11.1622 \n",
      "Epoch [9/30], Step [ 1100/1875 ], d_loss: 1.2085, g_loss: 0.9239,g_score: 13.2865 \n",
      "Epoch [9/30], Step [ 1200/1875 ], d_loss: 1.3554, g_loss: 0.9352,g_score: 13.0534 \n",
      "Epoch [9/30], Step [ 1300/1875 ], d_loss: 1.0605, g_loss: 0.9712,g_score: 12.3277 \n",
      "Epoch [9/30], Step [ 1400/1875 ], d_loss: 1.1745, g_loss: 1.1753,g_score: 10.1784 \n",
      "Epoch [9/30], Step [ 1500/1875 ], d_loss: 1.3927, g_loss: 0.9649,g_score: 12.7387 \n",
      "Epoch [9/30], Step [ 1600/1875 ], d_loss: 0.8521, g_loss: 1.1676,g_score: 10.3470 \n",
      "Epoch [9/30], Step [ 1700/1875 ], d_loss: 1.4311, g_loss: 1.0635,g_score: 11.5128 \n",
      "Epoch [9/30], Step [ 1800/1875 ], d_loss: 0.8911, g_loss: 1.0557,g_score: 11.8045 \n",
      "10\n",
      "Epoch [10/30], Step [ 0/1875 ], d_loss: 1.5627, g_loss: 1.0570,g_score: 11.6254 \n",
      "Epoch [10/30], Step [ 100/1875 ], d_loss: 1.0639, g_loss: 0.9554,g_score: 12.8117 \n",
      "Epoch [10/30], Step [ 200/1875 ], d_loss: 1.1296, g_loss: 0.9003,g_score: 13.5311 \n",
      "Epoch [10/30], Step [ 300/1875 ], d_loss: 1.3967, g_loss: 0.7807,g_score: 15.4935 \n",
      "Epoch [10/30], Step [ 400/1875 ], d_loss: 1.2150, g_loss: 1.0211,g_score: 12.1318 \n",
      "Epoch [10/30], Step [ 500/1875 ], d_loss: 1.2108, g_loss: 1.2514,g_score: 9.4253 \n",
      "Epoch [10/30], Step [ 600/1875 ], d_loss: 1.1789, g_loss: 0.9135,g_score: 13.4015 \n",
      "Epoch [10/30], Step [ 700/1875 ], d_loss: 1.0069, g_loss: 0.9108,g_score: 13.5990 \n",
      "Epoch [10/30], Step [ 800/1875 ], d_loss: 1.1643, g_loss: 0.9916,g_score: 12.2574 \n",
      "Epoch [10/30], Step [ 900/1875 ], d_loss: 1.0887, g_loss: 0.9869,g_score: 12.1874 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Step [ 1000/1875 ], d_loss: 1.3536, g_loss: 0.7807,g_score: 15.1465 \n",
      "Epoch [10/30], Step [ 1100/1875 ], d_loss: 1.0701, g_loss: 0.8831,g_score: 13.9212 \n",
      "Epoch [10/30], Step [ 1200/1875 ], d_loss: 1.2262, g_loss: 1.0201,g_score: 11.8343 \n",
      "Epoch [10/30], Step [ 1300/1875 ], d_loss: 1.0367, g_loss: 0.9848,g_score: 12.3687 \n",
      "Epoch [10/30], Step [ 1400/1875 ], d_loss: 1.1286, g_loss: 1.1847,g_score: 10.1500 \n",
      "Epoch [10/30], Step [ 1500/1875 ], d_loss: 1.0415, g_loss: 1.0886,g_score: 11.1141 \n",
      "Epoch [10/30], Step [ 1600/1875 ], d_loss: 1.0901, g_loss: 0.9205,g_score: 13.3377 \n",
      "Epoch [10/30], Step [ 1700/1875 ], d_loss: 1.1539, g_loss: 1.0943,g_score: 11.0637 \n",
      "Epoch [10/30], Step [ 1800/1875 ], d_loss: 1.3559, g_loss: 0.9623,g_score: 12.6158 \n",
      "11\n",
      "Epoch [11/30], Step [ 0/1875 ], d_loss: 0.8901, g_loss: 1.0087,g_score: 11.9453 \n",
      "Epoch [11/30], Step [ 100/1875 ], d_loss: 1.0947, g_loss: 0.8846,g_score: 13.6369 \n",
      "Epoch [11/30], Step [ 200/1875 ], d_loss: 1.1695, g_loss: 1.1760,g_score: 10.1766 \n",
      "Epoch [11/30], Step [ 300/1875 ], d_loss: 0.8451, g_loss: 1.0522,g_score: 11.7144 \n",
      "Epoch [11/30], Step [ 400/1875 ], d_loss: 1.1271, g_loss: 1.1312,g_score: 10.9750 \n",
      "Epoch [11/30], Step [ 500/1875 ], d_loss: 1.1256, g_loss: 1.1657,g_score: 10.3031 \n",
      "Epoch [11/30], Step [ 600/1875 ], d_loss: 1.2249, g_loss: 0.9597,g_score: 13.0619 \n",
      "Epoch [11/30], Step [ 700/1875 ], d_loss: 1.3519, g_loss: 0.7951,g_score: 15.1715 \n",
      "Epoch [11/30], Step [ 800/1875 ], d_loss: 1.1278, g_loss: 0.9809,g_score: 12.5891 \n",
      "Epoch [11/30], Step [ 900/1875 ], d_loss: 1.2224, g_loss: 1.3637,g_score: 8.5830 \n",
      "Epoch [11/30], Step [ 1000/1875 ], d_loss: 1.0103, g_loss: 1.1483,g_score: 10.8043 \n",
      "Epoch [11/30], Step [ 1100/1875 ], d_loss: 0.9643, g_loss: 1.0432,g_score: 11.7990 \n",
      "Epoch [11/30], Step [ 1200/1875 ], d_loss: 1.1156, g_loss: 0.9954,g_score: 12.2916 \n",
      "Epoch [11/30], Step [ 1300/1875 ], d_loss: 0.9086, g_loss: 1.0283,g_score: 12.0145 \n",
      "Epoch [11/30], Step [ 1400/1875 ], d_loss: 0.9262, g_loss: 1.1603,g_score: 10.4188 \n",
      "Epoch [11/30], Step [ 1500/1875 ], d_loss: 1.1135, g_loss: 1.2064,g_score: 10.0175 \n",
      "Epoch [11/30], Step [ 1600/1875 ], d_loss: 1.0764, g_loss: 1.1580,g_score: 10.2608 \n",
      "Epoch [11/30], Step [ 1700/1875 ], d_loss: 0.7803, g_loss: 0.8804,g_score: 13.4997 \n",
      "Epoch [11/30], Step [ 1800/1875 ], d_loss: 0.9583, g_loss: 1.0040,g_score: 12.1018 \n",
      "12\n",
      "Epoch [12/30], Step [ 0/1875 ], d_loss: 1.2190, g_loss: 1.0876,g_score: 11.1272 \n",
      "Epoch [12/30], Step [ 100/1875 ], d_loss: 0.8042, g_loss: 0.9572,g_score: 12.4959 \n",
      "Epoch [12/30], Step [ 200/1875 ], d_loss: 1.0520, g_loss: 0.8335,g_score: 14.3210 \n",
      "Epoch [12/30], Step [ 300/1875 ], d_loss: 1.1473, g_loss: 0.9242,g_score: 13.4310 \n",
      "Epoch [12/30], Step [ 400/1875 ], d_loss: 0.9591, g_loss: 1.0214,g_score: 11.9061 \n",
      "Epoch [12/30], Step [ 500/1875 ], d_loss: 1.0824, g_loss: 1.2243,g_score: 9.6205 \n",
      "Epoch [12/30], Step [ 600/1875 ], d_loss: 1.1449, g_loss: 0.9346,g_score: 13.4982 \n",
      "Epoch [12/30], Step [ 700/1875 ], d_loss: 1.2185, g_loss: 1.0856,g_score: 11.2825 \n",
      "Epoch [12/30], Step [ 800/1875 ], d_loss: 1.0227, g_loss: 1.0151,g_score: 11.8746 \n",
      "Epoch [12/30], Step [ 900/1875 ], d_loss: 0.9150, g_loss: 1.1080,g_score: 11.0359 \n",
      "Epoch [12/30], Step [ 1000/1875 ], d_loss: 1.2027, g_loss: 0.9938,g_score: 12.2122 \n",
      "Epoch [12/30], Step [ 1100/1875 ], d_loss: 0.9936, g_loss: 0.9997,g_score: 12.1232 \n",
      "Epoch [12/30], Step [ 1200/1875 ], d_loss: 1.1953, g_loss: 1.3084,g_score: 8.8908 \n",
      "Epoch [12/30], Step [ 1300/1875 ], d_loss: 0.9463, g_loss: 0.9793,g_score: 12.5980 \n",
      "Epoch [12/30], Step [ 1400/1875 ], d_loss: 1.4412, g_loss: 1.1205,g_score: 11.1201 \n",
      "Epoch [12/30], Step [ 1500/1875 ], d_loss: 1.2361, g_loss: 0.9219,g_score: 13.5512 \n",
      "Epoch [12/30], Step [ 1600/1875 ], d_loss: 1.2445, g_loss: 1.0798,g_score: 11.6276 \n",
      "Epoch [12/30], Step [ 1700/1875 ], d_loss: 0.9309, g_loss: 0.9633,g_score: 12.4169 \n",
      "Epoch [12/30], Step [ 1800/1875 ], d_loss: 1.5645, g_loss: 0.7881,g_score: 15.6375 \n",
      "13\n",
      "Epoch [13/30], Step [ 0/1875 ], d_loss: 1.2343, g_loss: 1.1064,g_score: 10.9217 \n",
      "Epoch [13/30], Step [ 100/1875 ], d_loss: 1.0648, g_loss: 1.0813,g_score: 11.2028 \n",
      "Epoch [13/30], Step [ 200/1875 ], d_loss: 1.0834, g_loss: 1.1074,g_score: 10.9013 \n",
      "Epoch [13/30], Step [ 300/1875 ], d_loss: 1.0654, g_loss: 0.8251,g_score: 14.9063 \n",
      "Epoch [13/30], Step [ 400/1875 ], d_loss: 1.3724, g_loss: 0.9318,g_score: 12.9955 \n",
      "Epoch [13/30], Step [ 500/1875 ], d_loss: 1.0859, g_loss: 1.1195,g_score: 10.7599 \n",
      "Epoch [13/30], Step [ 600/1875 ], d_loss: 1.1174, g_loss: 1.0483,g_score: 11.6202 \n",
      "Epoch [13/30], Step [ 700/1875 ], d_loss: 1.3385, g_loss: 0.9698,g_score: 12.4708 \n",
      "Epoch [13/30], Step [ 800/1875 ], d_loss: 0.8899, g_loss: 0.9381,g_score: 13.0582 \n",
      "Epoch [13/30], Step [ 900/1875 ], d_loss: 1.2515, g_loss: 0.9896,g_score: 12.6586 \n",
      "Epoch [13/30], Step [ 1000/1875 ], d_loss: 1.1996, g_loss: 1.4117,g_score: 8.1208 \n",
      "Epoch [13/30], Step [ 1100/1875 ], d_loss: 1.0839, g_loss: 1.0458,g_score: 11.7203 \n",
      "Epoch [13/30], Step [ 1200/1875 ], d_loss: 1.2093, g_loss: 1.1142,g_score: 10.8657 \n",
      "Epoch [13/30], Step [ 1300/1875 ], d_loss: 1.0013, g_loss: 1.2006,g_score: 9.9875 \n",
      "Epoch [13/30], Step [ 1400/1875 ], d_loss: 1.1087, g_loss: 1.0092,g_score: 12.1214 \n",
      "Epoch [13/30], Step [ 1500/1875 ], d_loss: 1.1791, g_loss: 0.8017,g_score: 15.1719 \n",
      "Epoch [13/30], Step [ 1600/1875 ], d_loss: 1.1332, g_loss: 1.0988,g_score: 10.9430 \n",
      "Epoch [13/30], Step [ 1700/1875 ], d_loss: 0.8064, g_loss: 0.9553,g_score: 12.8269 \n",
      "Epoch [13/30], Step [ 1800/1875 ], d_loss: 1.0772, g_loss: 1.0488,g_score: 11.6069 \n",
      "14\n",
      "Epoch [14/30], Step [ 0/1875 ], d_loss: 1.0088, g_loss: 1.3602,g_score: 8.6120 \n",
      "Epoch [14/30], Step [ 100/1875 ], d_loss: 0.9359, g_loss: 1.0040,g_score: 12.0699 \n",
      "Epoch [14/30], Step [ 200/1875 ], d_loss: 1.2147, g_loss: 0.9240,g_score: 13.2277 \n",
      "Epoch [14/30], Step [ 300/1875 ], d_loss: 1.2825, g_loss: 1.2389,g_score: 9.5268 \n",
      "Epoch [14/30], Step [ 400/1875 ], d_loss: 1.0198, g_loss: 1.0632,g_score: 11.4305 \n",
      "Epoch [14/30], Step [ 500/1875 ], d_loss: 1.5065, g_loss: 0.7750,g_score: 15.5720 \n",
      "Epoch [14/30], Step [ 600/1875 ], d_loss: 1.0679, g_loss: 0.8106,g_score: 14.7557 \n",
      "Epoch [14/30], Step [ 700/1875 ], d_loss: 1.1004, g_loss: 1.0655,g_score: 11.3396 \n",
      "Epoch [14/30], Step [ 800/1875 ], d_loss: 1.0482, g_loss: 0.8169,g_score: 14.5519 \n",
      "Epoch [14/30], Step [ 900/1875 ], d_loss: 0.7689, g_loss: 1.1510,g_score: 10.4574 \n",
      "Epoch [14/30], Step [ 1000/1875 ], d_loss: 1.4071, g_loss: 1.0753,g_score: 11.5063 \n",
      "Epoch [14/30], Step [ 1100/1875 ], d_loss: 1.3744, g_loss: 0.9273,g_score: 13.6286 \n",
      "Epoch [14/30], Step [ 1200/1875 ], d_loss: 1.0725, g_loss: 1.0454,g_score: 11.8184 \n",
      "Epoch [14/30], Step [ 1300/1875 ], d_loss: 1.3046, g_loss: 1.0341,g_score: 11.7178 \n",
      "Epoch [14/30], Step [ 1400/1875 ], d_loss: 0.9814, g_loss: 1.1980,g_score: 9.9570 \n",
      "Epoch [14/30], Step [ 1500/1875 ], d_loss: 1.2353, g_loss: 0.8549,g_score: 14.1226 \n",
      "Epoch [14/30], Step [ 1600/1875 ], d_loss: 1.2803, g_loss: 0.9138,g_score: 13.5073 \n",
      "Epoch [14/30], Step [ 1700/1875 ], d_loss: 1.3084, g_loss: 0.8158,g_score: 15.0673 \n",
      "Epoch [14/30], Step [ 1800/1875 ], d_loss: 1.4775, g_loss: 1.3184,g_score: 8.9587 \n",
      "15\n",
      "Epoch [15/30], Step [ 0/1875 ], d_loss: 1.2724, g_loss: 0.8284,g_score: 14.8183 \n",
      "Epoch [15/30], Step [ 100/1875 ], d_loss: 1.1804, g_loss: 0.7722,g_score: 15.3960 \n",
      "Epoch [15/30], Step [ 200/1875 ], d_loss: 1.2896, g_loss: 0.7872,g_score: 15.0640 \n",
      "Epoch [15/30], Step [ 300/1875 ], d_loss: 0.9307, g_loss: 0.8750,g_score: 13.8004 \n",
      "Epoch [15/30], Step [ 400/1875 ], d_loss: 1.1758, g_loss: 1.1142,g_score: 11.2737 \n",
      "Epoch [15/30], Step [ 500/1875 ], d_loss: 1.2424, g_loss: 1.0327,g_score: 11.9178 \n",
      "Epoch [15/30], Step [ 600/1875 ], d_loss: 0.9715, g_loss: 0.9852,g_score: 12.2112 \n",
      "Epoch [15/30], Step [ 700/1875 ], d_loss: 1.0573, g_loss: 1.2031,g_score: 10.0429 \n",
      "Epoch [15/30], Step [ 800/1875 ], d_loss: 1.3601, g_loss: 0.9829,g_score: 12.5012 \n",
      "Epoch [15/30], Step [ 900/1875 ], d_loss: 0.9572, g_loss: 0.9705,g_score: 12.4432 \n",
      "Epoch [15/30], Step [ 1000/1875 ], d_loss: 1.2440, g_loss: 1.0843,g_score: 11.7344 \n",
      "Epoch [15/30], Step [ 1100/1875 ], d_loss: 1.1191, g_loss: 0.9622,g_score: 12.5843 \n",
      "Epoch [15/30], Step [ 1200/1875 ], d_loss: 0.9650, g_loss: 1.1355,g_score: 10.7585 \n",
      "Epoch [15/30], Step [ 1300/1875 ], d_loss: 1.2499, g_loss: 0.8722,g_score: 14.0524 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Step [ 1400/1875 ], d_loss: 1.0546, g_loss: 0.8288,g_score: 14.6972 \n",
      "Epoch [15/30], Step [ 1500/1875 ], d_loss: 1.2270, g_loss: 1.0522,g_score: 11.8760 \n",
      "Epoch [15/30], Step [ 1600/1875 ], d_loss: 1.2507, g_loss: 0.8204,g_score: 14.6905 \n",
      "Epoch [15/30], Step [ 1700/1875 ], d_loss: 1.2127, g_loss: 1.3229,g_score: 9.2153 \n",
      "Epoch [15/30], Step [ 1800/1875 ], d_loss: 1.2614, g_loss: 0.9902,g_score: 12.4093 \n",
      "16\n",
      "Epoch [16/30], Step [ 0/1875 ], d_loss: 1.1819, g_loss: 1.1871,g_score: 10.1158 \n",
      "Epoch [16/30], Step [ 100/1875 ], d_loss: 1.0144, g_loss: 1.2864,g_score: 9.1932 \n",
      "Epoch [16/30], Step [ 200/1875 ], d_loss: 0.9959, g_loss: 1.2088,g_score: 9.9820 \n",
      "Epoch [16/30], Step [ 300/1875 ], d_loss: 1.2694, g_loss: 1.0965,g_score: 11.0932 \n",
      "Epoch [16/30], Step [ 400/1875 ], d_loss: 1.1965, g_loss: 0.8323,g_score: 14.7691 \n",
      "Epoch [16/30], Step [ 500/1875 ], d_loss: 1.1558, g_loss: 1.1135,g_score: 10.9030 \n",
      "Epoch [16/30], Step [ 600/1875 ], d_loss: 1.4083, g_loss: 1.0783,g_score: 11.4568 \n",
      "Epoch [16/30], Step [ 700/1875 ], d_loss: 0.9383, g_loss: 1.0817,g_score: 11.2955 \n",
      "Epoch [16/30], Step [ 800/1875 ], d_loss: 1.2344, g_loss: 0.9794,g_score: 13.0831 \n",
      "Epoch [16/30], Step [ 900/1875 ], d_loss: 1.2163, g_loss: 0.9820,g_score: 12.4347 \n",
      "Epoch [16/30], Step [ 1000/1875 ], d_loss: 1.3014, g_loss: 1.0945,g_score: 11.3986 \n",
      "Epoch [16/30], Step [ 1100/1875 ], d_loss: 1.2545, g_loss: 1.0524,g_score: 11.7343 \n",
      "Epoch [16/30], Step [ 1200/1875 ], d_loss: 0.9705, g_loss: 1.1376,g_score: 10.6282 \n",
      "Epoch [16/30], Step [ 1300/1875 ], d_loss: 1.2339, g_loss: 1.1099,g_score: 10.9108 \n",
      "Epoch [16/30], Step [ 1400/1875 ], d_loss: 0.9935, g_loss: 0.8521,g_score: 14.0420 \n",
      "Epoch [16/30], Step [ 1500/1875 ], d_loss: 1.0060, g_loss: 1.0371,g_score: 11.8508 \n",
      "Epoch [16/30], Step [ 1600/1875 ], d_loss: 1.2748, g_loss: 1.0180,g_score: 12.1885 \n",
      "Epoch [16/30], Step [ 1700/1875 ], d_loss: 0.8622, g_loss: 1.0200,g_score: 11.9650 \n",
      "Epoch [16/30], Step [ 1800/1875 ], d_loss: 1.1225, g_loss: 0.8155,g_score: 14.4743 \n",
      "17\n",
      "Epoch [17/30], Step [ 0/1875 ], d_loss: 1.0309, g_loss: 1.0787,g_score: 11.4717 \n",
      "Epoch [17/30], Step [ 100/1875 ], d_loss: 1.0049, g_loss: 1.1432,g_score: 10.8401 \n",
      "Epoch [17/30], Step [ 200/1875 ], d_loss: 1.2342, g_loss: 0.8564,g_score: 14.6141 \n",
      "Epoch [17/30], Step [ 300/1875 ], d_loss: 1.0833, g_loss: 0.8711,g_score: 14.0005 \n",
      "Epoch [17/30], Step [ 400/1875 ], d_loss: 1.2841, g_loss: 1.0832,g_score: 11.2056 \n",
      "Epoch [17/30], Step [ 500/1875 ], d_loss: 1.2574, g_loss: 1.1736,g_score: 10.2296 \n",
      "Epoch [17/30], Step [ 600/1875 ], d_loss: 1.1132, g_loss: 1.3064,g_score: 9.0682 \n",
      "Epoch [17/30], Step [ 700/1875 ], d_loss: 1.1842, g_loss: 0.9833,g_score: 12.5833 \n",
      "Epoch [17/30], Step [ 800/1875 ], d_loss: 1.1563, g_loss: 1.0426,g_score: 11.7485 \n",
      "Epoch [17/30], Step [ 900/1875 ], d_loss: 1.2681, g_loss: 0.9013,g_score: 13.8956 \n",
      "Epoch [17/30], Step [ 1000/1875 ], d_loss: 1.0243, g_loss: 0.9392,g_score: 13.1115 \n",
      "Epoch [17/30], Step [ 1100/1875 ], d_loss: 1.2602, g_loss: 0.8901,g_score: 13.7082 \n",
      "Epoch [17/30], Step [ 1200/1875 ], d_loss: 1.0016, g_loss: 1.3272,g_score: 8.7652 \n",
      "Epoch [17/30], Step [ 1300/1875 ], d_loss: 1.1949, g_loss: 1.0873,g_score: 11.1247 \n",
      "Epoch [17/30], Step [ 1400/1875 ], d_loss: 0.9996, g_loss: 1.0074,g_score: 12.2779 \n",
      "Epoch [17/30], Step [ 1500/1875 ], d_loss: 0.9019, g_loss: 0.9675,g_score: 12.5158 \n",
      "Epoch [17/30], Step [ 1600/1875 ], d_loss: 1.2590, g_loss: 1.0806,g_score: 11.4155 \n",
      "Epoch [17/30], Step [ 1700/1875 ], d_loss: 1.3117, g_loss: 1.0655,g_score: 11.8779 \n",
      "Epoch [17/30], Step [ 1800/1875 ], d_loss: 1.0028, g_loss: 1.2516,g_score: 9.6785 \n",
      "18\n",
      "Epoch [18/30], Step [ 0/1875 ], d_loss: 1.0387, g_loss: 0.9674,g_score: 12.5826 \n",
      "Epoch [18/30], Step [ 100/1875 ], d_loss: 1.2197, g_loss: 0.8338,g_score: 14.4048 \n",
      "Epoch [18/30], Step [ 200/1875 ], d_loss: 1.1421, g_loss: 0.8878,g_score: 13.8566 \n",
      "Epoch [18/30], Step [ 300/1875 ], d_loss: 0.9933, g_loss: 0.9323,g_score: 13.2417 \n",
      "Epoch [18/30], Step [ 400/1875 ], d_loss: 1.3994, g_loss: 1.1749,g_score: 10.3143 \n",
      "Epoch [18/30], Step [ 500/1875 ], d_loss: 0.9380, g_loss: 0.9586,g_score: 12.5951 \n",
      "Epoch [18/30], Step [ 600/1875 ], d_loss: 1.3009, g_loss: 1.0793,g_score: 11.7345 \n",
      "Epoch [18/30], Step [ 700/1875 ], d_loss: 1.1347, g_loss: 0.8903,g_score: 13.5912 \n",
      "Epoch [18/30], Step [ 800/1875 ], d_loss: 0.8547, g_loss: 1.0253,g_score: 11.8466 \n",
      "Epoch [18/30], Step [ 900/1875 ], d_loss: 1.3835, g_loss: 1.0806,g_score: 11.6951 \n",
      "Epoch [18/30], Step [ 1000/1875 ], d_loss: 1.1429, g_loss: 0.8613,g_score: 14.2317 \n",
      "Epoch [18/30], Step [ 1100/1875 ], d_loss: 1.1806, g_loss: 0.8840,g_score: 14.2893 \n",
      "Epoch [18/30], Step [ 1200/1875 ], d_loss: 1.2494, g_loss: 1.1903,g_score: 10.3242 \n",
      "Epoch [18/30], Step [ 1300/1875 ], d_loss: 1.1304, g_loss: 1.2466,g_score: 9.5964 \n",
      "Epoch [18/30], Step [ 1400/1875 ], d_loss: 1.1109, g_loss: 1.1894,g_score: 10.1233 \n",
      "Epoch [18/30], Step [ 1500/1875 ], d_loss: 1.1483, g_loss: 1.1124,g_score: 10.8168 \n",
      "Epoch [18/30], Step [ 1600/1875 ], d_loss: 1.3190, g_loss: 0.8864,g_score: 14.2556 \n",
      "Epoch [18/30], Step [ 1700/1875 ], d_loss: 0.9997, g_loss: 1.1247,g_score: 10.9733 \n",
      "Epoch [18/30], Step [ 1800/1875 ], d_loss: 0.9219, g_loss: 1.2460,g_score: 9.4936 \n",
      "19\n",
      "Epoch [19/30], Step [ 0/1875 ], d_loss: 1.1057, g_loss: 0.9762,g_score: 12.7250 \n",
      "Epoch [19/30], Step [ 100/1875 ], d_loss: 1.2691, g_loss: 1.1356,g_score: 10.6606 \n",
      "Epoch [19/30], Step [ 200/1875 ], d_loss: 0.8638, g_loss: 1.0712,g_score: 11.4778 \n",
      "Epoch [19/30], Step [ 300/1875 ], d_loss: 1.2805, g_loss: 1.2153,g_score: 9.8455 \n",
      "Epoch [19/30], Step [ 400/1875 ], d_loss: 1.1542, g_loss: 0.9701,g_score: 12.5972 \n",
      "Epoch [19/30], Step [ 500/1875 ], d_loss: 1.0148, g_loss: 1.0460,g_score: 12.0640 \n",
      "Epoch [19/30], Step [ 600/1875 ], d_loss: 1.0206, g_loss: 0.9527,g_score: 12.7880 \n",
      "Epoch [19/30], Step [ 700/1875 ], d_loss: 1.1819, g_loss: 0.9660,g_score: 12.6705 \n",
      "Epoch [19/30], Step [ 800/1875 ], d_loss: 1.0622, g_loss: 1.1494,g_score: 10.5896 \n",
      "Epoch [19/30], Step [ 900/1875 ], d_loss: 1.2294, g_loss: 0.9931,g_score: 12.5003 \n",
      "Epoch [19/30], Step [ 1000/1875 ], d_loss: 0.8599, g_loss: 0.9861,g_score: 12.1739 \n",
      "Epoch [19/30], Step [ 1100/1875 ], d_loss: 1.1205, g_loss: 1.0609,g_score: 11.6169 \n",
      "Epoch [19/30], Step [ 1200/1875 ], d_loss: 1.1383, g_loss: 1.0018,g_score: 12.6742 \n",
      "Epoch [19/30], Step [ 1300/1875 ], d_loss: 1.0476, g_loss: 0.9496,g_score: 12.8532 \n",
      "Epoch [19/30], Step [ 1400/1875 ], d_loss: 1.2223, g_loss: 1.0047,g_score: 12.2309 \n",
      "Epoch [19/30], Step [ 1500/1875 ], d_loss: 1.0145, g_loss: 0.9632,g_score: 12.7821 \n",
      "Epoch [19/30], Step [ 1600/1875 ], d_loss: 1.2175, g_loss: 0.9062,g_score: 13.4909 \n",
      "Epoch [19/30], Step [ 1700/1875 ], d_loss: 1.0604, g_loss: 0.9805,g_score: 12.5321 \n",
      "Epoch [19/30], Step [ 1800/1875 ], d_loss: 0.8608, g_loss: 1.0118,g_score: 11.9012 \n",
      "20\n",
      "Epoch [20/30], Step [ 0/1875 ], d_loss: 1.1133, g_loss: 1.0910,g_score: 11.4527 \n",
      "Epoch [20/30], Step [ 100/1875 ], d_loss: 1.1172, g_loss: 0.9621,g_score: 13.0225 \n",
      "Epoch [20/30], Step [ 200/1875 ], d_loss: 0.9054, g_loss: 1.0468,g_score: 11.8894 \n",
      "Epoch [20/30], Step [ 300/1875 ], d_loss: 1.3063, g_loss: 1.1266,g_score: 11.1399 \n",
      "Epoch [20/30], Step [ 400/1875 ], d_loss: 1.0865, g_loss: 1.0606,g_score: 11.3057 \n",
      "Epoch [20/30], Step [ 500/1875 ], d_loss: 1.0289, g_loss: 0.9426,g_score: 12.9225 \n",
      "Epoch [20/30], Step [ 600/1875 ], d_loss: 1.1657, g_loss: 0.9090,g_score: 13.4916 \n",
      "Epoch [20/30], Step [ 700/1875 ], d_loss: 1.1827, g_loss: 0.8941,g_score: 13.6640 \n",
      "Epoch [20/30], Step [ 800/1875 ], d_loss: 0.9527, g_loss: 1.1086,g_score: 10.9776 \n",
      "Epoch [20/30], Step [ 900/1875 ], d_loss: 1.3996, g_loss: 0.9291,g_score: 13.7643 \n",
      "Epoch [20/30], Step [ 1000/1875 ], d_loss: 1.2191, g_loss: 0.9323,g_score: 13.3317 \n",
      "Epoch [20/30], Step [ 1100/1875 ], d_loss: 1.1523, g_loss: 0.9297,g_score: 13.6782 \n",
      "Epoch [20/30], Step [ 1200/1875 ], d_loss: 1.3227, g_loss: 0.9773,g_score: 12.4304 \n",
      "Epoch [20/30], Step [ 1300/1875 ], d_loss: 0.9223, g_loss: 0.8278,g_score: 14.6950 \n",
      "Epoch [20/30], Step [ 1400/1875 ], d_loss: 1.2702, g_loss: 1.0069,g_score: 12.3244 \n",
      "Epoch [20/30], Step [ 1500/1875 ], d_loss: 0.9849, g_loss: 1.0835,g_score: 11.2095 \n",
      "Epoch [20/30], Step [ 1600/1875 ], d_loss: 1.0792, g_loss: 0.8660,g_score: 14.3472 \n",
      "Epoch [20/30], Step [ 1700/1875 ], d_loss: 1.0265, g_loss: 1.2190,g_score: 9.7965 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Step [ 1800/1875 ], d_loss: 1.0918, g_loss: 0.9861,g_score: 12.4588 \n",
      "21\n",
      "Epoch [21/30], Step [ 0/1875 ], d_loss: 1.1650, g_loss: 1.2157,g_score: 9.9013 \n",
      "Epoch [21/30], Step [ 100/1875 ], d_loss: 1.1486, g_loss: 1.0856,g_score: 11.5645 \n",
      "Epoch [21/30], Step [ 200/1875 ], d_loss: 1.3150, g_loss: 0.8565,g_score: 14.4908 \n",
      "Epoch [21/30], Step [ 300/1875 ], d_loss: 0.9272, g_loss: 1.0630,g_score: 11.4599 \n",
      "Epoch [21/30], Step [ 400/1875 ], d_loss: 1.0621, g_loss: 1.0453,g_score: 11.7565 \n",
      "Epoch [21/30], Step [ 500/1875 ], d_loss: 1.1557, g_loss: 1.0082,g_score: 12.1908 \n",
      "Epoch [21/30], Step [ 600/1875 ], d_loss: 1.1149, g_loss: 0.9123,g_score: 13.6039 \n",
      "Epoch [21/30], Step [ 700/1875 ], d_loss: 1.2361, g_loss: 1.2158,g_score: 10.0037 \n",
      "Epoch [21/30], Step [ 800/1875 ], d_loss: 1.0445, g_loss: 0.8251,g_score: 14.6646 \n",
      "Epoch [21/30], Step [ 900/1875 ], d_loss: 1.0241, g_loss: 0.9714,g_score: 12.7710 \n",
      "Epoch [21/30], Step [ 1000/1875 ], d_loss: 1.4206, g_loss: 1.3285,g_score: 8.9272 \n",
      "Epoch [21/30], Step [ 1100/1875 ], d_loss: 1.1681, g_loss: 0.8669,g_score: 14.4343 \n",
      "Epoch [21/30], Step [ 1200/1875 ], d_loss: 0.9937, g_loss: 0.9842,g_score: 12.5787 \n",
      "Epoch [21/30], Step [ 1300/1875 ], d_loss: 1.3215, g_loss: 1.1099,g_score: 11.1806 \n",
      "Epoch [21/30], Step [ 1400/1875 ], d_loss: 1.1472, g_loss: 0.9916,g_score: 12.2564 \n",
      "Epoch [21/30], Step [ 1500/1875 ], d_loss: 0.8751, g_loss: 0.9011,g_score: 13.5884 \n",
      "Epoch [21/30], Step [ 1600/1875 ], d_loss: 1.0738, g_loss: 1.2012,g_score: 10.2430 \n",
      "Epoch [21/30], Step [ 1700/1875 ], d_loss: 1.2249, g_loss: 1.4818,g_score: 7.6263 \n",
      "Epoch [21/30], Step [ 1800/1875 ], d_loss: 1.0290, g_loss: 1.0931,g_score: 11.3226 \n",
      "22\n",
      "Epoch [22/30], Step [ 0/1875 ], d_loss: 1.1883, g_loss: 1.1652,g_score: 10.2292 \n",
      "Epoch [22/30], Step [ 100/1875 ], d_loss: 1.0679, g_loss: 1.0612,g_score: 11.3857 \n",
      "Epoch [22/30], Step [ 200/1875 ], d_loss: 1.0176, g_loss: 1.0316,g_score: 11.8112 \n",
      "Epoch [22/30], Step [ 300/1875 ], d_loss: 1.1716, g_loss: 0.9320,g_score: 13.1342 \n",
      "Epoch [22/30], Step [ 400/1875 ], d_loss: 1.1798, g_loss: 0.8505,g_score: 14.3341 \n",
      "Epoch [22/30], Step [ 500/1875 ], d_loss: 1.0543, g_loss: 0.9008,g_score: 13.5712 \n",
      "Epoch [22/30], Step [ 600/1875 ], d_loss: 1.3995, g_loss: 1.0080,g_score: 12.1475 \n",
      "Epoch [22/30], Step [ 700/1875 ], d_loss: 1.1192, g_loss: 0.9212,g_score: 13.1359 \n",
      "Epoch [22/30], Step [ 800/1875 ], d_loss: 1.2642, g_loss: 1.0974,g_score: 11.1208 \n",
      "Epoch [22/30], Step [ 900/1875 ], d_loss: 1.0948, g_loss: 0.9157,g_score: 13.5057 \n",
      "Epoch [22/30], Step [ 1000/1875 ], d_loss: 1.2453, g_loss: 0.8414,g_score: 14.5520 \n",
      "Epoch [22/30], Step [ 1100/1875 ], d_loss: 1.4099, g_loss: 1.0639,g_score: 11.3183 \n",
      "Epoch [22/30], Step [ 1200/1875 ], d_loss: 0.9952, g_loss: 1.0505,g_score: 11.4896 \n",
      "Epoch [22/30], Step [ 1300/1875 ], d_loss: 1.1518, g_loss: 0.9903,g_score: 12.3820 \n",
      "Epoch [22/30], Step [ 1400/1875 ], d_loss: 1.3928, g_loss: 1.0515,g_score: 11.5135 \n",
      "Epoch [22/30], Step [ 1500/1875 ], d_loss: 1.0687, g_loss: 0.9711,g_score: 12.8824 \n",
      "Epoch [22/30], Step [ 1600/1875 ], d_loss: 0.9683, g_loss: 0.8484,g_score: 14.2513 \n",
      "Epoch [22/30], Step [ 1700/1875 ], d_loss: 1.0714, g_loss: 0.9857,g_score: 12.1813 \n",
      "Epoch [22/30], Step [ 1800/1875 ], d_loss: 1.5294, g_loss: 0.7629,g_score: 16.0540 \n",
      "23\n",
      "Epoch [23/30], Step [ 0/1875 ], d_loss: 1.1838, g_loss: 0.9875,g_score: 12.2795 \n",
      "Epoch [23/30], Step [ 100/1875 ], d_loss: 0.8163, g_loss: 0.9810,g_score: 12.7903 \n",
      "Epoch [23/30], Step [ 200/1875 ], d_loss: 1.1568, g_loss: 1.0788,g_score: 11.4816 \n",
      "Epoch [23/30], Step [ 300/1875 ], d_loss: 1.1020, g_loss: 0.9962,g_score: 12.3359 \n",
      "Epoch [23/30], Step [ 400/1875 ], d_loss: 1.1044, g_loss: 1.1292,g_score: 10.7936 \n",
      "Epoch [23/30], Step [ 500/1875 ], d_loss: 1.0622, g_loss: 1.1931,g_score: 10.0965 \n",
      "Epoch [23/30], Step [ 600/1875 ], d_loss: 1.3746, g_loss: 0.9913,g_score: 12.1066 \n",
      "Epoch [23/30], Step [ 700/1875 ], d_loss: 1.0402, g_loss: 0.8993,g_score: 13.6773 \n",
      "Epoch [23/30], Step [ 800/1875 ], d_loss: 1.1236, g_loss: 1.0686,g_score: 11.6516 \n",
      "Epoch [23/30], Step [ 900/1875 ], d_loss: 1.2337, g_loss: 0.9234,g_score: 13.2708 \n",
      "Epoch [23/30], Step [ 1000/1875 ], d_loss: 1.3913, g_loss: 1.0034,g_score: 12.5514 \n",
      "Epoch [23/30], Step [ 1100/1875 ], d_loss: 1.1466, g_loss: 1.0824,g_score: 11.4526 \n",
      "Epoch [23/30], Step [ 1200/1875 ], d_loss: 1.3810, g_loss: 1.0864,g_score: 11.3508 \n",
      "Epoch [23/30], Step [ 1300/1875 ], d_loss: 1.1424, g_loss: 1.0941,g_score: 11.5318 \n",
      "Epoch [23/30], Step [ 1400/1875 ], d_loss: 1.3895, g_loss: 1.1861,g_score: 10.3004 \n",
      "Epoch [23/30], Step [ 1500/1875 ], d_loss: 1.1019, g_loss: 0.9254,g_score: 13.1260 \n",
      "Epoch [23/30], Step [ 1600/1875 ], d_loss: 0.8203, g_loss: 0.9744,g_score: 12.6733 \n",
      "Epoch [23/30], Step [ 1700/1875 ], d_loss: 1.3732, g_loss: 1.1038,g_score: 11.3432 \n",
      "Epoch [23/30], Step [ 1800/1875 ], d_loss: 1.1758, g_loss: 1.0891,g_score: 11.3638 \n",
      "24\n",
      "Epoch [24/30], Step [ 0/1875 ], d_loss: 1.0350, g_loss: 0.8362,g_score: 14.6132 \n",
      "Epoch [24/30], Step [ 100/1875 ], d_loss: 1.4999, g_loss: 1.0594,g_score: 12.0130 \n",
      "Epoch [24/30], Step [ 200/1875 ], d_loss: 1.1663, g_loss: 0.8966,g_score: 13.8677 \n",
      "Epoch [24/30], Step [ 300/1875 ], d_loss: 0.9732, g_loss: 1.0617,g_score: 11.4504 \n",
      "Epoch [24/30], Step [ 400/1875 ], d_loss: 1.3586, g_loss: 1.1067,g_score: 10.9264 \n",
      "Epoch [24/30], Step [ 500/1875 ], d_loss: 1.2010, g_loss: 1.1109,g_score: 11.2970 \n",
      "Epoch [24/30], Step [ 600/1875 ], d_loss: 0.9293, g_loss: 0.8795,g_score: 13.7976 \n",
      "Epoch [24/30], Step [ 700/1875 ], d_loss: 0.9630, g_loss: 1.1741,g_score: 10.0857 \n",
      "Epoch [24/30], Step [ 800/1875 ], d_loss: 1.3286, g_loss: 0.7883,g_score: 15.4042 \n",
      "Epoch [24/30], Step [ 900/1875 ], d_loss: 0.9461, g_loss: 0.9187,g_score: 13.5553 \n",
      "Epoch [24/30], Step [ 1000/1875 ], d_loss: 1.2399, g_loss: 1.1277,g_score: 10.8051 \n",
      "Epoch [24/30], Step [ 1100/1875 ], d_loss: 1.1682, g_loss: 1.1151,g_score: 11.3799 \n",
      "Epoch [24/30], Step [ 1200/1875 ], d_loss: 1.5637, g_loss: 1.1267,g_score: 11.2696 \n",
      "Epoch [24/30], Step [ 1300/1875 ], d_loss: 1.1663, g_loss: 1.1381,g_score: 11.2296 \n",
      "Epoch [24/30], Step [ 1400/1875 ], d_loss: 1.0808, g_loss: 0.9299,g_score: 13.3083 \n",
      "Epoch [24/30], Step [ 1500/1875 ], d_loss: 1.0995, g_loss: 1.1070,g_score: 10.9206 \n",
      "Epoch [24/30], Step [ 1600/1875 ], d_loss: 1.1870, g_loss: 0.9524,g_score: 13.0768 \n",
      "Epoch [24/30], Step [ 1700/1875 ], d_loss: 1.2456, g_loss: 1.0192,g_score: 12.2148 \n",
      "Epoch [24/30], Step [ 1800/1875 ], d_loss: 0.8858, g_loss: 0.9903,g_score: 12.2013 \n",
      "25\n",
      "Epoch [25/30], Step [ 0/1875 ], d_loss: 0.9276, g_loss: 1.0412,g_score: 11.6191 \n",
      "Epoch [25/30], Step [ 100/1875 ], d_loss: 1.1170, g_loss: 0.8918,g_score: 13.8718 \n",
      "Epoch [25/30], Step [ 200/1875 ], d_loss: 1.1098, g_loss: 0.8982,g_score: 13.7227 \n",
      "Epoch [25/30], Step [ 300/1875 ], d_loss: 1.2448, g_loss: 0.8527,g_score: 14.5912 \n",
      "Epoch [25/30], Step [ 400/1875 ], d_loss: 1.1835, g_loss: 0.9123,g_score: 13.4812 \n",
      "Epoch [25/30], Step [ 500/1875 ], d_loss: 1.2891, g_loss: 1.0112,g_score: 12.3810 \n",
      "Epoch [25/30], Step [ 600/1875 ], d_loss: 1.2036, g_loss: 1.0810,g_score: 11.4263 \n",
      "Epoch [25/30], Step [ 700/1875 ], d_loss: 0.8833, g_loss: 1.0442,g_score: 11.7597 \n",
      "Epoch [25/30], Step [ 800/1875 ], d_loss: 1.0885, g_loss: 0.7868,g_score: 15.2290 \n",
      "Epoch [25/30], Step [ 900/1875 ], d_loss: 1.2770, g_loss: 0.8974,g_score: 13.5959 \n",
      "Epoch [25/30], Step [ 1000/1875 ], d_loss: 1.0178, g_loss: 1.1436,g_score: 10.5719 \n",
      "Epoch [25/30], Step [ 1100/1875 ], d_loss: 1.2982, g_loss: 0.8570,g_score: 14.2810 \n",
      "Epoch [25/30], Step [ 1200/1875 ], d_loss: 1.0474, g_loss: 0.9926,g_score: 12.1523 \n",
      "Epoch [25/30], Step [ 1300/1875 ], d_loss: 1.2205, g_loss: 1.0041,g_score: 12.3599 \n",
      "Epoch [25/30], Step [ 1400/1875 ], d_loss: 0.8082, g_loss: 1.1523,g_score: 10.5898 \n",
      "Epoch [25/30], Step [ 1500/1875 ], d_loss: 1.2434, g_loss: 0.8909,g_score: 13.7809 \n",
      "Epoch [25/30], Step [ 1600/1875 ], d_loss: 0.9095, g_loss: 0.9173,g_score: 13.3668 \n",
      "Epoch [25/30], Step [ 1700/1875 ], d_loss: 1.3246, g_loss: 1.0873,g_score: 11.5430 \n",
      "Epoch [25/30], Step [ 1800/1875 ], d_loss: 1.1875, g_loss: 1.1376,g_score: 10.5119 \n",
      "26\n",
      "Epoch [26/30], Step [ 0/1875 ], d_loss: 1.3034, g_loss: 0.9515,g_score: 12.6083 \n",
      "Epoch [26/30], Step [ 100/1875 ], d_loss: 1.0794, g_loss: 0.8201,g_score: 14.7026 \n",
      "Epoch [26/30], Step [ 200/1875 ], d_loss: 1.0443, g_loss: 1.0178,g_score: 11.9585 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Step [ 300/1875 ], d_loss: 1.2559, g_loss: 1.0981,g_score: 11.2487 \n",
      "Epoch [26/30], Step [ 400/1875 ], d_loss: 1.0428, g_loss: 1.0741,g_score: 11.3999 \n",
      "Epoch [26/30], Step [ 500/1875 ], d_loss: 1.0470, g_loss: 0.9025,g_score: 13.5940 \n",
      "Epoch [26/30], Step [ 600/1875 ], d_loss: 1.1268, g_loss: 0.8658,g_score: 14.1333 \n",
      "Epoch [26/30], Step [ 700/1875 ], d_loss: 1.2534, g_loss: 0.9416,g_score: 13.1376 \n",
      "Epoch [26/30], Step [ 800/1875 ], d_loss: 1.1337, g_loss: 1.1435,g_score: 10.5669 \n",
      "Epoch [26/30], Step [ 900/1875 ], d_loss: 1.0252, g_loss: 0.9995,g_score: 12.1755 \n",
      "Epoch [26/30], Step [ 1000/1875 ], d_loss: 0.9535, g_loss: 1.0692,g_score: 11.3104 \n",
      "Epoch [26/30], Step [ 1100/1875 ], d_loss: 1.2052, g_loss: 1.0011,g_score: 12.1948 \n",
      "Epoch [26/30], Step [ 1200/1875 ], d_loss: 1.0671, g_loss: 1.0886,g_score: 11.6981 \n",
      "Epoch [26/30], Step [ 1300/1875 ], d_loss: 1.1397, g_loss: 1.3589,g_score: 8.7615 \n",
      "Epoch [26/30], Step [ 1400/1875 ], d_loss: 1.2562, g_loss: 0.8787,g_score: 14.0373 \n",
      "Epoch [26/30], Step [ 1500/1875 ], d_loss: 1.0755, g_loss: 1.0171,g_score: 12.1438 \n",
      "Epoch [26/30], Step [ 1600/1875 ], d_loss: 1.1555, g_loss: 1.1886,g_score: 10.1652 \n",
      "Epoch [26/30], Step [ 1700/1875 ], d_loss: 0.9707, g_loss: 1.2705,g_score: 9.3951 \n",
      "Epoch [26/30], Step [ 1800/1875 ], d_loss: 1.1239, g_loss: 0.9953,g_score: 12.2825 \n",
      "27\n",
      "Epoch [27/30], Step [ 0/1875 ], d_loss: 1.1103, g_loss: 0.8930,g_score: 13.6378 \n",
      "Epoch [27/30], Step [ 100/1875 ], d_loss: 1.1739, g_loss: 1.0617,g_score: 11.8214 \n",
      "Epoch [27/30], Step [ 200/1875 ], d_loss: 1.0424, g_loss: 1.1836,g_score: 10.0862 \n",
      "Epoch [27/30], Step [ 300/1875 ], d_loss: 1.1062, g_loss: 1.0580,g_score: 11.4560 \n",
      "Epoch [27/30], Step [ 400/1875 ], d_loss: 1.0968, g_loss: 1.1410,g_score: 10.8234 \n",
      "Epoch [27/30], Step [ 500/1875 ], d_loss: 1.2504, g_loss: 1.1474,g_score: 10.7798 \n",
      "Epoch [27/30], Step [ 600/1875 ], d_loss: 1.0874, g_loss: 0.9638,g_score: 12.7937 \n",
      "Epoch [27/30], Step [ 700/1875 ], d_loss: 0.9887, g_loss: 1.0349,g_score: 11.8573 \n",
      "Epoch [27/30], Step [ 800/1875 ], d_loss: 1.1553, g_loss: 1.1211,g_score: 10.7930 \n",
      "Epoch [27/30], Step [ 900/1875 ], d_loss: 1.0525, g_loss: 0.8752,g_score: 13.8630 \n",
      "Epoch [27/30], Step [ 1000/1875 ], d_loss: 1.0864, g_loss: 1.0027,g_score: 12.4425 \n",
      "Epoch [27/30], Step [ 1100/1875 ], d_loss: 1.3925, g_loss: 0.6688,g_score: 17.2720 \n",
      "Epoch [27/30], Step [ 1200/1875 ], d_loss: 1.0471, g_loss: 1.0346,g_score: 12.0812 \n",
      "Epoch [27/30], Step [ 1300/1875 ], d_loss: 1.1094, g_loss: 0.9397,g_score: 13.0655 \n",
      "Epoch [27/30], Step [ 1400/1875 ], d_loss: 1.1337, g_loss: 1.1607,g_score: 10.4381 \n",
      "Epoch [27/30], Step [ 1500/1875 ], d_loss: 1.2824, g_loss: 0.9235,g_score: 13.7604 \n",
      "Epoch [27/30], Step [ 1600/1875 ], d_loss: 0.8249, g_loss: 1.1185,g_score: 10.7543 \n",
      "Epoch [27/30], Step [ 1700/1875 ], d_loss: 1.0313, g_loss: 0.9450,g_score: 13.0518 \n",
      "Epoch [27/30], Step [ 1800/1875 ], d_loss: 1.2875, g_loss: 0.8672,g_score: 14.2697 \n",
      "28\n",
      "Epoch [28/30], Step [ 0/1875 ], d_loss: 1.2405, g_loss: 1.2076,g_score: 10.2427 \n",
      "Epoch [28/30], Step [ 100/1875 ], d_loss: 1.3976, g_loss: 1.4504,g_score: 7.8479 \n",
      "Epoch [28/30], Step [ 200/1875 ], d_loss: 1.1911, g_loss: 1.2174,g_score: 9.8667 \n",
      "Epoch [28/30], Step [ 300/1875 ], d_loss: 0.9754, g_loss: 0.8652,g_score: 13.9574 \n",
      "Epoch [28/30], Step [ 400/1875 ], d_loss: 0.7335, g_loss: 1.0357,g_score: 11.7424 \n",
      "Epoch [28/30], Step [ 500/1875 ], d_loss: 1.1713, g_loss: 0.9848,g_score: 12.6622 \n",
      "Epoch [28/30], Step [ 600/1875 ], d_loss: 1.1762, g_loss: 1.0674,g_score: 11.7104 \n",
      "Epoch [28/30], Step [ 700/1875 ], d_loss: 1.1811, g_loss: 0.9589,g_score: 12.8184 \n",
      "Epoch [28/30], Step [ 800/1875 ], d_loss: 1.2026, g_loss: 1.1265,g_score: 10.8268 \n",
      "Epoch [28/30], Step [ 900/1875 ], d_loss: 1.1705, g_loss: 0.8461,g_score: 14.3507 \n",
      "Epoch [28/30], Step [ 1000/1875 ], d_loss: 1.0285, g_loss: 0.9342,g_score: 13.0885 \n",
      "Epoch [28/30], Step [ 1100/1875 ], d_loss: 1.0019, g_loss: 1.2566,g_score: 9.5620 \n",
      "Epoch [28/30], Step [ 1200/1875 ], d_loss: 1.4416, g_loss: 0.9898,g_score: 12.8219 \n",
      "Epoch [28/30], Step [ 1300/1875 ], d_loss: 0.9465, g_loss: 0.9677,g_score: 12.6710 \n",
      "Epoch [28/30], Step [ 1400/1875 ], d_loss: 1.0902, g_loss: 0.7705,g_score: 15.5774 \n",
      "Epoch [28/30], Step [ 1500/1875 ], d_loss: 1.0503, g_loss: 1.3157,g_score: 9.0100 \n",
      "Epoch [28/30], Step [ 1600/1875 ], d_loss: 1.0503, g_loss: 0.9290,g_score: 13.1016 \n",
      "Epoch [28/30], Step [ 1700/1875 ], d_loss: 1.3549, g_loss: 1.0894,g_score: 11.3896 \n",
      "Epoch [28/30], Step [ 1800/1875 ], d_loss: 1.0356, g_loss: 1.1357,g_score: 10.6483 \n",
      "29\n",
      "Epoch [29/30], Step [ 0/1875 ], d_loss: 1.0644, g_loss: 0.9332,g_score: 13.3440 \n",
      "Epoch [29/30], Step [ 100/1875 ], d_loss: 1.0357, g_loss: 0.9582,g_score: 12.7581 \n",
      "Epoch [29/30], Step [ 200/1875 ], d_loss: 0.8403, g_loss: 1.0858,g_score: 11.2910 \n",
      "Epoch [29/30], Step [ 300/1875 ], d_loss: 1.1288, g_loss: 1.1322,g_score: 10.8458 \n",
      "Epoch [29/30], Step [ 400/1875 ], d_loss: 1.0525, g_loss: 0.8055,g_score: 15.1855 \n",
      "Epoch [29/30], Step [ 500/1875 ], d_loss: 1.1302, g_loss: 1.0690,g_score: 11.5262 \n",
      "Epoch [29/30], Step [ 600/1875 ], d_loss: 1.0999, g_loss: 0.9907,g_score: 12.7044 \n",
      "Epoch [29/30], Step [ 700/1875 ], d_loss: 1.1511, g_loss: 1.0980,g_score: 11.2388 \n",
      "Epoch [29/30], Step [ 800/1875 ], d_loss: 1.0778, g_loss: 0.8812,g_score: 13.9976 \n",
      "Epoch [29/30], Step [ 900/1875 ], d_loss: 1.2086, g_loss: 0.8861,g_score: 13.8052 \n",
      "Epoch [29/30], Step [ 1000/1875 ], d_loss: 0.9592, g_loss: 1.0355,g_score: 11.8195 \n",
      "Epoch [29/30], Step [ 1100/1875 ], d_loss: 1.2064, g_loss: 0.8632,g_score: 14.8532 \n",
      "Epoch [29/30], Step [ 1200/1875 ], d_loss: 1.0828, g_loss: 0.9523,g_score: 12.8971 \n",
      "Epoch [29/30], Step [ 1300/1875 ], d_loss: 0.9148, g_loss: 1.0401,g_score: 11.8564 \n",
      "Epoch [29/30], Step [ 1400/1875 ], d_loss: 1.0817, g_loss: 1.0340,g_score: 11.7009 \n",
      "Epoch [29/30], Step [ 1500/1875 ], d_loss: 1.1284, g_loss: 1.1215,g_score: 10.7639 \n",
      "Epoch [29/30], Step [ 1600/1875 ], d_loss: 1.1752, g_loss: 1.0643,g_score: 11.9785 \n",
      "Epoch [29/30], Step [ 1700/1875 ], d_loss: 1.1525, g_loss: 0.8238,g_score: 14.8590 \n",
      "Epoch [29/30], Step [ 1800/1875 ], d_loss: 0.9748, g_loss: 1.2999,g_score: 9.3466 \n"
     ]
    }
   ],
   "source": [
    "total_steps = len(data_loader)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.reshape(batch_size, image_size).to(device)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        outputs = D(images)\n",
    "        d_loss_real = loss_fn(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = loss_fn(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = loss_fn(outputs, real_labels)\n",
    "        g_score = outputs\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        if i % 100 ==0:  #generator的score比较高的话效果就会比较好 一直上升   #拓展：提高效果\n",
    "            print(\"Epoch [{}/{}], Step [ {}/{} ], d_loss: {:.4f}, g_loss: {:.4f},g_score: {:.4f} \"#加g_score\n",
    "              .format(epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item(), g_score.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 展示生成效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4d7951d4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z=torch.randn(batch_size,latent_size).to(device)\n",
    "fake_images=G(z)\n",
    "#print(fake_images.shape)\n",
    "fake_images=fake_images.view(batch_size,28,28).data.cpu().numpy()\n",
    "plt.imshow(fake_images[0],cmap=plt.cm.gray)\n",
    "#camp=plt.cm.gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存储模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(),\"mnist_genator.pth\")\n",
    "torch.save(D.state_dict(),\"mnist_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载保存的生成器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (1): LeakyReLU(negative_slope=0.2)\n",
      "  (2): Linear(in_features=256, out_features=784, bias=True)\n",
      "  (3): Tanh()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4d796043c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = G\n",
    "model.load_state_dict(torch.load(\"mnist_genator.pth\"))\n",
    "print(model)\n",
    "z=torch.randn(batch_size,latent_size).to(device)\n",
    "fake_images=model(z)\n",
    "fake_images2=fake_images.view(batch_size,28,28).data.cpu().numpy()\n",
    "plt.imshow(fake_images2[0],cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载保存的判别器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): LeakyReLU(negative_slope=0.2)\n",
      "  (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (3): LeakyReLU(negative_slope=0.2)\n",
      "  (4): Sigmoid()\n",
      ")\n",
      "torch.Size([784])\n",
      "tensor(13.1055, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_D = D\n",
    "model_D.load_state_dict(torch.load(\"mnist_discriminator.pth\"))\n",
    "print(model_D)\n",
    "# print(fake_images)\n",
    "# z=torch.randn(batch_size,latent_size).to(device)\n",
    "print(fake_images[0].shape)\n",
    "images = fake_images.reshape(batch_size, image_size).to(device)\n",
    "outputs = model_D(images)\n",
    "g_score = outputs\n",
    "print(g_score.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 扩展：AutoEncoder MNIST数据集生成效果\n",
    "\n",
    "AutoEncoder通过设计encode和decode过程使输入和输出越来越接近，是一种无监督学习过程。\n",
    "\n",
    "https://blog.csdn.net/roguesir/article/details/77469665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "epoch: 1, Loss: 714.4178\n",
      "epoch: 2, Loss: 351.9808\n",
      "epoch: 3, Loss: 338.2034\n",
      "epoch: 4, Loss: 323.5892\n",
      "epoch: 5, Loss: 326.8159\n",
      "epoch: 6, Loss: 309.8697\n",
      "epoch: 7, Loss: 284.4203\n",
      "epoch: 8, Loss: 270.0626\n",
      "epoch: 9, Loss: 243.8725\n",
      "epoch: 10, Loss: 221.3715\n",
      "epoch: 11, Loss: 210.4372\n",
      "epoch: 12, Loss: 201.0090\n",
      "epoch: 13, Loss: 197.2456\n",
      "epoch: 14, Loss: 194.3874\n",
      "epoch: 15, Loss: 187.4022\n",
      "epoch: 16, Loss: 182.4980\n",
      "epoch: 17, Loss: 178.7067\n",
      "epoch: 18, Loss: 175.8451\n",
      "epoch: 19, Loss: 174.0426\n",
      "epoch: 20, Loss: 169.8318\n",
      "epoch: 21, Loss: 167.8898\n",
      "epoch: 22, Loss: 157.0388\n",
      "epoch: 23, Loss: 154.7070\n",
      "epoch: 24, Loss: 150.9691\n",
      "epoch: 25, Loss: 144.5764\n",
      "epoch: 26, Loss: 139.4753\n",
      "epoch: 27, Loss: 137.8630\n",
      "epoch: 28, Loss: 132.1287\n",
      "epoch: 29, Loss: 132.9336\n",
      "epoch: 30, Loss: 126.5048\n",
      "epoch: 31, Loss: 126.0144\n",
      "epoch: 32, Loss: 124.7734\n",
      "epoch: 33, Loss: 124.9719\n",
      "epoch: 34, Loss: 123.5425\n",
      "epoch: 35, Loss: 117.2334\n",
      "epoch: 36, Loss: 116.4017\n",
      "epoch: 37, Loss: 111.0442\n",
      "epoch: 38, Loss: 116.8391\n",
      "epoch: 39, Loss: 111.0010\n",
      "epoch: 40, Loss: 110.4081\n",
      "epoch: 41, Loss: 110.5038\n",
      "epoch: 42, Loss: 105.4271\n",
      "epoch: 43, Loss: 107.1498\n",
      "epoch: 44, Loss: 106.0103\n",
      "epoch: 45, Loss: 107.0592\n",
      "epoch: 46, Loss: 103.5947\n",
      "epoch: 47, Loss: 104.5231\n",
      "epoch: 48, Loss: 101.4239\n",
      "epoch: 49, Loss: 105.7113\n",
      "epoch: 50, Loss: 101.5323\n",
      "epoch: 51, Loss: 100.0647\n",
      "epoch: 52, Loss: 100.7316\n",
      "epoch: 53, Loss: 100.5479\n",
      "epoch: 54, Loss: 100.1949\n",
      "epoch: 55, Loss: 96.5664\n",
      "epoch: 56, Loss: 95.5441\n",
      "epoch: 57, Loss: 94.3081\n",
      "epoch: 58, Loss: 98.0496\n",
      "epoch: 59, Loss: 94.8134\n",
      "epoch: 60, Loss: 96.3395\n",
      "epoch: 61, Loss: 94.8134\n",
      "epoch: 62, Loss: 95.8389\n",
      "epoch: 63, Loss: 92.8919\n",
      "epoch: 64, Loss: 94.9045\n",
      "epoch: 65, Loss: 95.6896\n",
      "epoch: 66, Loss: 89.5006\n",
      "epoch: 67, Loss: 94.5867\n",
      "epoch: 68, Loss: 93.1558\n",
      "epoch: 69, Loss: 93.3490\n",
      "epoch: 70, Loss: 94.2745\n",
      "epoch: 71, Loss: 93.9548\n",
      "epoch: 72, Loss: 90.3129\n",
      "epoch: 73, Loss: 93.1372\n",
      "epoch: 74, Loss: 92.8720\n",
      "epoch: 75, Loss: 90.8141\n",
      "epoch: 76, Loss: 92.7817\n",
      "epoch: 77, Loss: 91.0254\n",
      "epoch: 78, Loss: 92.8166\n",
      "epoch: 79, Loss: 89.7364\n",
      "epoch: 80, Loss: 91.2404\n",
      "epoch: 81, Loss: 90.9506\n",
      "epoch: 82, Loss: 90.3763\n",
      "epoch: 83, Loss: 90.0755\n",
      "epoch: 84, Loss: 90.6893\n",
      "epoch: 85, Loss: 87.9939\n",
      "epoch: 86, Loss: 87.1936\n",
      "epoch: 87, Loss: 89.4234\n",
      "epoch: 88, Loss: 88.8698\n",
      "epoch: 89, Loss: 87.9529\n",
      "epoch: 90, Loss: 87.9545\n",
      "epoch: 91, Loss: 90.0508\n",
      "epoch: 92, Loss: 88.8540\n",
      "epoch: 93, Loss: 85.1982\n",
      "epoch: 94, Loss: 89.5362\n",
      "epoch: 95, Loss: 85.5633\n",
      "epoch: 96, Loss: 87.6390\n",
      "epoch: 97, Loss: 89.8230\n",
      "epoch: 98, Loss: 88.0849\n",
      "epoch: 99, Loss: 88.4473\n",
      "epoch: 100, Loss: 85.2014\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as tfs\n",
    "from torchvision.utils import save_image\n",
    "import m\n",
    "\n",
    "\n",
    "im_tfs = tfs.Compose([\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize([0.5], [0.5]) # 标准化\n",
    "])\n",
    "\n",
    "train_set = MNIST('./mnist_data', transform=im_tfs,download=True)\n",
    "train_data = DataLoader(train_set, batch_size=2048, shuffle=True)\n",
    "\n",
    "# 定义网络\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 3) # 输出的 code 是 3 维，便于可视化\n",
    "        )\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n",
    "\n",
    "\n",
    "class conv_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # (b, 16, 10, 10)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # (b, 8, 3, 3)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (b, 8, 2, 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # (b, 8, 15, 15)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # (b, 1, 28, 28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n",
    "\n",
    "# autoencoder\n",
    "net = autoencoder()\n",
    "x = Variable(torch.randn(1, 28*28)) # batch size 是 1\n",
    "code, _ = net(x)\n",
    "print(code.shape)\n",
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# conv_autoencoder\n",
    "conv_net = conv_autoencoder()\n",
    "if torch.cuda.is_available():\n",
    "    conv_net = conv_net.to(device)\n",
    "optimizer = torch.optim.Adam(conv_net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def to_img(x):\n",
    "    '''\n",
    "    定义一个函数将最后的结果转换回图片\n",
    "    '''\n",
    "    x = 0.5 * (x + 1.)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.shape[0], 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "# 开始训练自动编码器\n",
    "for e in range(100):\n",
    "    for im, _ in train_data:\n",
    "        # im = im.view(im.shape[0], -1)\n",
    "        im = Variable(im).to(device)\n",
    "        # 前向传播\n",
    "        _, output = conv_net(im)\n",
    "        loss = criterion(output, im) / im.shape[0] # 平均\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # if (e+1)%20==0:\n",
    "    print('epoch: {}, Loss: {:.4f}'.format(e + 1, loss.item()))\n",
    "    pic = to_img(output.cpu().data)\n",
    "    if not os.path.exists('./simple_autoencoder'):\n",
    "        os.mkdir('./simple_autoencoder')\n",
    "    save_image(pic, './simple_autoencoder/image_conv_epoch{}.png'.format(e + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果\n",
    "迭代一百次效果\n",
    "![](./simple_autoencoder/image_conv_epoch100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 扩展：DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, D: 1.386, G:0.9036\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "NUM_TRAIN = 50000\n",
    "NUM_VAL = 5000\n",
    "\n",
    "NOISE_DIM = 96\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "class ChunkSampler(sampler.Sampler): # 定义一个取样的函数\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def show_images(images): # 定义画图工具\n",
    "    images = np.reshape(images, [images.shape[0], -1])\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return \n",
    "\n",
    "\n",
    "def preprocess_img(x):\n",
    "    x = tfs.ToTensor()(x)\n",
    "    return (x - 0.5) / 0.5\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "class build_dc_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(build_dc_classifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, 1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5, 1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class build_dc_generator(nn.Module): \n",
    "    def __init__(self, noise_dim=NOISE_DIM):\n",
    "        super(build_dc_generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 7 * 7 * 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(7 * 7 * 128)\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.shape[0], 128, 7, 7) # reshape 通道是 128，大小是 7x7\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def discriminator_loss(logits_real, logits_fake): # 判别器的 loss\n",
    "    size = logits_real.shape[0]\n",
    "    true_labels = Variable(torch.ones(size, 1)).float()\n",
    "    false_labels = Variable(torch.zeros(size, 1)).float()\n",
    "    loss = bce_loss(logits_real, true_labels) + bce_loss(logits_fake, false_labels)\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake): # 生成器的 loss  \n",
    "    size = logits_fake.shape[0]\n",
    "    true_labels = Variable(torch.ones(size, 1)).float()\n",
    "    loss = bce_loss(logits_fake, true_labels)\n",
    "    return loss\n",
    "\n",
    "# 使用 adam 来进行训练，学习率是 3e-4, beta1 是 0.5, beta2 是 0.999\n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
    "    return optimizer\n",
    "\n",
    "def train_dc_gan(D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=250, \n",
    "                noise_size=96, num_epochs=10):\n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, _ in train_data:\n",
    "            bs = x.shape[0]\n",
    "            # 判别网络\n",
    "            real_data = Variable(x) # 真实数据\n",
    "            logits_real = D_net(real_data) # 判别网络得分\n",
    "            \n",
    "            sample_noise = (torch.rand(bs, noise_size) - 0.5) / 0.5 # -1 ~ 1 的均匀分布\n",
    "            g_fake_seed = Variable(sample_noise)\n",
    "            fake_images = G_net(g_fake_seed) # 生成的假的数据\n",
    "            logits_fake = D_net(fake_images) # 判别网络得分\n",
    "\n",
    "            d_total_error = discriminator_loss(logits_real, logits_fake) # 判别器的 loss\n",
    "            D_optimizer.zero_grad()\n",
    "            d_total_error.backward()\n",
    "            D_optimizer.step() # 优化判别网络\n",
    "            \n",
    "            # 生成网络\n",
    "            g_fake_seed = Variable(sample_noise)\n",
    "            fake_images = G_net(g_fake_seed) # 生成的假的数据\n",
    "\n",
    "            gen_logits_fake = D_net(fake_images)\n",
    "            g_error = generator_loss(gen_logits_fake) # 生成网络的 loss\n",
    "            G_optimizer.zero_grad()\n",
    "            g_error.backward()\n",
    "            G_optimizer.step() # 优化生成网络\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "                print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count, d_total_error.item(), g_error.item()))\n",
    "                imgs_numpy = deprocess_img(fake_images.data.cpu().numpy())\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.savefig(\"Iter-{}-DCGAN.png\".format(iter_count))\n",
    "            iter_count += 1\n",
    "\n",
    "\n",
    "train_set = MNIST('./mnist_data', train=True, download=True, transform=preprocess_img)\n",
    "\n",
    "train_data = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, 0))\n",
    "\n",
    "val_set = MNIST('./mnist_data', train=True, download=True, transform=preprocess_img)\n",
    "\n",
    "val_data = DataLoader(val_set, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))\n",
    "\n",
    "\n",
    "D_DC = build_dc_classifier()\n",
    "G_DC = build_dc_generator()\n",
    "\n",
    "D_DC_optim = get_optimizer(D_DC)\n",
    "G_DC_optim = get_optimizer(G_DC)\n",
    "\n",
    "train_dc_gan(D_DC, G_DC, D_DC_optim, G_DC_optim, discriminator_loss, generator_loss, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
