{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容一：在MNIST数据集上构建网络进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as tud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学会使用Dataloader来加载数据\n",
    "Dataloader能够帮我们打乱数据集，拿到batch数据 \\\n",
    "为了使用Dataloader，需要定义以下三个function\n",
    "- \\__init__: 模型初始化\n",
    "- \\__len__: 返回整个数据集有多少item\n",
    "- \\__getitem__: 根据给定的index返回一个item\n",
    "\n",
    "调用Dataloader之前还要先定义dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch帮助我们预先加载了一些常用的数据集\n",
    "# 如果使用这些数据集，会相对容易的进行数据加载\n",
    "# 例如：常用的Mnist数据集\n",
    "mnist_train_data = datasets.MNIST(\"./data\",train=True,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.13066062,),std=(0.30810776,))\n",
    "                                 ]))\n",
    "batch_size = 64\n",
    "train_dataloader = tud.DataLoader(mnist_train_data,batch_size = batch_size,shuffle=True) # 将dataset转换为iterator\n",
    "mnist_test_data = datasets.MNIST(\"./data\",train=False,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.13066062,),std=(0.30810776,))\n",
    "                                 ]))\n",
    "test_dataloader = tud.DataLoader(mnist_test_data,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 定义网络\n",
    "- 继承 nn.Module\n",
    "- 初始化函数\n",
    "- forward 函数\n",
    "- 其余可以根据模型需要定义相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的基于ConvNet的简单神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__() # the input is 1*28*28\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1) # (28-5)/1+1=24, 20*24*24\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1) # 12-5+1=8\n",
    "        self.fc1 = nn.Linear(4*4*50,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x)) # 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # 20 * 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # 50 * 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # 50 * 4 * 4\n",
    "        x = x.view(-1,4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x #F.log_softmax(x,dim=1)\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们把所有数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda Tensor\n",
    "- forward pass\n",
    "- 计算loss\n",
    "- 清空gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    for idx, (data, label) in enumerate(train_dataloader):\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {}, loss: {}\".format(\n",
    "                epoch,idx,loss.item()))  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (data,target) in enumerate(test_dataloader):\n",
    "            output = model(data) # batch_size * 10        \n",
    "            loss = loss_fn(output,target)*output.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            total_loss += loss\n",
    "            correct += pred.eq(target).sum()\n",
    "    total_loss /= len(test_dataloader.dataset)\n",
    "    acc = 100.*correct/len(test_dataloader.dataset)\n",
    "    print(\"Test Loss:{}, Accuracy:{}\".format(total_loss,acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 2.2965400218963623\n",
      "Train Epoch: 0, iteration: 100, loss: 0.49735018610954285\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model = train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(model,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"mnist_cnn.pth\")\n",
    "# num_epochs = 2\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(num_epochs):\n",
    "    train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    acc = test(model,test_dataloader,loss_fn)\n",
    "    if acc > best_valid_acc:\n",
    "        best_valid_acc = acc\n",
    "        torch.save(model.state_dict(),\"best_mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Net()\n",
    "test_model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "test(model,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./fashion_mnist_data\",train=True,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                   ])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True) # 将dataset转换为iterator\n",
    "test_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./fashion_mnist_data\",train=False,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                   ])),\n",
    "    batch_size=batch_size) # 将dataset转换为iterator\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr,momentum=momentum)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(model,test_dataloader,loss_fn)\n",
    "    \n",
    "torch.save(model.state_dict(),\"fashion_mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容二：CNN模型的迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 很多时候当我们训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用预训练的模型来加速训练的过程。我们经常使用在ImageNet上的预训练模型\n",
    "- 有两种方法做迁移学习\n",
    "    - finetuning：从一个预训练模型开始，改变一些模型的架构，然后继续训练整个模型的参数；\n",
    "    - feature extraction：不改变预训练模型的参数，只更新我们改变过的部分模型参数。（当成特征提取器来使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.utils.data as tud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据：使用hymenoptera_data数据集 \\\n",
    "数据集包括两类图片，bees和ants。这些数据都被处理成了可以使用ImageFolder来读取的格式。我们只需要把data_dir设置成数据的根目录，然后把model_name设置成我们想要使用的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "data_dir = \"./data/hymenoptera_data\"\n",
    "model_name = \"resnet18\"\n",
    "num_class = 2\n",
    "#feature_extract = True\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据: 把数据预处理成相应的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress\n",
    "all_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(input_size),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),                                    \n",
    "                                ]))\n",
    "loader = tud.DataLoader(all_imgs,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs[20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "batch_size = 32\n",
    "train_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(input_size),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "train_dataloader = tud.DataLoader(train_imgs,batch_size=batch_size,shuffle=True)\n",
    "test_imgs = datasets.ImageFolder(os.path.join(data_dir,\"val\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.Resize(input_size),  \n",
    "                                    transforms.CenterCrop(input_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "test_dataloader = tud.DataLoader(test_imgs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "def initialize_model(model_name,num_class,use_pretrained=True,feature_extract=True):\n",
    "    if model_name == \"resnet18\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        if feature_extract: # do not update the parameters\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_class)        \n",
    "    else:\n",
    "        print(\"model not implemented\")\n",
    "        return None\n",
    "    return model_ft\n",
    "model_ft = initialize_model(\"resnet18\",2,use_pretrained=False,feature_extract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ft.layer1[0].conv1.weight.requires_grad)\n",
    "print(model_ft.fc.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model_ft.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(train_dataloader.dataset)\n",
    "    epoch_accuracy = 100.*total_corrects / len(train_dataloader.dataset)\n",
    "    print(\"Epoch:{}, Training Loss:{}, Traning Acc:{}\".format(epoch,epoch_loss,epoch_accuracy))  \n",
    "    #return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(test_dataloader.dataset)\n",
    "    epoch_accuracy = 100.*total_corrects / len(test_dataloader.dataset)\n",
    "    print(\"acc type:\", epoch_accuracy)\n",
    "    print(\"Test Loss:{}, Test Acc:{}\".format(epoch_loss,epoch_accuracy))  \n",
    "    return epoch_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model_ft,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    acc = test_model(model_ft,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动搭建resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\" 3x3卷积（padding）\n",
    "    :param in_planes:\n",
    "    :param out_planes:\n",
    "    :param stride:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_channels=in_planes, out_channels=out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    BasicBlock\n",
    "    \"\"\"\n",
    "    expansion = 1  # 最后一层是前一层的expansion倍\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes=inplanes, out_planes=planes, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(in_planes=planes, out_planes=planes)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\" Constructs  a ResNet template\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, n_classes=1000):\n",
    "        \"\"\"\n",
    "        :param block: BasicBlock or Bottleneck\n",
    "        :param layers:\n",
    "        :param num_classes:\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)  # padding=(kernel_size-1)/2 bias=False\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # padding=(kernel_size-1)/2\n",
    "        self.layer1 = self._make_layer(block=block, planes=64, blocks=layers[0])\n",
    "        self.layer2 = self._make_layer(block=block, planes=128, blocks=layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block=block, planes=256, blocks=layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block=block, planes=512, blocks=layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.fc = nn.Linear(in_features=512*block.expansion, out_features=self.n_classes)\n",
    "\n",
    "\n",
    "        # 初始化卷积层和BN层\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        # stride = 1表示第一层，不需要下采样（使用maxpool下采样了），stride = 2表示第二，三，四层，需要下采样\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.inplanes, out_channels=planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(num_features=planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # blocks中的第一层决定是否有下采样，其中第一个block的第一层没有下采样，其他block的第一层有下采样\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "        # print('x.size():{}'.format(x.size()))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        # print('x.size():{}'.format(x.size()))\n",
    "        x = self.layer2(x)\n",
    "        # print('x.size():{}'.format(x.size()))\n",
    "        x = self.layer3(x)\n",
    "        # print('x.size():{}'.format(x.size()))\n",
    "        x = self.layer4(x)\n",
    "        # print('x.size():{}'.format(x.size()))\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_weights(self, url):\n",
    "        pretrained_dict = model_zoo.load_url(model_urls[url])\n",
    "        model_dict = self.state_dict()\n",
    "        # print('pretrained_dict.keys():', pretrained_dict.keys())\n",
    "        # print('model_dict.keys():', model_dict.keys())\n",
    "        if self.n_classes!=1000:\n",
    "            new_dict = {k: v for k, v in pretrained_dict.items() if k not in {'fc.weight', 'fc.bias'}}\n",
    "        else:\n",
    "            new_dict = pretrained_dict\n",
    "        model_dict.update(new_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_dict = model_zoo.load_url(model_urls['resnet50'])\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k not in {'fc.bias', 'fc.weight'}}\n",
    "        pretrained_dict.update(model.state_dict())\n",
    "        # print(pretrained_dict.keys())\n",
    "        model.load_state_dict(pretrained_dict)\n",
    "    return model\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model=torchvision.models.resnet18(pretrained=False)\n",
    "# model.fc = nn.Linear(model.fc.in_features,4,bias=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "image =\"./test.jpg\"\n",
    "transform = transforms.Compose([transforms.Resize((224,224))])\n",
    "image=cv2.imread(image)\n",
    "image=cv2.resize(image,(224,224))\n",
    "image = Image.fromarray(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "tensor=torch.from_numpy(np.asarray(image)).permute(2,0,1).float()/255.0\n",
    "tensor=tensor.reshape((1,3,224,224))\n",
    "tensor=tensor.to(device)\n",
    "output=model(tensor)\n",
    "# print(output)\n",
    "_, pred = torch.max(output.data,1)\n",
    "print(pred.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./imagenet.txt') as f:\n",
    "  classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes[pred[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
