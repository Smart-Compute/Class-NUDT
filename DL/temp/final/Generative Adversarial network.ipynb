{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #gpu\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(\"./mnist_data\", train=True, download=False,\n",
    "                            transform = transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5,),std=(0.5,))  #就会把tensor变成(-1,1)而不是用真实的均值和方差normalize\n",
    "                            ]))\n",
    "batch_size = 32\n",
    "data_loader = tud.DataLoader(mnist_data, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （1）配置网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28*28\n",
    "hidden_size = 256\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),#1：二分类问题，对和不对\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "latent_size= 64\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh()\n",
    ")\n",
    "D = D.to(device)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2)定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3)定义优化策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch [0/30], Step [ 0/1875 ], d_loss: 1.3360, g_loss: 0.6950,g_score: 15.9713 \n",
      "Epoch [0/30], Step [ 100/1875 ], d_loss: 0.7363, g_loss: 0.7445,g_score: 15.2107 \n",
      "Epoch [0/30], Step [ 200/1875 ], d_loss: 0.6506, g_loss: 0.8502,g_score: 13.6852 \n",
      "Epoch [0/30], Step [ 300/1875 ], d_loss: 0.5562, g_loss: 0.9915,g_score: 11.9202 \n",
      "Epoch [0/30], Step [ 400/1875 ], d_loss: 0.7065, g_loss: 0.8044,g_score: 14.3291 \n",
      "Epoch [0/30], Step [ 500/1875 ], d_loss: 1.1639, g_loss: 0.7148,g_score: 15.8424 \n",
      "Epoch [0/30], Step [ 600/1875 ], d_loss: 1.0757, g_loss: 0.6709,g_score: 16.4143 \n",
      "Epoch [0/30], Step [ 700/1875 ], d_loss: 0.9815, g_loss: 0.8124,g_score: 14.4821 \n",
      "Epoch [0/30], Step [ 800/1875 ], d_loss: 0.9020, g_loss: 0.8086,g_score: 14.6839 \n",
      "Epoch [0/30], Step [ 900/1875 ], d_loss: 0.6832, g_loss: 0.8887,g_score: 13.1896 \n",
      "Epoch [0/30], Step [ 1000/1875 ], d_loss: 1.0253, g_loss: 0.8050,g_score: 14.4458 \n",
      "Epoch [0/30], Step [ 1100/1875 ], d_loss: 0.9713, g_loss: 0.7098,g_score: 16.0218 \n",
      "Epoch [0/30], Step [ 1200/1875 ], d_loss: 1.7185, g_loss: 0.8243,g_score: 14.4674 \n",
      "Epoch [0/30], Step [ 1300/1875 ], d_loss: 1.3441, g_loss: 0.7787,g_score: 15.3567 \n",
      "Epoch [0/30], Step [ 1400/1875 ], d_loss: 0.9659, g_loss: 0.8183,g_score: 14.4191 \n",
      "Epoch [0/30], Step [ 1500/1875 ], d_loss: 0.8943, g_loss: 0.9233,g_score: 12.9771 \n",
      "Epoch [0/30], Step [ 1600/1875 ], d_loss: 1.5391, g_loss: 0.8107,g_score: 14.5685 \n",
      "Epoch [0/30], Step [ 1700/1875 ], d_loss: 0.8788, g_loss: 0.7811,g_score: 15.1041 \n",
      "Epoch [0/30], Step [ 1800/1875 ], d_loss: 1.0889, g_loss: 0.7927,g_score: 14.7830 \n",
      "1\n",
      "Epoch [1/30], Step [ 0/1875 ], d_loss: 1.2425, g_loss: 0.7134,g_score: 15.8366 \n",
      "Epoch [1/30], Step [ 100/1875 ], d_loss: 1.1803, g_loss: 0.8640,g_score: 13.6740 \n",
      "Epoch [1/30], Step [ 200/1875 ], d_loss: 1.4386, g_loss: 0.8719,g_score: 13.7295 \n",
      "Epoch [1/30], Step [ 300/1875 ], d_loss: 1.4225, g_loss: 0.9657,g_score: 12.4767 \n",
      "Epoch [1/30], Step [ 400/1875 ], d_loss: 1.7279, g_loss: 0.9097,g_score: 13.0989 \n",
      "Epoch [1/30], Step [ 500/1875 ], d_loss: 1.4751, g_loss: 0.8354,g_score: 14.3228 \n",
      "Epoch [1/30], Step [ 600/1875 ], d_loss: 0.9793, g_loss: 0.8402,g_score: 13.9643 \n",
      "Epoch [1/30], Step [ 700/1875 ], d_loss: 1.5215, g_loss: 0.9116,g_score: 12.9852 \n",
      "Epoch [1/30], Step [ 800/1875 ], d_loss: 1.0395, g_loss: 0.7649,g_score: 15.4121 \n",
      "Epoch [1/30], Step [ 900/1875 ], d_loss: 1.0129, g_loss: 0.8712,g_score: 13.4670 \n",
      "Epoch [1/30], Step [ 1000/1875 ], d_loss: 1.5197, g_loss: 0.8646,g_score: 13.7234 \n",
      "Epoch [1/30], Step [ 1100/1875 ], d_loss: 0.6798, g_loss: 0.8760,g_score: 13.3777 \n",
      "Epoch [1/30], Step [ 1200/1875 ], d_loss: 1.2391, g_loss: 0.9506,g_score: 12.7131 \n",
      "Epoch [1/30], Step [ 1300/1875 ], d_loss: 1.2734, g_loss: 0.8125,g_score: 14.4472 \n",
      "Epoch [1/30], Step [ 1400/1875 ], d_loss: 1.1384, g_loss: 0.7707,g_score: 15.2257 \n",
      "Epoch [1/30], Step [ 1500/1875 ], d_loss: 1.1635, g_loss: 0.8045,g_score: 14.5163 \n",
      "Epoch [1/30], Step [ 1600/1875 ], d_loss: 1.7015, g_loss: 0.7688,g_score: 15.3214 \n",
      "Epoch [1/30], Step [ 1700/1875 ], d_loss: 0.8426, g_loss: 0.9491,g_score: 12.8878 \n",
      "Epoch [1/30], Step [ 1800/1875 ], d_loss: 1.5138, g_loss: 0.9808,g_score: 12.1726 \n",
      "2\n",
      "Epoch [2/30], Step [ 0/1875 ], d_loss: 0.9324, g_loss: 0.8857,g_score: 13.4114 \n",
      "Epoch [2/30], Step [ 100/1875 ], d_loss: 1.1528, g_loss: 0.8684,g_score: 13.6824 \n",
      "Epoch [2/30], Step [ 200/1875 ], d_loss: 0.9361, g_loss: 0.8532,g_score: 13.7704 \n",
      "Epoch [2/30], Step [ 300/1875 ], d_loss: 1.1336, g_loss: 0.8159,g_score: 14.4284 \n",
      "Epoch [2/30], Step [ 400/1875 ], d_loss: 0.7948, g_loss: 0.8515,g_score: 13.9370 \n",
      "Epoch [2/30], Step [ 500/1875 ], d_loss: 1.0813, g_loss: 0.7201,g_score: 16.0259 \n",
      "Epoch [2/30], Step [ 600/1875 ], d_loss: 1.0452, g_loss: 0.8167,g_score: 14.7033 \n",
      "Epoch [2/30], Step [ 700/1875 ], d_loss: 1.2125, g_loss: 1.0135,g_score: 11.9777 \n",
      "Epoch [2/30], Step [ 800/1875 ], d_loss: 1.2623, g_loss: 0.8885,g_score: 13.6946 \n",
      "Epoch [2/30], Step [ 900/1875 ], d_loss: 1.0342, g_loss: 1.1144,g_score: 10.8241 \n",
      "Epoch [2/30], Step [ 1000/1875 ], d_loss: 1.5355, g_loss: 0.6564,g_score: 17.3367 \n",
      "Epoch [2/30], Step [ 1100/1875 ], d_loss: 1.2403, g_loss: 0.7573,g_score: 15.9601 \n",
      "Epoch [2/30], Step [ 1200/1875 ], d_loss: 1.1711, g_loss: 1.0622,g_score: 11.2300 \n",
      "Epoch [2/30], Step [ 1300/1875 ], d_loss: 1.0848, g_loss: 0.9604,g_score: 12.3873 \n",
      "Epoch [2/30], Step [ 1400/1875 ], d_loss: 1.3261, g_loss: 0.8901,g_score: 13.4650 \n",
      "Epoch [2/30], Step [ 1500/1875 ], d_loss: 0.9492, g_loss: 0.8254,g_score: 14.2164 \n",
      "Epoch [2/30], Step [ 1600/1875 ], d_loss: 1.4444, g_loss: 0.7571,g_score: 15.4652 \n",
      "Epoch [2/30], Step [ 1700/1875 ], d_loss: 1.3784, g_loss: 1.0500,g_score: 11.3466 \n",
      "Epoch [2/30], Step [ 1800/1875 ], d_loss: 0.9234, g_loss: 1.0510,g_score: 11.4538 \n",
      "3\n",
      "Epoch [3/30], Step [ 0/1875 ], d_loss: 1.4290, g_loss: 0.8860,g_score: 13.4469 \n",
      "Epoch [3/30], Step [ 100/1875 ], d_loss: 1.0742, g_loss: 0.8074,g_score: 14.8428 \n",
      "Epoch [3/30], Step [ 200/1875 ], d_loss: 1.2755, g_loss: 0.9123,g_score: 13.2005 \n",
      "Epoch [3/30], Step [ 300/1875 ], d_loss: 0.8735, g_loss: 1.0057,g_score: 12.2095 \n",
      "Epoch [3/30], Step [ 400/1875 ], d_loss: 1.2232, g_loss: 0.8837,g_score: 13.4526 \n",
      "Epoch [3/30], Step [ 500/1875 ], d_loss: 1.0929, g_loss: 0.9749,g_score: 12.5561 \n",
      "Epoch [3/30], Step [ 600/1875 ], d_loss: 1.1656, g_loss: 0.8480,g_score: 14.1017 \n",
      "Epoch [3/30], Step [ 700/1875 ], d_loss: 1.5299, g_loss: 0.8620,g_score: 14.0352 \n",
      "Epoch [3/30], Step [ 800/1875 ], d_loss: 1.3577, g_loss: 0.7737,g_score: 15.5911 \n",
      "Epoch [3/30], Step [ 900/1875 ], d_loss: 1.1262, g_loss: 0.6742,g_score: 16.6231 \n",
      "Epoch [3/30], Step [ 1000/1875 ], d_loss: 0.9144, g_loss: 0.9903,g_score: 12.0527 \n",
      "Epoch [3/30], Step [ 1100/1875 ], d_loss: 1.1669, g_loss: 0.8777,g_score: 13.4512 \n",
      "Epoch [3/30], Step [ 1200/1875 ], d_loss: 1.0137, g_loss: 0.8508,g_score: 13.8767 \n",
      "Epoch [3/30], Step [ 1300/1875 ], d_loss: 1.5846, g_loss: 0.7715,g_score: 15.1464 \n",
      "Epoch [3/30], Step [ 1400/1875 ], d_loss: 1.0219, g_loss: 0.8304,g_score: 14.0816 \n",
      "Epoch [3/30], Step [ 1500/1875 ], d_loss: 1.0138, g_loss: 0.8078,g_score: 14.4571 \n",
      "Epoch [3/30], Step [ 1600/1875 ], d_loss: 0.9760, g_loss: 0.9312,g_score: 12.7058 \n",
      "Epoch [3/30], Step [ 1700/1875 ], d_loss: 0.8826, g_loss: 0.8496,g_score: 14.1055 \n",
      "Epoch [3/30], Step [ 1800/1875 ], d_loss: 1.2739, g_loss: 0.9216,g_score: 13.1095 \n",
      "4\n",
      "Epoch [4/30], Step [ 0/1875 ], d_loss: 1.0592, g_loss: 0.9768,g_score: 12.9378 \n",
      "Epoch [4/30], Step [ 100/1875 ], d_loss: 1.2994, g_loss: 0.7358,g_score: 16.2036 \n",
      "Epoch [4/30], Step [ 200/1875 ], d_loss: 0.9992, g_loss: 0.9703,g_score: 12.4293 \n",
      "Epoch [4/30], Step [ 300/1875 ], d_loss: 1.1076, g_loss: 0.8069,g_score: 14.7510 \n",
      "Epoch [4/30], Step [ 400/1875 ], d_loss: 0.8962, g_loss: 0.8298,g_score: 14.3089 \n",
      "Epoch [4/30], Step [ 500/1875 ], d_loss: 1.0811, g_loss: 0.6703,g_score: 16.9957 \n",
      "Epoch [4/30], Step [ 600/1875 ], d_loss: 0.8480, g_loss: 0.9948,g_score: 11.9957 \n",
      "Epoch [4/30], Step [ 700/1875 ], d_loss: 1.2046, g_loss: 1.0069,g_score: 11.9471 \n",
      "Epoch [4/30], Step [ 800/1875 ], d_loss: 1.1983, g_loss: 0.9512,g_score: 12.5399 \n",
      "Epoch [4/30], Step [ 900/1875 ], d_loss: 1.0778, g_loss: 0.8995,g_score: 13.2008 \n",
      "Epoch [4/30], Step [ 1000/1875 ], d_loss: 1.1297, g_loss: 0.9457,g_score: 12.5931 \n",
      "Epoch [4/30], Step [ 1100/1875 ], d_loss: 1.2869, g_loss: 0.8551,g_score: 13.8296 \n",
      "Epoch [4/30], Step [ 1200/1875 ], d_loss: 1.1963, g_loss: 0.8127,g_score: 14.5851 \n",
      "Epoch [4/30], Step [ 1300/1875 ], d_loss: 1.5358, g_loss: 0.9531,g_score: 12.6192 \n",
      "Epoch [4/30], Step [ 1400/1875 ], d_loss: 0.7970, g_loss: 0.8039,g_score: 14.4639 \n",
      "Epoch [4/30], Step [ 1500/1875 ], d_loss: 1.3534, g_loss: 0.8116,g_score: 14.6212 \n",
      "Epoch [4/30], Step [ 1600/1875 ], d_loss: 0.9940, g_loss: 0.9523,g_score: 12.8924 \n",
      "Epoch [4/30], Step [ 1700/1875 ], d_loss: 1.3739, g_loss: 0.9557,g_score: 12.6181 \n",
      "Epoch [4/30], Step [ 1800/1875 ], d_loss: 1.4393, g_loss: 0.9356,g_score: 13.3900 \n",
      "5\n",
      "Epoch [5/30], Step [ 0/1875 ], d_loss: 1.2546, g_loss: 0.8360,g_score: 14.5607 \n",
      "Epoch [5/30], Step [ 100/1875 ], d_loss: 1.4032, g_loss: 0.7860,g_score: 15.7658 \n",
      "Epoch [5/30], Step [ 200/1875 ], d_loss: 1.3320, g_loss: 0.9835,g_score: 13.1680 \n",
      "Epoch [5/30], Step [ 300/1875 ], d_loss: 0.9921, g_loss: 0.8453,g_score: 14.0244 \n",
      "Epoch [5/30], Step [ 400/1875 ], d_loss: 1.2227, g_loss: 1.0848,g_score: 11.1315 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Step [ 500/1875 ], d_loss: 1.0163, g_loss: 0.8768,g_score: 13.7842 \n",
      "Epoch [5/30], Step [ 600/1875 ], d_loss: 1.3961, g_loss: 0.9641,g_score: 12.6129 \n",
      "Epoch [5/30], Step [ 700/1875 ], d_loss: 0.9606, g_loss: 1.0036,g_score: 12.0146 \n",
      "Epoch [5/30], Step [ 800/1875 ], d_loss: 0.8601, g_loss: 0.9104,g_score: 13.0219 \n",
      "Epoch [5/30], Step [ 900/1875 ], d_loss: 1.4880, g_loss: 0.8554,g_score: 13.7687 \n",
      "Epoch [5/30], Step [ 1000/1875 ], d_loss: 1.2699, g_loss: 1.0062,g_score: 11.8709 \n",
      "Epoch [5/30], Step [ 1100/1875 ], d_loss: 0.9038, g_loss: 0.9448,g_score: 12.5583 \n",
      "Epoch [5/30], Step [ 1200/1875 ], d_loss: 1.1293, g_loss: 0.9583,g_score: 12.6136 \n",
      "Epoch [5/30], Step [ 1300/1875 ], d_loss: 1.5492, g_loss: 0.8368,g_score: 14.3240 \n",
      "Epoch [5/30], Step [ 1400/1875 ], d_loss: 1.0449, g_loss: 1.0250,g_score: 11.9389 \n",
      "Epoch [5/30], Step [ 1500/1875 ], d_loss: 1.3767, g_loss: 1.0281,g_score: 11.5975 \n",
      "Epoch [5/30], Step [ 1600/1875 ], d_loss: 1.3667, g_loss: 0.8578,g_score: 14.0145 \n",
      "Epoch [5/30], Step [ 1700/1875 ], d_loss: 1.1534, g_loss: 0.8770,g_score: 13.6582 \n",
      "Epoch [5/30], Step [ 1800/1875 ], d_loss: 1.2167, g_loss: 0.8267,g_score: 14.4065 \n",
      "6\n",
      "Epoch [6/30], Step [ 0/1875 ], d_loss: 1.2541, g_loss: 1.0227,g_score: 11.7993 \n",
      "Epoch [6/30], Step [ 100/1875 ], d_loss: 1.3349, g_loss: 0.7466,g_score: 15.7307 \n",
      "Epoch [6/30], Step [ 200/1875 ], d_loss: 0.9125, g_loss: 1.1615,g_score: 10.3119 \n",
      "Epoch [6/30], Step [ 300/1875 ], d_loss: 1.1418, g_loss: 0.8538,g_score: 13.8918 \n",
      "Epoch [6/30], Step [ 400/1875 ], d_loss: 1.0242, g_loss: 0.7623,g_score: 15.3725 \n",
      "Epoch [6/30], Step [ 500/1875 ], d_loss: 1.1444, g_loss: 1.1898,g_score: 9.8754 \n",
      "Epoch [6/30], Step [ 600/1875 ], d_loss: 0.9790, g_loss: 0.8025,g_score: 14.5141 \n",
      "Epoch [6/30], Step [ 700/1875 ], d_loss: 1.1363, g_loss: 1.0510,g_score: 11.3200 \n",
      "Epoch [6/30], Step [ 800/1875 ], d_loss: 1.1105, g_loss: 0.7771,g_score: 15.0688 \n",
      "Epoch [6/30], Step [ 900/1875 ], d_loss: 1.3344, g_loss: 1.0251,g_score: 12.1058 \n",
      "Epoch [6/30], Step [ 1000/1875 ], d_loss: 1.1804, g_loss: 1.0070,g_score: 11.9268 \n",
      "Epoch [6/30], Step [ 1100/1875 ], d_loss: 1.0602, g_loss: 0.9930,g_score: 12.0890 \n",
      "Epoch [6/30], Step [ 1200/1875 ], d_loss: 1.0281, g_loss: 0.9696,g_score: 12.5183 \n",
      "Epoch [6/30], Step [ 1300/1875 ], d_loss: 1.3672, g_loss: 0.8658,g_score: 14.0838 \n",
      "Epoch [6/30], Step [ 1400/1875 ], d_loss: 1.4805, g_loss: 0.8400,g_score: 14.6632 \n",
      "Epoch [6/30], Step [ 1500/1875 ], d_loss: 1.0807, g_loss: 0.8535,g_score: 14.5357 \n",
      "Epoch [6/30], Step [ 1600/1875 ], d_loss: 1.0106, g_loss: 0.9291,g_score: 12.9862 \n",
      "Epoch [6/30], Step [ 1700/1875 ], d_loss: 1.2793, g_loss: 0.8386,g_score: 14.4419 \n",
      "Epoch [6/30], Step [ 1800/1875 ], d_loss: 1.4618, g_loss: 1.0124,g_score: 12.2797 \n",
      "7\n",
      "Epoch [7/30], Step [ 0/1875 ], d_loss: 1.2107, g_loss: 1.0436,g_score: 11.6728 \n",
      "Epoch [7/30], Step [ 100/1875 ], d_loss: 1.2630, g_loss: 0.8340,g_score: 14.3121 \n",
      "Epoch [7/30], Step [ 200/1875 ], d_loss: 0.9615, g_loss: 0.9047,g_score: 13.4555 \n",
      "Epoch [7/30], Step [ 300/1875 ], d_loss: 1.7581, g_loss: 0.8545,g_score: 14.3490 \n",
      "Epoch [7/30], Step [ 400/1875 ], d_loss: 0.8318, g_loss: 1.0575,g_score: 11.2882 \n",
      "Epoch [7/30], Step [ 500/1875 ], d_loss: 1.3710, g_loss: 0.9494,g_score: 12.8189 \n",
      "Epoch [7/30], Step [ 600/1875 ], d_loss: 1.1653, g_loss: 0.8186,g_score: 14.3597 \n",
      "Epoch [7/30], Step [ 700/1875 ], d_loss: 1.0914, g_loss: 1.0795,g_score: 11.0899 \n",
      "Epoch [7/30], Step [ 800/1875 ], d_loss: 1.2318, g_loss: 0.9038,g_score: 13.1790 \n",
      "Epoch [7/30], Step [ 900/1875 ], d_loss: 1.3046, g_loss: 0.9157,g_score: 13.0837 \n",
      "Epoch [7/30], Step [ 1000/1875 ], d_loss: 1.0840, g_loss: 0.8055,g_score: 14.6570 \n",
      "Epoch [7/30], Step [ 1100/1875 ], d_loss: 1.1913, g_loss: 1.0092,g_score: 11.9828 \n",
      "Epoch [7/30], Step [ 1200/1875 ], d_loss: 1.2529, g_loss: 1.0121,g_score: 11.9676 \n",
      "Epoch [7/30], Step [ 1300/1875 ], d_loss: 1.2543, g_loss: 0.9966,g_score: 12.1122 \n",
      "Epoch [7/30], Step [ 1400/1875 ], d_loss: 1.4451, g_loss: 0.8668,g_score: 13.9761 \n",
      "Epoch [7/30], Step [ 1500/1875 ], d_loss: 0.8146, g_loss: 0.9389,g_score: 12.8368 \n",
      "Epoch [7/30], Step [ 1600/1875 ], d_loss: 1.5821, g_loss: 0.9327,g_score: 12.9883 \n",
      "Epoch [7/30], Step [ 1700/1875 ], d_loss: 0.8828, g_loss: 1.0216,g_score: 11.7678 \n",
      "Epoch [7/30], Step [ 1800/1875 ], d_loss: 1.3820, g_loss: 0.9262,g_score: 13.3332 \n",
      "8\n",
      "Epoch [8/30], Step [ 0/1875 ], d_loss: 1.0257, g_loss: 1.0071,g_score: 12.2336 \n",
      "Epoch [8/30], Step [ 100/1875 ], d_loss: 1.2628, g_loss: 1.0981,g_score: 10.8473 \n",
      "Epoch [8/30], Step [ 200/1875 ], d_loss: 0.9138, g_loss: 0.9531,g_score: 13.0155 \n",
      "Epoch [8/30], Step [ 300/1875 ], d_loss: 1.0463, g_loss: 0.9004,g_score: 13.5510 \n",
      "Epoch [8/30], Step [ 400/1875 ], d_loss: 0.9999, g_loss: 0.8883,g_score: 13.6675 \n",
      "Epoch [8/30], Step [ 500/1875 ], d_loss: 1.6336, g_loss: 0.8536,g_score: 14.1689 \n",
      "Epoch [8/30], Step [ 600/1875 ], d_loss: 0.9370, g_loss: 0.8813,g_score: 13.5523 \n",
      "Epoch [8/30], Step [ 700/1875 ], d_loss: 1.5232, g_loss: 0.7523,g_score: 15.8662 \n",
      "Epoch [8/30], Step [ 800/1875 ], d_loss: 1.2713, g_loss: 1.1010,g_score: 10.8964 \n",
      "Epoch [8/30], Step [ 900/1875 ], d_loss: 1.1428, g_loss: 0.9405,g_score: 12.9002 \n",
      "Epoch [8/30], Step [ 1000/1875 ], d_loss: 1.4324, g_loss: 0.8337,g_score: 14.3612 \n",
      "Epoch [8/30], Step [ 1100/1875 ], d_loss: 0.9118, g_loss: 0.9266,g_score: 13.0810 \n",
      "Epoch [8/30], Step [ 1200/1875 ], d_loss: 1.1889, g_loss: 0.9737,g_score: 12.4613 \n",
      "Epoch [8/30], Step [ 1300/1875 ], d_loss: 1.3850, g_loss: 0.7808,g_score: 15.8950 \n",
      "Epoch [8/30], Step [ 1400/1875 ], d_loss: 1.2314, g_loss: 0.8995,g_score: 13.5801 \n",
      "Epoch [8/30], Step [ 1500/1875 ], d_loss: 0.9730, g_loss: 0.9494,g_score: 12.7303 \n",
      "Epoch [8/30], Step [ 1600/1875 ], d_loss: 1.0162, g_loss: 1.1999,g_score: 10.0190 \n",
      "Epoch [8/30], Step [ 1700/1875 ], d_loss: 1.2713, g_loss: 0.9094,g_score: 13.3966 \n",
      "Epoch [8/30], Step [ 1800/1875 ], d_loss: 1.4385, g_loss: 0.9924,g_score: 12.6922 \n",
      "9\n",
      "Epoch [9/30], Step [ 0/1875 ], d_loss: 1.0399, g_loss: 0.9940,g_score: 12.6561 \n",
      "Epoch [9/30], Step [ 100/1875 ], d_loss: 0.9063, g_loss: 1.2553,g_score: 9.6848 \n",
      "Epoch [9/30], Step [ 200/1875 ], d_loss: 1.4439, g_loss: 0.9640,g_score: 12.8094 \n",
      "Epoch [9/30], Step [ 300/1875 ], d_loss: 0.9149, g_loss: 0.8546,g_score: 13.9270 \n",
      "Epoch [9/30], Step [ 400/1875 ], d_loss: 1.2464, g_loss: 0.8929,g_score: 13.5929 \n",
      "Epoch [9/30], Step [ 500/1875 ], d_loss: 1.0797, g_loss: 0.7867,g_score: 14.9824 \n",
      "Epoch [9/30], Step [ 600/1875 ], d_loss: 0.8598, g_loss: 0.9661,g_score: 12.4699 \n",
      "Epoch [9/30], Step [ 700/1875 ], d_loss: 1.3346, g_loss: 0.9646,g_score: 12.4935 \n",
      "Epoch [9/30], Step [ 800/1875 ], d_loss: 1.0667, g_loss: 0.8585,g_score: 14.1362 \n",
      "Epoch [9/30], Step [ 900/1875 ], d_loss: 1.2231, g_loss: 0.9564,g_score: 12.8607 \n",
      "Epoch [9/30], Step [ 1000/1875 ], d_loss: 0.9009, g_loss: 0.9684,g_score: 12.3626 \n",
      "Epoch [9/30], Step [ 1100/1875 ], d_loss: 1.2406, g_loss: 1.2345,g_score: 9.5201 \n",
      "Epoch [9/30], Step [ 1200/1875 ], d_loss: 1.0700, g_loss: 0.9258,g_score: 12.9938 \n",
      "Epoch [9/30], Step [ 1300/1875 ], d_loss: 0.9296, g_loss: 0.9240,g_score: 12.9748 \n",
      "Epoch [9/30], Step [ 1400/1875 ], d_loss: 1.4643, g_loss: 0.8461,g_score: 14.2472 \n",
      "Epoch [9/30], Step [ 1500/1875 ], d_loss: 0.8996, g_loss: 1.0995,g_score: 10.9992 \n",
      "Epoch [9/30], Step [ 1600/1875 ], d_loss: 1.5867, g_loss: 0.8469,g_score: 14.3895 \n",
      "Epoch [9/30], Step [ 1700/1875 ], d_loss: 1.0637, g_loss: 0.8688,g_score: 13.9032 \n",
      "Epoch [9/30], Step [ 1800/1875 ], d_loss: 1.1804, g_loss: 0.7856,g_score: 15.0927 \n",
      "10\n",
      "Epoch [10/30], Step [ 0/1875 ], d_loss: 1.4479, g_loss: 1.0519,g_score: 11.6088 \n",
      "Epoch [10/30], Step [ 100/1875 ], d_loss: 1.0386, g_loss: 0.9715,g_score: 12.2868 \n",
      "Epoch [10/30], Step [ 200/1875 ], d_loss: 1.2199, g_loss: 1.0046,g_score: 12.1481 \n",
      "Epoch [10/30], Step [ 300/1875 ], d_loss: 1.5543, g_loss: 1.0557,g_score: 11.5214 \n",
      "Epoch [10/30], Step [ 400/1875 ], d_loss: 0.9632, g_loss: 0.9674,g_score: 12.6852 \n",
      "Epoch [10/30], Step [ 500/1875 ], d_loss: 1.2656, g_loss: 0.6708,g_score: 16.6967 \n",
      "Epoch [10/30], Step [ 600/1875 ], d_loss: 1.0166, g_loss: 0.9197,g_score: 13.3880 \n",
      "Epoch [10/30], Step [ 700/1875 ], d_loss: 1.0316, g_loss: 0.9986,g_score: 12.0357 \n",
      "Epoch [10/30], Step [ 800/1875 ], d_loss: 0.9673, g_loss: 0.8279,g_score: 14.6072 \n",
      "Epoch [10/30], Step [ 900/1875 ], d_loss: 1.0412, g_loss: 0.9621,g_score: 12.7764 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Step [ 1000/1875 ], d_loss: 1.1214, g_loss: 1.0540,g_score: 11.3686 \n",
      "Epoch [10/30], Step [ 1100/1875 ], d_loss: 1.1401, g_loss: 0.9921,g_score: 12.0724 \n",
      "Epoch [10/30], Step [ 1200/1875 ], d_loss: 1.0663, g_loss: 1.0138,g_score: 12.0260 \n",
      "Epoch [10/30], Step [ 1300/1875 ], d_loss: 1.3640, g_loss: 1.1877,g_score: 10.1296 \n",
      "Epoch [10/30], Step [ 1400/1875 ], d_loss: 0.9598, g_loss: 1.0069,g_score: 12.1430 \n",
      "Epoch [10/30], Step [ 1500/1875 ], d_loss: 0.9868, g_loss: 0.9510,g_score: 12.6643 \n",
      "Epoch [10/30], Step [ 1600/1875 ], d_loss: 1.0480, g_loss: 1.0410,g_score: 11.8867 \n",
      "Epoch [10/30], Step [ 1700/1875 ], d_loss: 1.0778, g_loss: 0.8712,g_score: 13.7596 \n",
      "Epoch [10/30], Step [ 1800/1875 ], d_loss: 0.9885, g_loss: 0.9247,g_score: 13.2509 \n",
      "11\n",
      "Epoch [11/30], Step [ 0/1875 ], d_loss: 1.0309, g_loss: 1.0858,g_score: 11.1006 \n",
      "Epoch [11/30], Step [ 100/1875 ], d_loss: 1.0507, g_loss: 1.0912,g_score: 10.9454 \n",
      "Epoch [11/30], Step [ 200/1875 ], d_loss: 1.1595, g_loss: 0.9410,g_score: 12.9067 \n",
      "Epoch [11/30], Step [ 300/1875 ], d_loss: 0.9198, g_loss: 1.1428,g_score: 10.4522 \n",
      "Epoch [11/30], Step [ 400/1875 ], d_loss: 1.2836, g_loss: 1.0091,g_score: 12.1001 \n",
      "Epoch [11/30], Step [ 500/1875 ], d_loss: 1.1888, g_loss: 1.0248,g_score: 11.8165 \n",
      "Epoch [11/30], Step [ 600/1875 ], d_loss: 0.9973, g_loss: 0.9350,g_score: 13.0191 \n",
      "Epoch [11/30], Step [ 700/1875 ], d_loss: 1.3232, g_loss: 0.8851,g_score: 13.6305 \n",
      "Epoch [11/30], Step [ 800/1875 ], d_loss: 0.8005, g_loss: 0.7965,g_score: 14.9817 \n",
      "Epoch [11/30], Step [ 900/1875 ], d_loss: 1.1877, g_loss: 1.1866,g_score: 10.0246 \n",
      "Epoch [11/30], Step [ 1000/1875 ], d_loss: 1.3524, g_loss: 0.9255,g_score: 13.4226 \n",
      "Epoch [11/30], Step [ 1100/1875 ], d_loss: 0.9201, g_loss: 0.9676,g_score: 12.7034 \n",
      "Epoch [11/30], Step [ 1200/1875 ], d_loss: 1.1629, g_loss: 0.9380,g_score: 12.8423 \n",
      "Epoch [11/30], Step [ 1300/1875 ], d_loss: 1.1205, g_loss: 0.9171,g_score: 13.2222 \n",
      "Epoch [11/30], Step [ 1400/1875 ], d_loss: 1.0102, g_loss: 0.9772,g_score: 13.1116 \n",
      "Epoch [11/30], Step [ 1500/1875 ], d_loss: 1.3038, g_loss: 1.1056,g_score: 11.3147 \n",
      "Epoch [11/30], Step [ 1600/1875 ], d_loss: 1.0779, g_loss: 0.8371,g_score: 14.4340 \n",
      "Epoch [11/30], Step [ 1700/1875 ], d_loss: 1.2756, g_loss: 1.3035,g_score: 9.0383 \n",
      "Epoch [11/30], Step [ 1800/1875 ], d_loss: 1.0058, g_loss: 0.9098,g_score: 13.3007 \n",
      "12\n",
      "Epoch [12/30], Step [ 0/1875 ], d_loss: 1.1255, g_loss: 0.9402,g_score: 13.1676 \n",
      "Epoch [12/30], Step [ 100/1875 ], d_loss: 1.1231, g_loss: 1.0546,g_score: 11.5744 \n",
      "Epoch [12/30], Step [ 200/1875 ], d_loss: 0.9237, g_loss: 1.0167,g_score: 11.9374 \n",
      "Epoch [12/30], Step [ 300/1875 ], d_loss: 1.1006, g_loss: 1.0209,g_score: 12.0322 \n",
      "Epoch [12/30], Step [ 400/1875 ], d_loss: 1.0265, g_loss: 0.9679,g_score: 12.7528 \n",
      "Epoch [12/30], Step [ 500/1875 ], d_loss: 0.9230, g_loss: 1.0370,g_score: 11.8407 \n",
      "Epoch [12/30], Step [ 600/1875 ], d_loss: 1.0542, g_loss: 0.9828,g_score: 12.6498 \n",
      "Epoch [12/30], Step [ 700/1875 ], d_loss: 1.0843, g_loss: 1.0912,g_score: 11.1105 \n",
      "Epoch [12/30], Step [ 800/1875 ], d_loss: 1.1299, g_loss: 0.9031,g_score: 13.4296 \n",
      "Epoch [12/30], Step [ 900/1875 ], d_loss: 1.0622, g_loss: 1.2417,g_score: 9.5592 \n",
      "Epoch [12/30], Step [ 1000/1875 ], d_loss: 1.0702, g_loss: 1.0073,g_score: 12.0147 \n",
      "Epoch [12/30], Step [ 1100/1875 ], d_loss: 1.0920, g_loss: 0.9270,g_score: 13.3213 \n",
      "Epoch [12/30], Step [ 1200/1875 ], d_loss: 1.1206, g_loss: 1.1690,g_score: 10.1976 \n",
      "Epoch [12/30], Step [ 1300/1875 ], d_loss: 0.9477, g_loss: 1.0234,g_score: 11.9830 \n",
      "Epoch [12/30], Step [ 1400/1875 ], d_loss: 1.1588, g_loss: 0.8090,g_score: 14.9523 \n",
      "Epoch [12/30], Step [ 1500/1875 ], d_loss: 1.1709, g_loss: 1.0957,g_score: 10.9969 \n",
      "Epoch [12/30], Step [ 1600/1875 ], d_loss: 1.0276, g_loss: 1.0186,g_score: 12.0802 \n",
      "Epoch [12/30], Step [ 1700/1875 ], d_loss: 1.0764, g_loss: 0.9678,g_score: 12.7873 \n",
      "Epoch [12/30], Step [ 1800/1875 ], d_loss: 1.0459, g_loss: 1.0860,g_score: 11.2079 \n",
      "13\n",
      "Epoch [13/30], Step [ 0/1875 ], d_loss: 0.9391, g_loss: 0.9716,g_score: 12.4858 \n",
      "Epoch [13/30], Step [ 100/1875 ], d_loss: 1.1600, g_loss: 0.9351,g_score: 12.9613 \n",
      "Epoch [13/30], Step [ 200/1875 ], d_loss: 0.9422, g_loss: 1.0346,g_score: 11.5452 \n",
      "Epoch [13/30], Step [ 300/1875 ], d_loss: 1.2859, g_loss: 1.1498,g_score: 10.8864 \n",
      "Epoch [13/30], Step [ 400/1875 ], d_loss: 0.8539, g_loss: 1.0854,g_score: 11.0806 \n",
      "Epoch [13/30], Step [ 500/1875 ], d_loss: 1.0180, g_loss: 0.8509,g_score: 14.4151 \n",
      "Epoch [13/30], Step [ 600/1875 ], d_loss: 1.4960, g_loss: 1.2707,g_score: 9.5830 \n",
      "Epoch [13/30], Step [ 700/1875 ], d_loss: 0.9901, g_loss: 1.0065,g_score: 12.2149 \n",
      "Epoch [13/30], Step [ 800/1875 ], d_loss: 1.3544, g_loss: 0.9187,g_score: 13.2474 \n",
      "Epoch [13/30], Step [ 900/1875 ], d_loss: 0.9933, g_loss: 0.9032,g_score: 13.8079 \n",
      "Epoch [13/30], Step [ 1000/1875 ], d_loss: 1.0029, g_loss: 0.8977,g_score: 13.5505 \n",
      "Epoch [13/30], Step [ 1100/1875 ], d_loss: 1.2789, g_loss: 1.2766,g_score: 9.3616 \n",
      "Epoch [13/30], Step [ 1200/1875 ], d_loss: 1.2575, g_loss: 0.8615,g_score: 14.2547 \n",
      "Epoch [13/30], Step [ 1300/1875 ], d_loss: 1.2554, g_loss: 0.9958,g_score: 12.4495 \n",
      "Epoch [13/30], Step [ 1400/1875 ], d_loss: 0.9083, g_loss: 1.1858,g_score: 10.0950 \n",
      "Epoch [13/30], Step [ 1500/1875 ], d_loss: 1.1542, g_loss: 1.2957,g_score: 9.1226 \n",
      "Epoch [13/30], Step [ 1600/1875 ], d_loss: 1.0540, g_loss: 0.9795,g_score: 12.2678 \n",
      "Epoch [13/30], Step [ 1700/1875 ], d_loss: 1.0309, g_loss: 0.9439,g_score: 12.7866 \n",
      "Epoch [13/30], Step [ 1800/1875 ], d_loss: 1.2899, g_loss: 1.0582,g_score: 11.4628 \n",
      "14\n",
      "Epoch [14/30], Step [ 0/1875 ], d_loss: 1.1010, g_loss: 1.2104,g_score: 9.8388 \n",
      "Epoch [14/30], Step [ 100/1875 ], d_loss: 1.1396, g_loss: 1.1874,g_score: 10.2224 \n",
      "Epoch [14/30], Step [ 200/1875 ], d_loss: 1.3376, g_loss: 0.8146,g_score: 15.0666 \n",
      "Epoch [14/30], Step [ 300/1875 ], d_loss: 0.9916, g_loss: 1.0823,g_score: 11.1164 \n",
      "Epoch [14/30], Step [ 400/1875 ], d_loss: 1.2682, g_loss: 1.0728,g_score: 11.3985 \n",
      "Epoch [14/30], Step [ 500/1875 ], d_loss: 0.8139, g_loss: 0.9718,g_score: 12.4707 \n",
      "Epoch [14/30], Step [ 600/1875 ], d_loss: 1.2690, g_loss: 0.9535,g_score: 12.8805 \n",
      "Epoch [14/30], Step [ 700/1875 ], d_loss: 1.0652, g_loss: 0.9665,g_score: 12.7772 \n",
      "Epoch [14/30], Step [ 800/1875 ], d_loss: 1.1947, g_loss: 0.8734,g_score: 14.6841 \n",
      "Epoch [14/30], Step [ 900/1875 ], d_loss: 1.4183, g_loss: 1.0519,g_score: 11.7834 \n",
      "Epoch [14/30], Step [ 1000/1875 ], d_loss: 0.9727, g_loss: 1.0256,g_score: 11.9240 \n",
      "Epoch [14/30], Step [ 1100/1875 ], d_loss: 1.0386, g_loss: 1.1227,g_score: 11.1066 \n",
      "Epoch [14/30], Step [ 1200/1875 ], d_loss: 1.3184, g_loss: 1.0203,g_score: 11.9957 \n",
      "Epoch [14/30], Step [ 1300/1875 ], d_loss: 1.1731, g_loss: 0.9967,g_score: 12.3881 \n",
      "Epoch [14/30], Step [ 1400/1875 ], d_loss: 1.2493, g_loss: 0.9279,g_score: 13.3097 \n",
      "Epoch [14/30], Step [ 1500/1875 ], d_loss: 1.2176, g_loss: 0.8025,g_score: 15.1456 \n",
      "Epoch [14/30], Step [ 1600/1875 ], d_loss: 1.4132, g_loss: 1.0854,g_score: 11.0867 \n",
      "Epoch [14/30], Step [ 1700/1875 ], d_loss: 1.2193, g_loss: 1.4547,g_score: 7.8015 \n",
      "Epoch [14/30], Step [ 1800/1875 ], d_loss: 0.9706, g_loss: 0.9754,g_score: 12.6278 \n",
      "15\n",
      "Epoch [15/30], Step [ 0/1875 ], d_loss: 1.0981, g_loss: 0.8821,g_score: 13.5693 \n",
      "Epoch [15/30], Step [ 100/1875 ], d_loss: 1.0399, g_loss: 1.0882,g_score: 11.1312 \n",
      "Epoch [15/30], Step [ 200/1875 ], d_loss: 1.2333, g_loss: 1.1857,g_score: 10.6011 \n",
      "Epoch [15/30], Step [ 300/1875 ], d_loss: 1.1163, g_loss: 0.9723,g_score: 12.5449 \n",
      "Epoch [15/30], Step [ 400/1875 ], d_loss: 1.1496, g_loss: 0.8339,g_score: 14.7153 \n",
      "Epoch [15/30], Step [ 500/1875 ], d_loss: 1.3172, g_loss: 1.1845,g_score: 10.1929 \n",
      "Epoch [15/30], Step [ 600/1875 ], d_loss: 1.0029, g_loss: 0.9876,g_score: 12.3997 \n",
      "Epoch [15/30], Step [ 700/1875 ], d_loss: 1.2163, g_loss: 0.8852,g_score: 13.7325 \n",
      "Epoch [15/30], Step [ 800/1875 ], d_loss: 1.2036, g_loss: 1.1550,g_score: 10.3996 \n",
      "Epoch [15/30], Step [ 900/1875 ], d_loss: 1.2756, g_loss: 1.3100,g_score: 8.8931 \n",
      "Epoch [15/30], Step [ 1000/1875 ], d_loss: 1.0749, g_loss: 1.0206,g_score: 12.0166 \n",
      "Epoch [15/30], Step [ 1100/1875 ], d_loss: 1.2525, g_loss: 1.0452,g_score: 11.7899 \n",
      "Epoch [15/30], Step [ 1200/1875 ], d_loss: 1.1199, g_loss: 0.9951,g_score: 12.3302 \n",
      "Epoch [15/30], Step [ 1300/1875 ], d_loss: 1.0776, g_loss: 1.0885,g_score: 11.0688 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Step [ 1400/1875 ], d_loss: 1.1950, g_loss: 1.0220,g_score: 12.0711 \n",
      "Epoch [15/30], Step [ 1500/1875 ], d_loss: 0.9118, g_loss: 1.0307,g_score: 11.8878 \n",
      "Epoch [15/30], Step [ 1600/1875 ], d_loss: 1.2887, g_loss: 1.1289,g_score: 10.6618 \n",
      "Epoch [15/30], Step [ 1700/1875 ], d_loss: 0.9777, g_loss: 0.9734,g_score: 12.4539 \n",
      "Epoch [15/30], Step [ 1800/1875 ], d_loss: 1.0507, g_loss: 1.1584,g_score: 10.3886 \n",
      "16\n",
      "Epoch [16/30], Step [ 0/1875 ], d_loss: 1.2610, g_loss: 0.9244,g_score: 13.3270 \n",
      "Epoch [16/30], Step [ 100/1875 ], d_loss: 0.9076, g_loss: 1.1542,g_score: 10.3945 \n",
      "Epoch [16/30], Step [ 200/1875 ], d_loss: 0.9937, g_loss: 0.9534,g_score: 12.8218 \n",
      "Epoch [16/30], Step [ 300/1875 ], d_loss: 1.0684, g_loss: 1.0438,g_score: 11.7527 \n",
      "Epoch [16/30], Step [ 400/1875 ], d_loss: 1.0739, g_loss: 1.0958,g_score: 11.1423 \n",
      "Epoch [16/30], Step [ 500/1875 ], d_loss: 1.1660, g_loss: 1.1703,g_score: 10.4350 \n",
      "Epoch [16/30], Step [ 600/1875 ], d_loss: 0.7951, g_loss: 1.0011,g_score: 12.4824 \n",
      "Epoch [16/30], Step [ 700/1875 ], d_loss: 1.0208, g_loss: 1.0858,g_score: 11.2150 \n",
      "Epoch [16/30], Step [ 800/1875 ], d_loss: 1.1783, g_loss: 1.0805,g_score: 11.7935 \n",
      "Epoch [16/30], Step [ 900/1875 ], d_loss: 1.0784, g_loss: 0.9355,g_score: 13.3237 \n",
      "Epoch [16/30], Step [ 1000/1875 ], d_loss: 1.0767, g_loss: 0.9511,g_score: 12.7093 \n",
      "Epoch [16/30], Step [ 1100/1875 ], d_loss: 1.1827, g_loss: 1.3949,g_score: 8.7317 \n",
      "Epoch [16/30], Step [ 1200/1875 ], d_loss: 1.2148, g_loss: 0.8103,g_score: 15.0877 \n",
      "Epoch [16/30], Step [ 1300/1875 ], d_loss: 0.8510, g_loss: 1.1043,g_score: 11.3160 \n",
      "Epoch [16/30], Step [ 1400/1875 ], d_loss: 1.2629, g_loss: 0.8376,g_score: 14.8338 \n",
      "Epoch [16/30], Step [ 1500/1875 ], d_loss: 1.2671, g_loss: 0.9769,g_score: 12.6247 \n",
      "Epoch [16/30], Step [ 1600/1875 ], d_loss: 1.0815, g_loss: 1.1302,g_score: 11.1656 \n",
      "Epoch [16/30], Step [ 1700/1875 ], d_loss: 1.1279, g_loss: 1.0318,g_score: 11.7578 \n",
      "Epoch [16/30], Step [ 1800/1875 ], d_loss: 0.8884, g_loss: 1.0896,g_score: 11.3127 \n",
      "17\n",
      "Epoch [17/30], Step [ 0/1875 ], d_loss: 0.9875, g_loss: 1.0911,g_score: 11.1880 \n",
      "Epoch [17/30], Step [ 100/1875 ], d_loss: 1.0679, g_loss: 0.9615,g_score: 12.6423 \n",
      "Epoch [17/30], Step [ 200/1875 ], d_loss: 1.2616, g_loss: 1.0066,g_score: 12.7305 \n",
      "Epoch [17/30], Step [ 300/1875 ], d_loss: 1.0980, g_loss: 1.0896,g_score: 11.1731 \n",
      "Epoch [17/30], Step [ 400/1875 ], d_loss: 1.2211, g_loss: 1.1367,g_score: 10.7854 \n",
      "Epoch [17/30], Step [ 500/1875 ], d_loss: 1.0235, g_loss: 1.0631,g_score: 11.3890 \n",
      "Epoch [17/30], Step [ 600/1875 ], d_loss: 1.1913, g_loss: 1.1637,g_score: 10.5528 \n",
      "Epoch [17/30], Step [ 700/1875 ], d_loss: 1.2815, g_loss: 1.1126,g_score: 11.4068 \n",
      "Epoch [17/30], Step [ 800/1875 ], d_loss: 1.1875, g_loss: 0.9875,g_score: 12.6134 \n",
      "Epoch [17/30], Step [ 900/1875 ], d_loss: 0.9592, g_loss: 1.0978,g_score: 11.0773 \n",
      "Epoch [17/30], Step [ 1000/1875 ], d_loss: 1.2013, g_loss: 1.0699,g_score: 11.2504 \n",
      "Epoch [17/30], Step [ 1100/1875 ], d_loss: 1.0683, g_loss: 1.0828,g_score: 11.2589 \n",
      "Epoch [17/30], Step [ 1200/1875 ], d_loss: 1.2066, g_loss: 0.9425,g_score: 13.3157 \n",
      "Epoch [17/30], Step [ 1300/1875 ], d_loss: 1.1907, g_loss: 0.8775,g_score: 13.7340 \n",
      "Epoch [17/30], Step [ 1400/1875 ], d_loss: 0.9946, g_loss: 1.0645,g_score: 11.2901 \n",
      "Epoch [17/30], Step [ 1500/1875 ], d_loss: 1.0554, g_loss: 1.0712,g_score: 11.5687 \n",
      "Epoch [17/30], Step [ 1600/1875 ], d_loss: 1.1883, g_loss: 1.0665,g_score: 11.3768 \n",
      "Epoch [17/30], Step [ 1700/1875 ], d_loss: 1.2707, g_loss: 0.8689,g_score: 14.0463 \n",
      "Epoch [17/30], Step [ 1800/1875 ], d_loss: 1.0199, g_loss: 1.1091,g_score: 11.2390 \n",
      "18\n",
      "Epoch [18/30], Step [ 0/1875 ], d_loss: 1.1225, g_loss: 1.1369,g_score: 10.7375 \n",
      "Epoch [18/30], Step [ 100/1875 ], d_loss: 0.9525, g_loss: 0.9860,g_score: 12.5553 \n",
      "Epoch [18/30], Step [ 200/1875 ], d_loss: 0.9787, g_loss: 1.0792,g_score: 11.4419 \n",
      "Epoch [18/30], Step [ 300/1875 ], d_loss: 1.1170, g_loss: 1.1551,g_score: 10.6339 \n",
      "Epoch [18/30], Step [ 400/1875 ], d_loss: 1.5892, g_loss: 1.0077,g_score: 12.9930 \n",
      "Epoch [18/30], Step [ 500/1875 ], d_loss: 0.9939, g_loss: 1.1915,g_score: 10.2749 \n",
      "Epoch [18/30], Step [ 600/1875 ], d_loss: 0.9565, g_loss: 0.8564,g_score: 14.0505 \n",
      "Epoch [18/30], Step [ 700/1875 ], d_loss: 0.9761, g_loss: 1.1272,g_score: 10.9025 \n",
      "Epoch [18/30], Step [ 800/1875 ], d_loss: 0.9967, g_loss: 0.9444,g_score: 12.9174 \n",
      "Epoch [18/30], Step [ 900/1875 ], d_loss: 0.9203, g_loss: 1.3222,g_score: 9.1863 \n",
      "Epoch [18/30], Step [ 1000/1875 ], d_loss: 1.0055, g_loss: 1.1016,g_score: 11.2231 \n",
      "Epoch [18/30], Step [ 1100/1875 ], d_loss: 1.0858, g_loss: 1.1329,g_score: 10.6643 \n",
      "Epoch [18/30], Step [ 1200/1875 ], d_loss: 1.1028, g_loss: 1.1058,g_score: 10.8785 \n",
      "Epoch [18/30], Step [ 1300/1875 ], d_loss: 1.2821, g_loss: 1.3321,g_score: 9.0552 \n",
      "Epoch [18/30], Step [ 1400/1875 ], d_loss: 1.3825, g_loss: 0.8972,g_score: 13.8229 \n",
      "Epoch [18/30], Step [ 1500/1875 ], d_loss: 1.1558, g_loss: 1.3703,g_score: 8.5829 \n",
      "Epoch [18/30], Step [ 1600/1875 ], d_loss: 0.8654, g_loss: 0.9316,g_score: 13.3100 \n",
      "Epoch [18/30], Step [ 1700/1875 ], d_loss: 1.3117, g_loss: 1.1675,g_score: 10.5603 \n",
      "Epoch [18/30], Step [ 1800/1875 ], d_loss: 1.0412, g_loss: 1.0713,g_score: 11.5874 \n",
      "19\n",
      "Epoch [19/30], Step [ 0/1875 ], d_loss: 0.9397, g_loss: 1.0377,g_score: 12.0241 \n",
      "Epoch [19/30], Step [ 100/1875 ], d_loss: 1.2386, g_loss: 1.0097,g_score: 12.4241 \n",
      "Epoch [19/30], Step [ 200/1875 ], d_loss: 1.1948, g_loss: 1.2543,g_score: 9.5106 \n",
      "Epoch [19/30], Step [ 300/1875 ], d_loss: 1.2630, g_loss: 0.9607,g_score: 12.8105 \n",
      "Epoch [19/30], Step [ 400/1875 ], d_loss: 0.9309, g_loss: 0.9991,g_score: 12.0931 \n",
      "Epoch [19/30], Step [ 500/1875 ], d_loss: 0.9199, g_loss: 1.0810,g_score: 11.1263 \n",
      "Epoch [19/30], Step [ 600/1875 ], d_loss: 1.1615, g_loss: 1.0157,g_score: 12.0189 \n",
      "Epoch [19/30], Step [ 700/1875 ], d_loss: 1.0605, g_loss: 0.9266,g_score: 13.3836 \n",
      "Epoch [19/30], Step [ 800/1875 ], d_loss: 1.0966, g_loss: 1.0416,g_score: 11.6342 \n",
      "Epoch [19/30], Step [ 900/1875 ], d_loss: 1.2660, g_loss: 0.9142,g_score: 13.8254 \n",
      "Epoch [19/30], Step [ 1000/1875 ], d_loss: 1.0119, g_loss: 1.2624,g_score: 9.3888 \n",
      "Epoch [19/30], Step [ 1100/1875 ], d_loss: 1.1530, g_loss: 1.2276,g_score: 9.7759 \n",
      "Epoch [19/30], Step [ 1200/1875 ], d_loss: 1.2490, g_loss: 1.2318,g_score: 9.7284 \n",
      "Epoch [19/30], Step [ 1300/1875 ], d_loss: 0.9503, g_loss: 1.0017,g_score: 12.3070 \n",
      "Epoch [19/30], Step [ 1400/1875 ], d_loss: 1.0777, g_loss: 1.1331,g_score: 10.8107 \n",
      "Epoch [19/30], Step [ 1500/1875 ], d_loss: 1.2228, g_loss: 0.9358,g_score: 13.4181 \n",
      "Epoch [19/30], Step [ 1600/1875 ], d_loss: 1.1028, g_loss: 1.2257,g_score: 9.7726 \n",
      "Epoch [19/30], Step [ 1700/1875 ], d_loss: 1.0896, g_loss: 1.0208,g_score: 11.8937 \n",
      "Epoch [19/30], Step [ 1800/1875 ], d_loss: 1.4147, g_loss: 1.1392,g_score: 10.9824 \n",
      "20\n",
      "Epoch [20/30], Step [ 0/1875 ], d_loss: 0.9447, g_loss: 1.0843,g_score: 11.3540 \n",
      "Epoch [20/30], Step [ 100/1875 ], d_loss: 1.1365, g_loss: 1.0159,g_score: 12.1334 \n",
      "Epoch [20/30], Step [ 200/1875 ], d_loss: 1.2282, g_loss: 1.0535,g_score: 11.7596 \n",
      "Epoch [20/30], Step [ 300/1875 ], d_loss: 0.9249, g_loss: 0.9780,g_score: 12.6176 \n",
      "Epoch [20/30], Step [ 400/1875 ], d_loss: 1.1984, g_loss: 0.8542,g_score: 14.2698 \n",
      "Epoch [20/30], Step [ 500/1875 ], d_loss: 1.0942, g_loss: 0.9643,g_score: 12.6684 \n",
      "Epoch [20/30], Step [ 600/1875 ], d_loss: 1.0389, g_loss: 0.8594,g_score: 14.5695 \n",
      "Epoch [20/30], Step [ 700/1875 ], d_loss: 1.2672, g_loss: 1.1701,g_score: 10.2591 \n",
      "Epoch [20/30], Step [ 800/1875 ], d_loss: 1.0539, g_loss: 1.0027,g_score: 12.1839 \n",
      "Epoch [20/30], Step [ 900/1875 ], d_loss: 1.3733, g_loss: 1.0274,g_score: 12.6375 \n",
      "Epoch [20/30], Step [ 1000/1875 ], d_loss: 1.2238, g_loss: 0.9875,g_score: 12.5544 \n",
      "Epoch [20/30], Step [ 1100/1875 ], d_loss: 0.9197, g_loss: 1.1082,g_score: 10.8557 \n",
      "Epoch [20/30], Step [ 1200/1875 ], d_loss: 1.4530, g_loss: 1.0281,g_score: 12.0122 \n",
      "Epoch [20/30], Step [ 1300/1875 ], d_loss: 1.0358, g_loss: 1.1636,g_score: 10.5455 \n",
      "Epoch [20/30], Step [ 1400/1875 ], d_loss: 0.9642, g_loss: 0.9941,g_score: 12.3373 \n",
      "Epoch [20/30], Step [ 1500/1875 ], d_loss: 1.3050, g_loss: 1.1908,g_score: 10.2177 \n",
      "Epoch [20/30], Step [ 1600/1875 ], d_loss: 0.9832, g_loss: 1.3316,g_score: 8.8295 \n",
      "Epoch [20/30], Step [ 1700/1875 ], d_loss: 1.2242, g_loss: 1.2271,g_score: 9.9429 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Step [ 1800/1875 ], d_loss: 1.1123, g_loss: 1.1412,g_score: 10.8128 \n",
      "21\n",
      "Epoch [21/30], Step [ 0/1875 ], d_loss: 0.9765, g_loss: 1.1071,g_score: 11.0648 \n",
      "Epoch [21/30], Step [ 100/1875 ], d_loss: 1.1921, g_loss: 1.1284,g_score: 11.1702 \n",
      "Epoch [21/30], Step [ 200/1875 ], d_loss: 1.0975, g_loss: 0.8919,g_score: 13.8571 \n",
      "Epoch [21/30], Step [ 300/1875 ], d_loss: 1.0874, g_loss: 1.0807,g_score: 11.7158 \n",
      "Epoch [21/30], Step [ 400/1875 ], d_loss: 1.0105, g_loss: 0.9854,g_score: 12.4314 \n",
      "Epoch [21/30], Step [ 500/1875 ], d_loss: 1.6174, g_loss: 1.7587,g_score: 5.9486 \n",
      "Epoch [21/30], Step [ 600/1875 ], d_loss: 1.1340, g_loss: 1.0122,g_score: 12.1945 \n",
      "Epoch [21/30], Step [ 700/1875 ], d_loss: 0.9430, g_loss: 1.4215,g_score: 8.1937 \n",
      "Epoch [21/30], Step [ 800/1875 ], d_loss: 1.2007, g_loss: 1.0514,g_score: 11.9483 \n",
      "Epoch [21/30], Step [ 900/1875 ], d_loss: 1.2308, g_loss: 1.0970,g_score: 11.4087 \n",
      "Epoch [21/30], Step [ 1000/1875 ], d_loss: 1.3135, g_loss: 1.1106,g_score: 10.8819 \n",
      "Epoch [21/30], Step [ 1100/1875 ], d_loss: 0.9391, g_loss: 0.9100,g_score: 13.2874 \n",
      "Epoch [21/30], Step [ 1200/1875 ], d_loss: 1.0199, g_loss: 1.1189,g_score: 11.0969 \n",
      "Epoch [21/30], Step [ 1300/1875 ], d_loss: 1.3565, g_loss: 0.8977,g_score: 13.8973 \n",
      "Epoch [21/30], Step [ 1400/1875 ], d_loss: 1.3227, g_loss: 1.2658,g_score: 9.8046 \n",
      "Epoch [21/30], Step [ 1500/1875 ], d_loss: 0.8912, g_loss: 0.9910,g_score: 12.5788 \n",
      "Epoch [21/30], Step [ 1600/1875 ], d_loss: 1.0675, g_loss: 0.8938,g_score: 13.9215 \n",
      "Epoch [21/30], Step [ 1700/1875 ], d_loss: 1.2874, g_loss: 1.2386,g_score: 9.8270 \n",
      "Epoch [21/30], Step [ 1800/1875 ], d_loss: 0.9201, g_loss: 0.8734,g_score: 14.1084 \n",
      "22\n",
      "Epoch [22/30], Step [ 0/1875 ], d_loss: 0.8663, g_loss: 1.1182,g_score: 10.8678 \n",
      "Epoch [22/30], Step [ 100/1875 ], d_loss: 1.1892, g_loss: 1.3924,g_score: 8.2599 \n",
      "Epoch [22/30], Step [ 200/1875 ], d_loss: 0.9037, g_loss: 1.0797,g_score: 11.1014 \n",
      "Epoch [22/30], Step [ 300/1875 ], d_loss: 1.4294, g_loss: 0.8712,g_score: 13.9550 \n",
      "Epoch [22/30], Step [ 400/1875 ], d_loss: 1.4307, g_loss: 0.8231,g_score: 14.8791 \n",
      "Epoch [22/30], Step [ 500/1875 ], d_loss: 1.5530, g_loss: 0.9186,g_score: 13.3518 \n",
      "Epoch [22/30], Step [ 600/1875 ], d_loss: 0.9871, g_loss: 1.0312,g_score: 11.6829 \n",
      "Epoch [22/30], Step [ 700/1875 ], d_loss: 1.1025, g_loss: 1.1286,g_score: 11.0218 \n",
      "Epoch [22/30], Step [ 800/1875 ], d_loss: 1.0048, g_loss: 0.9587,g_score: 12.9872 \n",
      "Epoch [22/30], Step [ 900/1875 ], d_loss: 1.1682, g_loss: 1.0137,g_score: 12.2627 \n",
      "Epoch [22/30], Step [ 1000/1875 ], d_loss: 1.1385, g_loss: 0.9742,g_score: 12.4619 \n",
      "Epoch [22/30], Step [ 1100/1875 ], d_loss: 0.9526, g_loss: 1.0731,g_score: 11.6954 \n",
      "Epoch [22/30], Step [ 1200/1875 ], d_loss: 1.0403, g_loss: 1.2088,g_score: 9.9558 \n",
      "Epoch [22/30], Step [ 1300/1875 ], d_loss: 1.0941, g_loss: 1.0072,g_score: 11.9171 \n",
      "Epoch [22/30], Step [ 1400/1875 ], d_loss: 1.1174, g_loss: 1.0388,g_score: 11.7349 \n",
      "Epoch [22/30], Step [ 1500/1875 ], d_loss: 1.0076, g_loss: 1.1234,g_score: 11.0063 \n",
      "Epoch [22/30], Step [ 1600/1875 ], d_loss: 1.1804, g_loss: 1.3980,g_score: 8.3533 \n",
      "Epoch [22/30], Step [ 1700/1875 ], d_loss: 1.0871, g_loss: 0.8475,g_score: 14.4114 \n",
      "Epoch [22/30], Step [ 1800/1875 ], d_loss: 1.1177, g_loss: 0.8271,g_score: 14.7914 \n",
      "23\n",
      "Epoch [23/30], Step [ 0/1875 ], d_loss: 1.2276, g_loss: 1.0629,g_score: 11.4014 \n",
      "Epoch [23/30], Step [ 100/1875 ], d_loss: 1.0370, g_loss: 1.0146,g_score: 12.0977 \n",
      "Epoch [23/30], Step [ 200/1875 ], d_loss: 1.0993, g_loss: 1.2057,g_score: 10.0303 \n",
      "Epoch [23/30], Step [ 300/1875 ], d_loss: 1.0290, g_loss: 1.0136,g_score: 12.2718 \n",
      "Epoch [23/30], Step [ 400/1875 ], d_loss: 1.1523, g_loss: 1.1173,g_score: 10.9426 \n",
      "Epoch [23/30], Step [ 500/1875 ], d_loss: 1.0182, g_loss: 1.0332,g_score: 11.6344 \n",
      "Epoch [23/30], Step [ 600/1875 ], d_loss: 1.1937, g_loss: 1.1250,g_score: 10.5981 \n",
      "Epoch [23/30], Step [ 700/1875 ], d_loss: 1.0623, g_loss: 0.9591,g_score: 12.7925 \n",
      "Epoch [23/30], Step [ 800/1875 ], d_loss: 1.0440, g_loss: 1.1849,g_score: 10.1621 \n",
      "Epoch [23/30], Step [ 900/1875 ], d_loss: 1.0650, g_loss: 0.9851,g_score: 12.4219 \n",
      "Epoch [23/30], Step [ 1000/1875 ], d_loss: 1.0050, g_loss: 1.3202,g_score: 8.9868 \n",
      "Epoch [23/30], Step [ 1100/1875 ], d_loss: 0.8399, g_loss: 0.9703,g_score: 12.4394 \n",
      "Epoch [23/30], Step [ 1200/1875 ], d_loss: 1.3953, g_loss: 0.9326,g_score: 13.2366 \n",
      "Epoch [23/30], Step [ 1300/1875 ], d_loss: 1.0047, g_loss: 0.9563,g_score: 12.7016 \n",
      "Epoch [23/30], Step [ 1400/1875 ], d_loss: 1.1821, g_loss: 1.0821,g_score: 11.3372 \n",
      "Epoch [23/30], Step [ 1500/1875 ], d_loss: 1.2150, g_loss: 1.0048,g_score: 12.6086 \n",
      "Epoch [23/30], Step [ 1600/1875 ], d_loss: 1.2337, g_loss: 1.0657,g_score: 11.7239 \n",
      "Epoch [23/30], Step [ 1700/1875 ], d_loss: 1.0863, g_loss: 0.9593,g_score: 13.0611 \n",
      "Epoch [23/30], Step [ 1800/1875 ], d_loss: 1.0043, g_loss: 0.9245,g_score: 13.6325 \n",
      "24\n",
      "Epoch [24/30], Step [ 0/1875 ], d_loss: 1.0431, g_loss: 0.8913,g_score: 13.7051 \n",
      "Epoch [24/30], Step [ 100/1875 ], d_loss: 1.2155, g_loss: 1.1784,g_score: 10.3929 \n",
      "Epoch [24/30], Step [ 200/1875 ], d_loss: 1.0989, g_loss: 0.8245,g_score: 14.6840 \n",
      "Epoch [24/30], Step [ 300/1875 ], d_loss: 1.2731, g_loss: 0.9401,g_score: 12.8735 \n",
      "Epoch [24/30], Step [ 400/1875 ], d_loss: 0.9880, g_loss: 0.9287,g_score: 13.6518 \n",
      "Epoch [24/30], Step [ 500/1875 ], d_loss: 1.1600, g_loss: 1.0928,g_score: 11.3774 \n",
      "Epoch [24/30], Step [ 600/1875 ], d_loss: 1.2132, g_loss: 1.1106,g_score: 11.1353 \n",
      "Epoch [24/30], Step [ 700/1875 ], d_loss: 1.1612, g_loss: 1.0391,g_score: 11.8930 \n",
      "Epoch [24/30], Step [ 800/1875 ], d_loss: 0.9729, g_loss: 1.0771,g_score: 11.2552 \n",
      "Epoch [24/30], Step [ 900/1875 ], d_loss: 1.1746, g_loss: 1.2531,g_score: 9.3803 \n",
      "Epoch [24/30], Step [ 1000/1875 ], d_loss: 0.8829, g_loss: 0.9694,g_score: 12.7595 \n",
      "Epoch [24/30], Step [ 1100/1875 ], d_loss: 1.0120, g_loss: 1.0095,g_score: 11.9064 \n",
      "Epoch [24/30], Step [ 1200/1875 ], d_loss: 1.1614, g_loss: 0.8486,g_score: 14.8065 \n",
      "Epoch [24/30], Step [ 1300/1875 ], d_loss: 1.2946, g_loss: 0.8105,g_score: 14.8818 \n",
      "Epoch [24/30], Step [ 1400/1875 ], d_loss: 1.1498, g_loss: 1.0504,g_score: 11.5940 \n",
      "Epoch [24/30], Step [ 1500/1875 ], d_loss: 1.0198, g_loss: 1.2894,g_score: 9.6534 \n",
      "Epoch [24/30], Step [ 1600/1875 ], d_loss: 1.2827, g_loss: 0.7899,g_score: 15.4148 \n",
      "Epoch [24/30], Step [ 1700/1875 ], d_loss: 1.3615, g_loss: 1.1378,g_score: 10.8275 \n",
      "Epoch [24/30], Step [ 1800/1875 ], d_loss: 1.1387, g_loss: 0.8383,g_score: 14.6702 \n",
      "25\n",
      "Epoch [25/30], Step [ 0/1875 ], d_loss: 1.2876, g_loss: 1.0672,g_score: 11.2989 \n",
      "Epoch [25/30], Step [ 100/1875 ], d_loss: 1.1029, g_loss: 0.9948,g_score: 12.4936 \n",
      "Epoch [25/30], Step [ 200/1875 ], d_loss: 1.3405, g_loss: 1.1622,g_score: 10.8449 \n",
      "Epoch [25/30], Step [ 300/1875 ], d_loss: 1.2594, g_loss: 0.9769,g_score: 12.6595 \n",
      "Epoch [25/30], Step [ 400/1875 ], d_loss: 1.0744, g_loss: 1.3721,g_score: 8.5969 \n",
      "Epoch [25/30], Step [ 500/1875 ], d_loss: 1.1190, g_loss: 1.0404,g_score: 11.9562 \n",
      "Epoch [25/30], Step [ 600/1875 ], d_loss: 1.4017, g_loss: 1.1475,g_score: 10.3677 \n",
      "Epoch [25/30], Step [ 700/1875 ], d_loss: 1.3327, g_loss: 0.9104,g_score: 14.3009 \n",
      "Epoch [25/30], Step [ 800/1875 ], d_loss: 1.2157, g_loss: 1.3312,g_score: 8.7839 \n",
      "Epoch [25/30], Step [ 900/1875 ], d_loss: 1.1328, g_loss: 1.1892,g_score: 10.2031 \n",
      "Epoch [25/30], Step [ 1000/1875 ], d_loss: 1.1898, g_loss: 1.1172,g_score: 11.0314 \n",
      "Epoch [25/30], Step [ 1100/1875 ], d_loss: 1.0517, g_loss: 1.1953,g_score: 10.2563 \n",
      "Epoch [25/30], Step [ 1200/1875 ], d_loss: 1.2096, g_loss: 1.2354,g_score: 9.7872 \n",
      "Epoch [25/30], Step [ 1300/1875 ], d_loss: 0.9674, g_loss: 1.0917,g_score: 11.2887 \n",
      "Epoch [25/30], Step [ 1400/1875 ], d_loss: 1.1090, g_loss: 0.9721,g_score: 12.7286 \n",
      "Epoch [25/30], Step [ 1500/1875 ], d_loss: 1.1053, g_loss: 1.0813,g_score: 11.2101 \n",
      "Epoch [25/30], Step [ 1600/1875 ], d_loss: 1.0788, g_loss: 1.1595,g_score: 10.5498 \n",
      "Epoch [25/30], Step [ 1700/1875 ], d_loss: 0.9888, g_loss: 0.9909,g_score: 12.5938 \n",
      "Epoch [25/30], Step [ 1800/1875 ], d_loss: 1.0411, g_loss: 1.1047,g_score: 11.1176 \n",
      "26\n",
      "Epoch [26/30], Step [ 0/1875 ], d_loss: 1.1800, g_loss: 1.2304,g_score: 10.0099 \n",
      "Epoch [26/30], Step [ 100/1875 ], d_loss: 1.4220, g_loss: 0.8151,g_score: 14.9428 \n",
      "Epoch [26/30], Step [ 200/1875 ], d_loss: 1.1193, g_loss: 0.8581,g_score: 14.1394 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Step [ 300/1875 ], d_loss: 1.0825, g_loss: 0.9623,g_score: 12.7909 \n",
      "Epoch [26/30], Step [ 400/1875 ], d_loss: 1.2299, g_loss: 0.8172,g_score: 15.0566 \n",
      "Epoch [26/30], Step [ 500/1875 ], d_loss: 1.0104, g_loss: 1.0820,g_score: 11.7187 \n",
      "Epoch [26/30], Step [ 600/1875 ], d_loss: 1.1194, g_loss: 0.9589,g_score: 13.0290 \n",
      "Epoch [26/30], Step [ 700/1875 ], d_loss: 1.3107, g_loss: 0.8690,g_score: 14.4893 \n",
      "Epoch [26/30], Step [ 800/1875 ], d_loss: 1.2808, g_loss: 0.8781,g_score: 13.9104 \n",
      "Epoch [26/30], Step [ 900/1875 ], d_loss: 1.0208, g_loss: 0.9205,g_score: 13.1359 \n",
      "Epoch [26/30], Step [ 1000/1875 ], d_loss: 1.1005, g_loss: 1.4761,g_score: 7.7213 \n",
      "Epoch [26/30], Step [ 1100/1875 ], d_loss: 1.0147, g_loss: 0.9872,g_score: 12.6150 \n",
      "Epoch [26/30], Step [ 1200/1875 ], d_loss: 1.0367, g_loss: 1.0559,g_score: 11.9795 \n",
      "Epoch [26/30], Step [ 1300/1875 ], d_loss: 1.0665, g_loss: 0.9133,g_score: 13.4610 \n",
      "Epoch [26/30], Step [ 1400/1875 ], d_loss: 1.0668, g_loss: 1.0690,g_score: 11.6246 \n",
      "Epoch [26/30], Step [ 1500/1875 ], d_loss: 1.2111, g_loss: 1.1152,g_score: 10.7179 \n",
      "Epoch [26/30], Step [ 1600/1875 ], d_loss: 1.0776, g_loss: 1.2829,g_score: 9.1370 \n",
      "Epoch [26/30], Step [ 1700/1875 ], d_loss: 1.0060, g_loss: 1.2728,g_score: 9.5799 \n",
      "Epoch [26/30], Step [ 1800/1875 ], d_loss: 1.2986, g_loss: 1.0021,g_score: 12.5142 \n",
      "27\n",
      "Epoch [27/30], Step [ 0/1875 ], d_loss: 1.1352, g_loss: 1.0434,g_score: 11.8389 \n",
      "Epoch [27/30], Step [ 100/1875 ], d_loss: 0.7983, g_loss: 1.1114,g_score: 11.0716 \n",
      "Epoch [27/30], Step [ 200/1875 ], d_loss: 1.2965, g_loss: 1.0309,g_score: 12.0377 \n",
      "Epoch [27/30], Step [ 300/1875 ], d_loss: 1.1506, g_loss: 1.1042,g_score: 11.4071 \n",
      "Epoch [27/30], Step [ 400/1875 ], d_loss: 0.9074, g_loss: 1.3093,g_score: 9.0044 \n",
      "Epoch [27/30], Step [ 500/1875 ], d_loss: 1.0328, g_loss: 0.9715,g_score: 12.8147 \n",
      "Epoch [27/30], Step [ 600/1875 ], d_loss: 1.0240, g_loss: 1.1056,g_score: 11.1471 \n",
      "Epoch [27/30], Step [ 700/1875 ], d_loss: 1.1149, g_loss: 1.1171,g_score: 11.1766 \n",
      "Epoch [27/30], Step [ 800/1875 ], d_loss: 1.1830, g_loss: 1.0021,g_score: 12.6219 \n",
      "Epoch [27/30], Step [ 900/1875 ], d_loss: 1.1331, g_loss: 0.9449,g_score: 13.0905 \n",
      "Epoch [27/30], Step [ 1000/1875 ], d_loss: 0.8508, g_loss: 1.1198,g_score: 10.8169 \n",
      "Epoch [27/30], Step [ 1100/1875 ], d_loss: 1.3488, g_loss: 1.2055,g_score: 9.8506 \n",
      "Epoch [27/30], Step [ 1200/1875 ], d_loss: 1.3062, g_loss: 1.1544,g_score: 10.6245 \n",
      "Epoch [27/30], Step [ 1300/1875 ], d_loss: 0.9167, g_loss: 1.0051,g_score: 12.1406 \n",
      "Epoch [27/30], Step [ 1400/1875 ], d_loss: 0.9775, g_loss: 0.9253,g_score: 13.1065 \n",
      "Epoch [27/30], Step [ 1500/1875 ], d_loss: 1.1713, g_loss: 1.2165,g_score: 9.9787 \n",
      "Epoch [27/30], Step [ 1600/1875 ], d_loss: 1.1492, g_loss: 0.9224,g_score: 13.2860 \n",
      "Epoch [27/30], Step [ 1700/1875 ], d_loss: 0.9248, g_loss: 1.1782,g_score: 10.1396 \n",
      "Epoch [27/30], Step [ 1800/1875 ], d_loss: 1.0543, g_loss: 1.0265,g_score: 11.9994 \n",
      "28\n",
      "Epoch [28/30], Step [ 0/1875 ], d_loss: 0.9874, g_loss: 0.9187,g_score: 13.6801 \n",
      "Epoch [28/30], Step [ 100/1875 ], d_loss: 1.0706, g_loss: 0.9713,g_score: 12.7610 \n",
      "Epoch [28/30], Step [ 200/1875 ], d_loss: 1.1537, g_loss: 0.9218,g_score: 13.5745 \n",
      "Epoch [28/30], Step [ 300/1875 ], d_loss: 1.0442, g_loss: 0.9506,g_score: 12.8929 \n",
      "Epoch [28/30], Step [ 400/1875 ], d_loss: 1.0856, g_loss: 0.9340,g_score: 13.4893 \n",
      "Epoch [28/30], Step [ 500/1875 ], d_loss: 1.1570, g_loss: 1.0690,g_score: 11.4147 \n",
      "Epoch [28/30], Step [ 600/1875 ], d_loss: 1.0210, g_loss: 1.1650,g_score: 10.3959 \n",
      "Epoch [28/30], Step [ 700/1875 ], d_loss: 1.0476, g_loss: 0.9959,g_score: 12.4805 \n",
      "Epoch [28/30], Step [ 800/1875 ], d_loss: 1.1403, g_loss: 1.0906,g_score: 11.5674 \n",
      "Epoch [28/30], Step [ 900/1875 ], d_loss: 0.9208, g_loss: 0.8019,g_score: 15.0787 \n",
      "Epoch [28/30], Step [ 1000/1875 ], d_loss: 1.0543, g_loss: 1.0916,g_score: 11.5685 \n",
      "Epoch [28/30], Step [ 1100/1875 ], d_loss: 1.0757, g_loss: 1.0545,g_score: 11.8574 \n",
      "Epoch [28/30], Step [ 1200/1875 ], d_loss: 1.3189, g_loss: 1.0016,g_score: 12.2952 \n",
      "Epoch [28/30], Step [ 1300/1875 ], d_loss: 1.1017, g_loss: 1.3126,g_score: 9.0648 \n",
      "Epoch [28/30], Step [ 1400/1875 ], d_loss: 1.1658, g_loss: 1.0488,g_score: 11.8463 \n",
      "Epoch [28/30], Step [ 1500/1875 ], d_loss: 1.1879, g_loss: 1.1441,g_score: 10.5502 \n",
      "Epoch [28/30], Step [ 1600/1875 ], d_loss: 1.1631, g_loss: 1.1160,g_score: 11.1113 \n",
      "Epoch [28/30], Step [ 1700/1875 ], d_loss: 1.0675, g_loss: 1.1886,g_score: 10.2578 \n",
      "Epoch [28/30], Step [ 1800/1875 ], d_loss: 0.8086, g_loss: 0.9078,g_score: 13.3827 \n",
      "29\n",
      "Epoch [29/30], Step [ 0/1875 ], d_loss: 0.9717, g_loss: 1.1986,g_score: 10.1883 \n",
      "Epoch [29/30], Step [ 100/1875 ], d_loss: 1.1771, g_loss: 1.4237,g_score: 8.0224 \n",
      "Epoch [29/30], Step [ 200/1875 ], d_loss: 1.0014, g_loss: 1.3337,g_score: 8.8989 \n",
      "Epoch [29/30], Step [ 300/1875 ], d_loss: 1.1163, g_loss: 1.1965,g_score: 10.6410 \n",
      "Epoch [29/30], Step [ 400/1875 ], d_loss: 1.0421, g_loss: 0.9463,g_score: 12.9143 \n",
      "Epoch [29/30], Step [ 500/1875 ], d_loss: 0.9443, g_loss: 1.1096,g_score: 11.1569 \n",
      "Epoch [29/30], Step [ 600/1875 ], d_loss: 1.0265, g_loss: 1.0130,g_score: 12.3820 \n",
      "Epoch [29/30], Step [ 700/1875 ], d_loss: 1.2866, g_loss: 0.8389,g_score: 14.8786 \n",
      "Epoch [29/30], Step [ 800/1875 ], d_loss: 1.1214, g_loss: 1.0963,g_score: 11.1787 \n",
      "Epoch [29/30], Step [ 900/1875 ], d_loss: 0.9372, g_loss: 1.0004,g_score: 12.0231 \n",
      "Epoch [29/30], Step [ 1000/1875 ], d_loss: 0.9327, g_loss: 1.2701,g_score: 9.5458 \n",
      "Epoch [29/30], Step [ 1100/1875 ], d_loss: 1.1440, g_loss: 1.0597,g_score: 12.0452 \n",
      "Epoch [29/30], Step [ 1200/1875 ], d_loss: 1.2793, g_loss: 0.9072,g_score: 14.1594 \n",
      "Epoch [29/30], Step [ 1300/1875 ], d_loss: 1.1142, g_loss: 1.1633,g_score: 10.8737 \n",
      "Epoch [29/30], Step [ 1400/1875 ], d_loss: 1.1806, g_loss: 1.2572,g_score: 9.7000 \n",
      "Epoch [29/30], Step [ 1500/1875 ], d_loss: 0.8658, g_loss: 1.0465,g_score: 11.7094 \n",
      "Epoch [29/30], Step [ 1600/1875 ], d_loss: 1.2678, g_loss: 1.3630,g_score: 8.7202 \n",
      "Epoch [29/30], Step [ 1700/1875 ], d_loss: 1.1808, g_loss: 0.9572,g_score: 12.7654 \n",
      "Epoch [29/30], Step [ 1800/1875 ], d_loss: 1.1025, g_loss: 1.0745,g_score: 11.3247 \n"
     ]
    }
   ],
   "source": [
    "total_steps = len(data_loader)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.reshape(batch_size, image_size).to(device)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        outputs = D(images)\n",
    "        d_loss_real = loss_fn(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = loss_fn(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = loss_fn(outputs, real_labels)\n",
    "        g_score = outputs\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        if i % 100 ==0:  #generator的score比较高的话效果就会比较好 一直上升   #拓展：提高效果\n",
    "            print(\"Epoch [{}/{}], Step [ {}/{} ], d_loss: {:.4f}, g_loss: {:.4f},g_score: {:.4f} \"#加g_score\n",
    "              .format(epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item(), g_score.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 展示生成效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f35101d8630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z=torch.randn(batch_size,latent_size).to(device)\n",
    "fake_images=G(z)\n",
    "#print(fake_images.shape)\n",
    "fake_images=fake_images.view(batch_size,28,28).data.cpu().numpy()\n",
    "plt.imshow(fake_images[0],cmap=plt.cm.gray)\n",
    "#camp=plt.cm.gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存储模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(),\"mnist_genator.pth\")\n",
    "torch.save(D.state_dict(),\"mnist_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载保存的生成器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (1): LeakyReLU(negative_slope=0.2)\n",
      "  (2): Linear(in_features=256, out_features=784, bias=True)\n",
      "  (3): Tanh()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3511fdd550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = G\n",
    "model.load_state_dict(torch.load(\"mnist_genator.pth\"))\n",
    "print(model)\n",
    "z=torch.randn(batch_size,latent_size).to(device)\n",
    "fake_images=model(z)\n",
    "fake_images2=fake_images.view(batch_size,28,28).data.cpu().numpy()\n",
    "plt.imshow(fake_images2[0],cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载保存的判别器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): LeakyReLU(negative_slope=0.2)\n",
      "  (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (3): LeakyReLU(negative_slope=0.2)\n",
      "  (4): Sigmoid()\n",
      ")\n",
      "torch.Size([784])\n",
      "tensor(9.7276, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_D = D\n",
    "model_D.load_state_dict(torch.load(\"mnist_discriminator.pth\"))\n",
    "print(model_D)\n",
    "# print(fake_images)\n",
    "# z=torch.randn(batch_size,latent_size).to(device)\n",
    "print(fake_images[0].shape)\n",
    "images = fake_images.reshape(batch_size, image_size).to(device)\n",
    "outputs = model_D(images)\n",
    "g_score = outputs\n",
    "print(g_score.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 扩展：AutoEncoder MNIST数据集生成效果\n",
    "\n",
    "AutoEncoder通过设计encode和decode过程使输入和输出越来越接近，是一种无监督学习过程。\n",
    "\n",
    "https://blog.csdn.net/roguesir/article/details/77469665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "epoch: 1, Loss: 373.6883\n",
      "epoch: 2, Loss: 323.0797\n",
      "epoch: 3, Loss: 289.3018\n",
      "epoch: 4, Loss: 275.0599\n",
      "epoch: 5, Loss: 254.7936\n",
      "epoch: 6, Loss: 213.2516\n",
      "epoch: 7, Loss: 203.1416\n",
      "epoch: 8, Loss: 203.8627\n",
      "epoch: 9, Loss: 200.3034\n",
      "epoch: 10, Loss: 190.4212\n",
      "epoch: 11, Loss: 191.5134\n",
      "epoch: 12, Loss: 185.1606\n",
      "epoch: 13, Loss: 181.8550\n",
      "epoch: 14, Loss: 175.7604\n",
      "epoch: 15, Loss: 172.6214\n",
      "epoch: 16, Loss: 162.2693\n",
      "epoch: 17, Loss: 160.3041\n",
      "epoch: 18, Loss: 157.7487\n",
      "epoch: 19, Loss: 150.8010\n",
      "epoch: 20, Loss: 148.2188\n",
      "epoch: 21, Loss: 143.4731\n",
      "epoch: 22, Loss: 137.7948\n",
      "epoch: 23, Loss: 135.4882\n",
      "epoch: 24, Loss: 132.4848\n",
      "epoch: 25, Loss: 125.1537\n",
      "epoch: 26, Loss: 128.1185\n",
      "epoch: 27, Loss: 127.7759\n",
      "epoch: 28, Loss: 123.9745\n",
      "epoch: 29, Loss: 121.9467\n",
      "epoch: 30, Loss: 122.0019\n",
      "epoch: 31, Loss: 116.5510\n",
      "epoch: 32, Loss: 118.3284\n",
      "epoch: 33, Loss: 121.0883\n",
      "epoch: 34, Loss: 112.5018\n",
      "epoch: 35, Loss: 114.9447\n",
      "epoch: 36, Loss: 114.1455\n",
      "epoch: 37, Loss: 116.1165\n",
      "epoch: 38, Loss: 115.0061\n",
      "epoch: 39, Loss: 114.0631\n",
      "epoch: 40, Loss: 115.1805\n",
      "epoch: 41, Loss: 115.4937\n",
      "epoch: 42, Loss: 114.3049\n",
      "epoch: 43, Loss: 111.7989\n",
      "epoch: 44, Loss: 111.8565\n",
      "epoch: 45, Loss: 111.7147\n",
      "epoch: 46, Loss: 108.9448\n",
      "epoch: 47, Loss: 110.9186\n",
      "epoch: 48, Loss: 107.6919\n",
      "epoch: 49, Loss: 108.1306\n",
      "epoch: 50, Loss: 107.4309\n",
      "epoch: 51, Loss: 108.7296\n",
      "epoch: 52, Loss: 109.9063\n",
      "epoch: 53, Loss: 109.1449\n",
      "epoch: 54, Loss: 107.8115\n",
      "epoch: 55, Loss: 108.0868\n",
      "epoch: 56, Loss: 108.7117\n",
      "epoch: 57, Loss: 111.5032\n",
      "epoch: 58, Loss: 104.3062\n",
      "epoch: 59, Loss: 108.3681\n",
      "epoch: 60, Loss: 105.6366\n",
      "epoch: 61, Loss: 106.6270\n",
      "epoch: 62, Loss: 104.8824\n",
      "epoch: 63, Loss: 104.5274\n",
      "epoch: 64, Loss: 105.2564\n",
      "epoch: 65, Loss: 106.2609\n",
      "epoch: 66, Loss: 103.8324\n",
      "epoch: 67, Loss: 102.6281\n",
      "epoch: 68, Loss: 105.1063\n",
      "epoch: 69, Loss: 105.3823\n",
      "epoch: 70, Loss: 103.0302\n",
      "epoch: 71, Loss: 105.2235\n",
      "epoch: 72, Loss: 100.8318\n",
      "epoch: 73, Loss: 104.2212\n",
      "epoch: 74, Loss: 103.4625\n",
      "epoch: 75, Loss: 101.1608\n",
      "epoch: 76, Loss: 103.4942\n",
      "epoch: 77, Loss: 105.5042\n",
      "epoch: 78, Loss: 104.4627\n",
      "epoch: 79, Loss: 104.8660\n",
      "epoch: 80, Loss: 102.6888\n",
      "epoch: 81, Loss: 100.4685\n",
      "epoch: 82, Loss: 99.6534\n",
      "epoch: 83, Loss: 103.9988\n",
      "epoch: 84, Loss: 102.4629\n",
      "epoch: 85, Loss: 99.6743\n",
      "epoch: 86, Loss: 98.0703\n",
      "epoch: 87, Loss: 99.8311\n",
      "epoch: 88, Loss: 103.4275\n",
      "epoch: 89, Loss: 100.4897\n",
      "epoch: 90, Loss: 102.4651\n",
      "epoch: 91, Loss: 102.1401\n",
      "epoch: 92, Loss: 101.7406\n",
      "epoch: 93, Loss: 101.8409\n",
      "epoch: 94, Loss: 101.2774\n",
      "epoch: 95, Loss: 100.8082\n",
      "epoch: 96, Loss: 100.0312\n",
      "epoch: 97, Loss: 96.2103\n",
      "epoch: 98, Loss: 101.4306\n",
      "epoch: 99, Loss: 98.6221\n",
      "epoch: 100, Loss: 100.2187\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as tfs\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "im_tfs = tfs.Compose([\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize([0.5], [0.5]) # 标准化\n",
    "])\n",
    "\n",
    "train_set = MNIST('./mnist_data', transform=im_tfs,download=True)\n",
    "train_data = DataLoader(train_set, batch_size=2048, shuffle=True)\n",
    "\n",
    "# 定义网络\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 3) # 输出的 code 是 3 维，便于可视化\n",
    "        )\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n",
    "\n",
    "\n",
    "class conv_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # (b, 16, 10, 10)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # (b, 8, 3, 3)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (b, 8, 2, 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # (b, 8, 15, 15)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # (b, 1, 28, 28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n",
    "\n",
    "# autoencoder\n",
    "net = autoencoder()\n",
    "x = Variable(torch.randn(1, 28*28)) # batch size 是 1\n",
    "code, _ = net(x)\n",
    "print(code.shape)\n",
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# conv_autoencoder\n",
    "conv_net = conv_autoencoder()\n",
    "if torch.cuda.is_available():\n",
    "    conv_net = conv_net.to(device)\n",
    "optimizer = torch.optim.Adam(conv_net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def to_img(x):\n",
    "    '''\n",
    "    定义一个函数将最后的结果转换回图片\n",
    "    '''\n",
    "    x = 0.5 * (x + 1.)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.shape[0], 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "# 开始训练自动编码器\n",
    "for e in range(100):\n",
    "    for im, _ in train_data:\n",
    "        # im = im.view(im.shape[0], -1)\n",
    "        im = Variable(im).to(device)\n",
    "        # 前向传播\n",
    "        _, output = conv_net(im)\n",
    "        loss = criterion(output, im) / im.shape[0] # 平均\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # if (e+1)%20==0:\n",
    "    print('epoch: {}, Loss: {:.4f}'.format(e + 1, loss.item()))\n",
    "    pic = to_img(output.cpu().data)\n",
    "    if not os.path.exists('./simple_autoencoder'):\n",
    "        os.mkdir('./simple_autoencoder')\n",
    "    save_image(pic, './simple_autoencoder/image_conv_epoch{}.png'.format(e + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果\n",
    "迭代一百次效果\n",
    "![](./simple_autoencoder/image_conv_epoch100.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}