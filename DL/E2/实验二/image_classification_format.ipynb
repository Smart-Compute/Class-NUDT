{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容一：在MNIST数据集上构建网络进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as tud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学会使用Dataloader来加载数据\n",
    "Dataloader能够帮我们打乱数据集，拿到batch数据 \\\n",
    "为了使用Dataloader，需要定义以下三个function\n",
    "- \\__init__: 模型初始化\n",
    "- \\__len__: 返回整个数据集有多少item\n",
    "- \\__getitem__: 根据给定的index返回一个item\n",
    "\n",
    "调用Dataloader之前还要先定义dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch帮助我们预先加载了一些常用的数据集\n",
    "# 如果使用这些数据集，会相对容易的进行数据加载\n",
    "# 例如：常用的Mnist数据集\n",
    "mnist_train_data = datasets.MNIST(\"./data\",train=True,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.13066062,),std=(0.30810776,))\n",
    "                                 ]))\n",
    "batch_size = 64\n",
    "train_dataloader = tud.DataLoader(mnist_train_data,batch_size = batch_size,shuffle=True) # 将dataset转换为iterator\n",
    "mnist_test_data = datasets.MNIST(\"./data\",train=False,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.13066062,),std=(0.30810776,))\n",
    "                                 ]))\n",
    "test_dataloader = tud.DataLoader(mnist_test_data,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 定义网络\n",
    "- 继承 nn.Module\n",
    "- 初始化函数\n",
    "- forward 函数\n",
    "- 其余可以根据模型需要定义相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的基于ConvNet的简单神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__() # the input is 1*28*28\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1) # (28-5)/1+1=24, 20*24*24\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1) # 12-5+1=8\n",
    "        self.fc1 = nn.Linear(4*4*50,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x)) # 20 * 24 * 24\n",
    "        x = F.max_pool2d(x,2,2) # 20 * 12 * 12\n",
    "        x = F.relu(self.conv2(x)) # 50 * 8 * 8\n",
    "        x = F.max_pool2d(x,2,2) # 50 * 4 * 4\n",
    "        x = x.view(-1,4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x #F.log_softmax(x,dim=1)\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们把所有数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda Tensor\n",
    "- forward pass\n",
    "- 计算loss\n",
    "- 清空gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    for idx, (data, label) in enumerate(train_dataloader):\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Train Epoch: {}, iteration: {}, loss: {}\".format(\n",
    "                epoch,idx,loss.item()))  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (data,target) in enumerate(test_dataloader):\n",
    "            output = model(data) # batch_size * 10        \n",
    "            loss = loss_fn(output,target)*output.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            total_loss += loss\n",
    "            correct += pred.eq(target).sum()\n",
    "    total_loss /= len(test_dataloader.dataset)\n",
    "    acc = 100.*correct/len(test_dataloader.dataset)\n",
    "    print(\"Test Loss:{}, Accuracy:{}\".format(total_loss,acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 0.009488344192504883\n",
      "Train Epoch: 0, iteration: 100, loss: 0.1327199935913086\n",
      "Train Epoch: 0, iteration: 200, loss: 0.01545263547450304\n",
      "Train Epoch: 0, iteration: 300, loss: 0.012227410450577736\n",
      "Train Epoch: 0, iteration: 400, loss: 0.010789387859404087\n",
      "Train Epoch: 0, iteration: 500, loss: 0.03559090942144394\n",
      "Train Epoch: 0, iteration: 600, loss: 0.20145973563194275\n",
      "Train Epoch: 0, iteration: 700, loss: 0.03271903470158577\n",
      "Train Epoch: 0, iteration: 800, loss: 0.02920989692211151\n",
      "Train Epoch: 0, iteration: 900, loss: 0.027539599686861038\n",
      "Test Loss:0.038897547870874405, Accuracy:98\n",
      "Train Epoch: 1, iteration: 0, loss: 0.041452355682849884\n",
      "Train Epoch: 1, iteration: 100, loss: 0.010027386248111725\n",
      "Train Epoch: 1, iteration: 200, loss: 0.004974566865712404\n",
      "Train Epoch: 1, iteration: 300, loss: 0.05770661681890488\n",
      "Train Epoch: 1, iteration: 400, loss: 0.15365464985370636\n",
      "Train Epoch: 1, iteration: 500, loss: 0.001344119431450963\n",
      "Train Epoch: 1, iteration: 600, loss: 0.08116044104099274\n",
      "Train Epoch: 1, iteration: 700, loss: 0.057694513350725174\n",
      "Train Epoch: 1, iteration: 800, loss: 0.011324846185743809\n",
      "Train Epoch: 1, iteration: 900, loss: 0.009986087679862976\n",
      "Test Loss:0.03296409174799919, Accuracy:98\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model = train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(model,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 0.019819634035229683\n",
      "Train Epoch: 0, iteration: 100, loss: 0.005775895901024342\n",
      "Train Epoch: 0, iteration: 200, loss: 0.0707644447684288\n",
      "Train Epoch: 0, iteration: 300, loss: 0.06397535651922226\n",
      "Train Epoch: 0, iteration: 400, loss: 0.014788621105253696\n",
      "Train Epoch: 0, iteration: 500, loss: 0.006537220440804958\n",
      "Train Epoch: 0, iteration: 600, loss: 0.038363415747880936\n",
      "Train Epoch: 0, iteration: 700, loss: 0.006373587995767593\n",
      "Train Epoch: 0, iteration: 800, loss: 0.003357541048899293\n",
      "Train Epoch: 0, iteration: 900, loss: 0.0035391380079090595\n",
      "Test Loss:0.038350194692611694, Accuracy:98\n",
      "Train Epoch: 1, iteration: 0, loss: 0.054631706327199936\n",
      "Train Epoch: 1, iteration: 100, loss: 0.09225575625896454\n",
      "Train Epoch: 1, iteration: 200, loss: 0.0015655563911423087\n",
      "Train Epoch: 1, iteration: 300, loss: 0.04000664874911308\n",
      "Train Epoch: 1, iteration: 400, loss: 0.02154170535504818\n",
      "Train Epoch: 1, iteration: 500, loss: 0.009217644110321999\n",
      "Train Epoch: 1, iteration: 600, loss: 0.008485698141157627\n",
      "Train Epoch: 1, iteration: 700, loss: 0.0022953308653086424\n",
      "Train Epoch: 1, iteration: 800, loss: 0.09785564988851547\n",
      "Train Epoch: 1, iteration: 900, loss: 0.09012392908334732\n",
      "Test Loss:0.028637487441301346, Accuracy:99\n"
     ]
    }
   ],
   "source": [
    "#torch.save(model.state_dict(),\"mnist_cnn.pth\")\n",
    "# num_epochs = 2\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(num_epochs):\n",
    "    train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    acc = test(model,test_dataloader,loss_fn)\n",
    "    if acc > best_valid_acc:\n",
    "        best_valid_acc = acc\n",
    "        torch.save(model.state_dict(),\"best_mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:0.028637487441301346, Accuracy:99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(99)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = Net()\n",
    "test_model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "test(model,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, iteration: 0, loss: 2.3000686168670654\n",
      "Train Epoch: 0, iteration: 100, loss: 0.855004608631134\n",
      "Train Epoch: 0, iteration: 200, loss: 0.9181270599365234\n",
      "Train Epoch: 0, iteration: 300, loss: 0.9267852902412415\n",
      "Train Epoch: 0, iteration: 400, loss: 0.661331295967102\n",
      "Train Epoch: 0, iteration: 500, loss: 0.8637953400611877\n",
      "Train Epoch: 0, iteration: 600, loss: 0.5660000443458557\n",
      "Train Epoch: 0, iteration: 700, loss: 0.6191807985305786\n",
      "Train Epoch: 0, iteration: 800, loss: 0.8863906860351562\n",
      "Train Epoch: 0, iteration: 900, loss: 0.3977414071559906\n",
      "Train Epoch: 0, iteration: 1000, loss: 0.33772027492523193\n",
      "Train Epoch: 0, iteration: 1100, loss: 0.3567613363265991\n",
      "Train Epoch: 0, iteration: 1200, loss: 0.4134741425514221\n",
      "Train Epoch: 0, iteration: 1300, loss: 0.42777499556541443\n",
      "Train Epoch: 0, iteration: 1400, loss: 0.30109351873397827\n",
      "Train Epoch: 0, iteration: 1500, loss: 0.5608471035957336\n",
      "Train Epoch: 0, iteration: 1600, loss: 0.4938139319419861\n",
      "Train Epoch: 0, iteration: 1700, loss: 0.6408547759056091\n",
      "Train Epoch: 0, iteration: 1800, loss: 0.29374298453330994\n",
      "Test Loss:0.4396461844444275, Accuracy:84\n",
      "Train Epoch: 1, iteration: 0, loss: 0.2888000011444092\n",
      "Train Epoch: 1, iteration: 100, loss: 0.6127199530601501\n",
      "Train Epoch: 1, iteration: 200, loss: 0.19247622787952423\n",
      "Train Epoch: 1, iteration: 300, loss: 0.8792325258255005\n",
      "Train Epoch: 1, iteration: 400, loss: 0.3635019361972809\n",
      "Train Epoch: 1, iteration: 500, loss: 0.42323848605155945\n",
      "Train Epoch: 1, iteration: 600, loss: 0.1578446924686432\n",
      "Train Epoch: 1, iteration: 700, loss: 0.2313484251499176\n",
      "Train Epoch: 1, iteration: 800, loss: 0.35378584265708923\n",
      "Train Epoch: 1, iteration: 900, loss: 0.34003928303718567\n",
      "Train Epoch: 1, iteration: 1000, loss: 0.5605223178863525\n",
      "Train Epoch: 1, iteration: 1100, loss: 0.6576738953590393\n",
      "Train Epoch: 1, iteration: 1200, loss: 0.39148780703544617\n",
      "Train Epoch: 1, iteration: 1300, loss: 0.348854124546051\n",
      "Train Epoch: 1, iteration: 1400, loss: 0.4536255896091461\n",
      "Train Epoch: 1, iteration: 1500, loss: 0.2236243039369583\n",
      "Train Epoch: 1, iteration: 1600, loss: 0.3038178086280823\n",
      "Train Epoch: 1, iteration: 1700, loss: 0.40239763259887695\n",
      "Train Epoch: 1, iteration: 1800, loss: 0.4798986315727234\n",
      "Test Loss:0.37299150228500366, Accuracy:86\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./fashion_mnist_data\",train=True,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                   ])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True) # 将dataset转换为iterator\n",
    "test_dataloader = tud.DataLoader(\n",
    "    datasets.FashionMNIST(\"./fashion_mnist_data\",train=False,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean=(0.2860402,),std=(0.3530239,))\n",
    "                   ])),\n",
    "    batch_size=batch_size) # 将dataset转换为iterator\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr,momentum=momentum)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(model,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    test(model,test_dataloader,loss_fn)\n",
    "    \n",
    "torch.save(model.state_dict(),\"fashion_mnist_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容二：CNN模型的迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 很多时候当我们训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用预训练的模型来加速训练的过程。我们经常使用在ImageNet上的预训练模型\n",
    "- 有两种方法做迁移学习\n",
    "    - finetuning：从一个预训练模型开始，改变一些模型的架构，然后继续训练整个模型的参数；\n",
    "    - feature extraction：不改变预训练模型的参数，只更新我们改变过的部分模型参数。（当成特征提取器来使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.utils.data as tud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据：使用hymenoptera_data数据集 \\\n",
    "数据集包括两类图片，bees和ants。这些数据都被处理成了可以使用ImageFolder来读取的格式。我们只需要把data_dir设置成数据的根目录，然后把model_name设置成我们想要使用的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "data_dir = \"./data/hymenoptera_data\"\n",
    "model_name = \"resnet18\"\n",
    "num_class = 2\n",
    "#feature_extract = True\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据: 把数据预处理成相应的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress\n",
    "all_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(input_size),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),                                    \n",
    "                                ]))\n",
    "loader = tud.DataLoader(all_imgs,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6784, 0.6667, 0.6745,  ..., 0.4784, 0.4667, 0.4471],\n",
       "         [0.6667, 0.6588, 0.6627,  ..., 0.4745, 0.4706, 0.4667],\n",
       "         [0.6902, 0.6863, 0.6863,  ..., 0.4863, 0.4863, 0.4784],\n",
       "         ...,\n",
       "         [0.4549, 0.4902, 0.4784,  ..., 0.3843, 0.4706, 0.4510],\n",
       "         [0.5490, 0.4784, 0.5137,  ..., 0.4392, 0.4706, 0.4667],\n",
       "         [0.6431, 0.5451, 0.5569,  ..., 0.4941, 0.3961, 0.4039]],\n",
       "\n",
       "        [[0.6902, 0.6941, 0.6902,  ..., 0.4706, 0.4627, 0.4510],\n",
       "         [0.6941, 0.6941, 0.6980,  ..., 0.4784, 0.4706, 0.4588],\n",
       "         [0.7020, 0.6980, 0.6902,  ..., 0.4784, 0.4745, 0.4667],\n",
       "         ...,\n",
       "         [0.4667, 0.4980, 0.4824,  ..., 0.3686, 0.4745, 0.4431],\n",
       "         [0.5569, 0.4863, 0.5176,  ..., 0.4235, 0.4627, 0.4784],\n",
       "         [0.6902, 0.5843, 0.5961,  ..., 0.5137, 0.3922, 0.3961]],\n",
       "\n",
       "        [[0.7373, 0.7294, 0.7216,  ..., 0.4941, 0.4902, 0.4824],\n",
       "         [0.7255, 0.7216, 0.7176,  ..., 0.5059, 0.5020, 0.4863],\n",
       "         [0.7373, 0.7255, 0.7216,  ..., 0.5059, 0.5098, 0.5020],\n",
       "         ...,\n",
       "         [0.4667, 0.4863, 0.4784,  ..., 0.3490, 0.4196, 0.4667],\n",
       "         [0.5686, 0.4941, 0.5333,  ..., 0.4196, 0.4510, 0.5059],\n",
       "         [0.7176, 0.6078, 0.6235,  ..., 0.5216, 0.4745, 0.4157]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_imgs[20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "batch_size = 32\n",
    "train_imgs = datasets.ImageFolder(os.path.join(data_dir,\"train\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.RandomResizedCrop(input_size),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "train_dataloader = tud.DataLoader(train_imgs,batch_size=batch_size,shuffle=True)\n",
    "test_imgs = datasets.ImageFolder(os.path.join(data_dir,\"val\"),\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.Resize(input_size),  \n",
    "                                    transforms.CenterCrop(input_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225])\n",
    "                                ]))\n",
    "test_dataloader = tud.DataLoader(test_imgs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format\n",
    "def initialize_model(model_name,num_class,use_pretrained=True,feature_extract=True):\n",
    "    if model_name == \"resnet18\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        if feature_extract: # do not update the parameters\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_class)        \n",
    "    else:\n",
    "        print(\"model not implemented\")\n",
    "        return None\n",
    "    return model_ft\n",
    "model_ft = initialize_model(\"resnet18\",2,use_pretrained=False,feature_extract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model_ft.layer1[0].conv1.weight.requires_grad)\n",
    "print(model_ft.fc.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "optimizer = optim.SGD(model_ft.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_dataloader,loss_fn,optimizer,epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(train_dataloader.dataset)\n",
    "    epoch_accuracy = total_corrects / len(train_dataloader.dataset)\n",
    "    print(\"Epoch:{}, Training Loss:{}, Traning Acc:{}\".format(epoch,epoch_loss,epoch_accuracy))  \n",
    "    #return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_corrects = 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_corrects += torch.sum(preds.eq(labels))\n",
    "    epoch_loss = total_loss / len(test_dataloader.dataset)\n",
    "    epoch_accuracy = total_corrects / len(test_dataloader.dataset)\n",
    "    print(\"acc type:\", epoch_accuracy)\n",
    "    print(\"Test Loss:{}, Test Acc:{}\".format(epoch_loss,epoch_accuracy))  \n",
    "    return epoch_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Training Loss:0.8485996078272335, Traning Acc:0\n",
      "Test Loss:0.722400501857396, Test Acc:0\n",
      "Epoch:1, Training Loss:0.7474724112964067, Traning Acc:0\n",
      "Test Loss:0.6469219225684023, Test Acc:0\n",
      "Epoch:2, Training Loss:0.6163499804793812, Traning Acc:0\n",
      "Test Loss:0.5612572902947469, Test Acc:0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model_ft,train_dataloader,loss_fn,optimizer,epoch)\n",
    "    acc = test_model(model_ft,test_dataloader,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
